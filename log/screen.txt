total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
Training max steps: 300001
Num GPUs: 1
Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 10-resmlp-single-2x
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, y = res+2*attn(x), x = x + mlp(LN(res)), ELEMENTWISEAFFINE=False
I am GPU 0  of  1
found 99 shards for split train
train: loaded 100000000 tokens (first shard)
train: 1 epoch (1 shard) = 12207 mini-batches
found 1 shards for split val
val: loaded 100000000 tokens (first shard)
val: 1 epoch (1 shard) = 12207 mini-batches
compilation is off
num decayed parameter tensors: 10, with 53,575,680 parameters
num non-decayed parameter tensors: 8, with 13,824 parameters
using fused AdamW: True
