total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
Training max steps: 300001
Num GPUs: 1
Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 10-resmlp-single-axm-novalue-extramlp
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, z = self.attn(x)*self.mlp(x)+self.mlp2(x), x=res+z, no value matrix (use identity instead), ELEMENTWISEAFFINE=False
I am GPU 0  of  1
found 99 shards for split train
train: loaded 100000000 tokens (first shard)
train: 1 epoch (1 shard) = 12207 mini-batches
found 1 shards for split val
val: loaded 100000000 tokens (first shard)
val: 1 epoch (1 shard) = 12207 mini-batches
compilation is off
num decayed parameter tensors: 10, with 52,396,032 parameters
num non-decayed parameter tensors: 8, with 12,288 parameters
using fused AdamW: True
@ 0 train 10.8877 , allloss: 10.8877, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-05, norm:10.4041, dt: 3215.09ms, tok/sec: 40767.80, flops:17.66, batch-reuse:1
INFO nextres 0.40590202808380127 attn*mlp 0.4044242799282074 layernormed 1.0006083250045776
			attn_hist -38.8125<tensor([  0.,   0.,   0.,  36., 346., 347.,  39.,   0.,   0.,   0.])>39.375
			mlp_hist -0.220703125<tensor([  0.,   0.,   0.,   0.,  36., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2119140625
			x_hist -5.9761738777160645<tensor([  0.,   0.,   0.,   0., 356., 412.,   0.,   0.,   0.,   0.])>4.212235927581787
INFO nextres 0.6768593192100525 attn*mlp 0.5050751566886902 layernormed 1.000638484954834
			attn_hist -71.625<tensor([  0.,   2.,   7.,  29., 318., 384.,  25.,   3.,   0.,   0.])>50.625
			mlp_hist -0.248046875<tensor([  0.,   0.,   0.,   0.,  28., 740.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2431640625
			x_hist -7.433075904846191<tensor([  0.,   0.,   0.,   0., 376., 392.,   0.,   0.,   0.,   0.])>6.9641547203063965
INFO nextres 1.2612006664276123 attn*mlp 0.7568367719650269 layernormed 1.0006479024887085
			attn_hist -89.25<tensor([  1.,   1.,   5.,  23., 346., 359.,  28.,   3.,   1.,   1.])>83.625
			mlp_hist -0.22265625<tensor([  0.,   0.,   0.,   0.,  32., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.25
			x_hist -12.653864860534668<tensor([  0.,   0.,   0.,   1., 312., 455.,   0.,   0.,   0.,   0.])>6.41892147064209
INFO nextres 2.109121799468994 attn*mlp 0.9778839349746704 layernormed 1.000650405883789
			attn_hist -151.5<tensor([  0.,   2.,   2.,  17., 290., 439.,  10.,   4.,   2.,   0.])>76.875
			mlp_hist -0.2470703125<tensor([  0.,   0.,   0.,   0.,  34., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2392578125
			x_hist -14.214946746826172<tensor([  0.,   0.,   0.,   2., 272., 494.,   0.,   0.,   0.,   0.])>6.782434940338135
INFO nextres 3.1024045944213867 attn*mlp 1.111498236656189 layernormed 1.0006511211395264
			attn_hist -170.25<tensor([  2.,   0.,   2.,  14., 254., 479.,   9.,   5.,   0.,   1.])>81.375
			mlp_hist -0.220703125<tensor([  0.,   0.,   0.,   0.,  27., 740.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.21484375
			x_hist -14.588468551635742<tensor([  0.,   0.,   0.,   2., 250., 516.,   0.,   0.,   0.,   0.])>6.815230846405029
INFO nextres 4.19564151763916 attn*mlp 1.2058111429214478 layernormed 1.0006513595581055
			attn_hist -174.75<tensor([  2.,   0.,   3.,  12., 233., 503.,   8.,   4.,   0.,   1.])>81.75
			mlp_hist -0.2099609375<tensor([  0.,   0.,   0.,   0.,  29., 740.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.216796875
			x_hist -14.612198829650879<tensor([  0.,   0.,   0.,   2., 239., 527.,   0.,   0.,   0.,   0.])>6.712491035461426
INFO nextres 5.365484237670898 attn*mlp 1.2783234119415283 layernormed 1.0006515979766846
			attn_hist -175.5<tensor([  2.,   0.,   3.,  10., 224., 516.,   8.,   2.,   0.,   1.])>80.625
			mlp_hist -0.2041015625<tensor([  0.,   0.,   0.,   0.,  32., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2177734375
			x_hist -14.476119041442871<tensor([  0.,   0.,   0.,   2., 225., 541.,   0.,   0.,   0.,   0.])>6.5435638427734375
INFO nextres 6.59404182434082 attn*mlp 1.3343437910079956 layernormed 1.0006515979766846
			attn_hist -174.0<tensor([  1.,   1.,   2.,  10., 211., 530.,   8.,   2.,   1.,   0.])>78.375
			mlp_hist -0.2001953125<tensor([  0.,   0.,   0.,   0.,  36., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.21875
			x_hist -15.01264762878418<tensor([  0.,   0.,   0.,   2., 221., 545.,   0.,   0.,   0.,   0.])>6.344444751739502
INFO nextres 7.8703813552856445 attn*mlp 1.3796169757843018 layernormed 1.0006517171859741
			attn_hist -180.0<tensor([  1.,   1.,   1.,  10., 208., 535.,   7.,   1.,   2.,   0.])>76.125
			mlp_hist -0.203125<tensor([  0.,   0.,   0.,   0.,  33., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.21875
			x_hist -15.622541427612305<tensor([  0.,   0.,   0.,   2., 219., 547.,   0.,   0.,   0.,   0.])>6.135796546936035
INFO nextres 9.189035415649414 attn*mlp 1.4198216199874878 layernormed 1.0006517171859741
			attn_hist -187.5<tensor([  1.,   1.,   1.,   9., 207., 538.,   6.,   1.,   2.,   0.])>73.5
			mlp_hist -0.2099609375<tensor([  0.,   0.,   0.,   0.,  32., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2177734375
			x_hist -16.171266555786133<tensor([  0.,   0.,   0.,   2., 215., 551.,   0.,   0.,   0.,   0.])>5.9244256019592285
INFO nextres 10.542814254760742 attn*mlp 1.4526749849319458 layernormed 1.0006517171859741
			attn_hist -193.5<tensor([  1.,   2.,   0.,   8., 204., 542.,   6.,   1.,   2.,   0.])>71.25
			mlp_hist -0.2138671875<tensor([  0.,   0.,   0.,   0.,  33., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2177734375
			x_hist -16.64906120300293<tensor([  0.,   0.,   0.,   2., 215., 551.,   0.,   0.,   0.,   0.])>6.174801349639893
INFO nextres 11.928020477294922 attn*mlp 1.4816986322402954 layernormed 1.0006517171859741
			attn_hist -199.5<tensor([  1.,   2.,   0.,   7., 205., 543.,   5.,   1.,   2.,   0.])>74.25
			mlp_hist -0.21875<tensor([  0.,   0.,   0.,   0.,  33., 736.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.216796875
			x_hist -17.08201789855957<tensor([  0.,   0.,   0.,   2., 212., 554.,   0.,   0.,   0.,   0.])>6.471080303192139
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		(etime:0.0002 Lo:0.0002conservative:0.0002 pulls:0.0001 Le:0.0002mbol:0.0002 Lockheed:0.0002Fr:0.0002)
		(inventoryQuantity:0.0002 Rugby:0.0002 girlfriend:0.0002 congen:0.0002 seat:0.0002 barren:0.0002 amphib:0.0002 discovering:0.0002)
		( hysteria:0.0002 Rugby:0.0002does:0.0002A:0.0001does:0.0002 barren:0.0002 IO:0.0002 Provides:0.0002)
		(A:0.0002 winter:0.0002does:0.0002A:0.0002does:0.0003A:0.0002 circadian:0.0002does:0.0002)
		(A:0.0002does:0.0002does:0.0002A:0.0003does:0.0003A:0.0002 circadian:0.0002does:0.0002)
		(A:0.0002A:0.0002does:0.0002A:0.0003does:0.0003A:0.0002does:0.0002does:0.0002)
		(A:0.0003A:0.0003A:0.0002A:0.0003does:0.0003A:0.0002A:0.0002does:0.0002)
		(A:0.0003A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002)
		(A:0.0003A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002)
		(A:0.0002A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002)
		(A:0.0002A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002)
		(A:0.0002A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002A:0.0002)
		(A:0.0002A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002A:0.0002)
A
------
		( Lo:0.0002conservative:0.0002 pulls:0.0001 Le:0.0002mbol:0.0002 Lockheed:0.0002Fr:0.0002 circadian:0.0002)
		( Rugby:0.0002 girlfriend:0.0002 congen:0.0002 seat:0.0002 barren:0.0002 amphib:0.0002 discovering:0.0002 circadian:0.0002)
		( Rugby:0.0002does:0.0002A:0.0001does:0.0002 barren:0.0002 IO:0.0002 Provides:0.0002 cock:0.0002)
		( winter:0.0002does:0.0002A:0.0002does:0.0003A:0.0002 circadian:0.0002does:0.0002 receiving:0.0002)
		(does:0.0002does:0.0002A:0.0003does:0.0003A:0.0002 circadian:0.0002does:0.0002A:0.0002)
		(A:0.0002does:0.0002A:0.0003does:0.0003A:0.0002does:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0002A:0.0003does:0.0003A:0.0002A:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0003A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002does:0.0002A:0.0002)
		(A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002A:0.0002A:0.0002)
		(A:0.0003A:0.0002A:0.0003A:0.0003A:0.0002A:0.0002A:0.0002A:0.0002)
AAAAAAAdoesAA partialdoesdoesdoesA
@ 1 train 10.6893 , allloss: 10.6893, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:1.2000e-04, norm:13.4442, dt: 2120.31ms, tok/sec: 61817.40, flops:26.77, batch-reuse:1
@ 2 train 10.6732 , allloss: 10.6732, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:1.8000e-04, norm:21.2608, dt: 1585.94ms, tok/sec: 82646.26, flops:35.80, batch-reuse:1
@ 3 train 10.4843 , allloss: 10.4843, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:2.4000e-04, norm:10.1459, dt: 1437.03ms, tok/sec: 91210.46, flops:39.51, batch-reuse:1
@ 4 train 10.3039 , allloss: 10.3039, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:3.0000e-04, norm:7.9636, dt: 1437.74ms, tok/sec: 91165.13, flops:39.49, batch-reuse:1
@ 5 train 10.1434 , allloss: 10.1434, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:3.6000e-04, norm:7.2452, dt: 1441.64ms, tok/sec: 90918.91, flops:39.38, batch-reuse:1
@ 6 train 9.9544 , allloss: 9.9544, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:4.2000e-04, norm:5.2316, dt: 1440.66ms, tok/sec: 90980.57, flops:39.41, batch-reuse:1
@ 7 train 9.7433 , allloss: 9.7433, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:4.8000e-04, norm:3.9428, dt: 1440.34ms, tok/sec: 91000.79, flops:39.42, batch-reuse:1
@ 8 train 9.5263 , allloss: 9.5263, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:5.4000e-04, norm:4.1013, dt: 1441.22ms, tok/sec: 90944.96, flops:39.39, batch-reuse:1
@ 9 train 9.2332 , allloss: 9.2332, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:3.4615, dt: 1440.10ms, tok/sec: 91016.04, flops:39.42, batch-reuse:1
@ 10 train 8.9827 , allloss: 8.9827, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:3.0882, dt: 1445.72ms, tok/sec: 90662.07, flops:39.27, batch-reuse:1
@ 11 train 8.7029 , allloss: 8.7029, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:2.3804, dt: 1446.05ms, tok/sec: 90641.39, flops:39.26, batch-reuse:1
@ 12 train 8.3950 , allloss: 8.3950, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:2.0444, dt: 1441.55ms, tok/sec: 90924.05, flops:39.38, batch-reuse:1
