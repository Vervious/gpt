Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 11-apm
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, 
Setting:
========
attn = self.attn(x, x)
mlp = self.mlp(x)
y = attn + mlp
x = res + y
newres = x
x = RMSNorm(x, ELEMENTWISEAFFINE=False), 
======== 
VALUEMATRIX=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1num decayed parameter tensors: 10, with 52,396,032 parameters
num non-decayed parameter tensors: 8, with 12,288 parameters
@ 0 train 10.3099 , allloss: 10.3099, norm:2.2978, dt: 2999.11ms, tok/sec: 43703.62, flops:18.93, batch-reuse:1
INFO nextres 5.433274269104004 attn*mlp 5.420048713684082 layernormed 1.0002602338790894
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  36., 331., 361.,  40.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.3209218978881836<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7263810634613037
INFO nextres 15.413111686706543 attn*mlp 10.41655445098877 layernormed 1.0001344680786133
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  36., 332., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.3208813667297363<tensor([  0.,   0.,   0.,   0., 367., 401.,   0.,   0.,   0.,   0.])>2.72634220123291
INFO nextres 26.27106285095215 attn*mlp 11.271517753601074 layernormed 1.0000407695770264
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  36., 331., 362.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.320716381072998<tensor([  0.,   0.,   0.,   0., 367., 401.,   0.,   0.,   0.,   0.])>2.726191282272339
INFO nextres 37.450111389160156 attn*mlp 11.52442455291748 layernormed 0.9999945163726807
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 332., 362.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.320801258087158<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7262532711029053
INFO nextres 48.79875183105469 attn*mlp 11.642145156860352 layernormed 0.9999703168869019
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.32088041305542<tensor([  0.,   0.,   0.,   0., 367., 401.,   0.,   0.,   0.,   0.])>2.7263054847717285
INFO nextres 60.255470275878906 attn*mlp 11.712681770324707 layernormed 0.9999580979347229
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 332., 362.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.3208258152008057<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.726257562637329
INFO nextres 71.7857666015625 attn*mlp 11.758035659790039 layernormed 0.9999518394470215
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2236328125
			x_hist -3.3207032680511475<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.726149320602417
INFO nextres 83.3697738647461 attn*mlp 11.78996467590332 layernormed 0.9999490976333618
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -3.3206052780151367<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.726062774658203
INFO nextres 94.99446105957031 attn*mlp 11.813177108764648 layernormed 0.9999485611915588
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2236328125
			x_hist -3.3205604553222656<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7260212898254395
INFO nextres 106.65211486816406 attn*mlp 11.831849098205566 layernormed 0.9999495148658752
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2236328125
			x_hist -3.320533037185669<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7259984016418457
INFO nextres 118.33604431152344 attn*mlp 11.846263885498047 layernormed 0.9999510645866394
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2236328125
			x_hist -3.3205440044403076<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7260043621063232
INFO nextres 130.0424346923828 attn*mlp 11.85860538482666 layernormed 0.9999532699584961
			attn_hist -39.9375<tensor([  0.,   0.,   0.,  35., 333., 361.,  39.,   0.,   0.,   0.])>32.625
			mlp_hist -0.2021484375<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2236328125
			x_hist -3.320565938949585<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>2.7260193824768066
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Roses:0.0049 are:0.0073 Roses:0.0034 are:0.0027 are:0.0020 are:0.0024 are:0.0223 are:0.0170)
		(A:0.0145A:0.0113A:0.0078A:0.0081A:0.0061A:0.0051A:0.0035A:0.0032)
		(A:0.0374A:0.0302A:0.0229A:0.0223A:0.0179A:0.0154A:0.0117A:0.0104)
		(A:0.0620A:0.0518A:0.0422A:0.0386A:0.0322A:0.0284A:0.0231A:0.0204)
		(A:0.0830A:0.0737A:0.0620A:0.0552A:0.0476A:0.0422A:0.0364A:0.0322)
		(A:0.1040A:0.0879A:0.0781A:0.0698A:0.0620A:0.0552A:0.0491A:0.0435)
		(A:0.1226A:0.1040A:0.0928A:0.0879A:0.0781A:0.0698A:0.0620A:0.0552)
		(A:0.1367A:0.1226A:0.1099A:0.0981A:0.0879A:0.0830A:0.0737A:0.0659)
		(A:0.1445A:0.1299A:0.1226A:0.1099A:0.0981A:0.0928A:0.0830A:0.0781)
		(A:0.1602A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040A:0.0928A:0.0879)
		(A:0.1689A:0.1523A:0.1445A:0.1299A:0.1226A:0.1162A:0.1040A:0.0981)
		(A:0.1777A:0.1602A:0.1523A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040)
		(A:0.1777A:0.1602A:0.1523A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040)
A
------
		( are:0.0073 Roses:0.0034 are:0.0027 are:0.0020 are:0.0024 are:0.0223 are:0.0170A:0.0054)
		(A:0.0113A:0.0078A:0.0081A:0.0061A:0.0051A:0.0035A:0.0032A:0.0067)
		(A:0.0302A:0.0229A:0.0223A:0.0179A:0.0154A:0.0117A:0.0104A:0.0145)
		(A:0.0518A:0.0422A:0.0386A:0.0322A:0.0284A:0.0231A:0.0204A:0.0253)
		(A:0.0737A:0.0620A:0.0552A:0.0476A:0.0422A:0.0364A:0.0322A:0.0364)
		(A:0.0879A:0.0781A:0.0698A:0.0620A:0.0552A:0.0491A:0.0435A:0.0476)
		(A:0.1040A:0.0928A:0.0879A:0.0781A:0.0698A:0.0620A:0.0552A:0.0586)
		(A:0.1226A:0.1099A:0.0981A:0.0879A:0.0830A:0.0737A:0.0659A:0.0698)
		(A:0.1299A:0.1226A:0.1099A:0.0981A:0.0928A:0.0830A:0.0781A:0.0781)
		(A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040A:0.0928A:0.0879A:0.0879)
		(A:0.1523A:0.1445A:0.1299A:0.1226A:0.1162A:0.1040A:0.0981A:0.0981)
		(A:0.1602A:0.1523A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040A:0.1040)
		(A:0.1602A:0.1523A:0.1445A:0.1299A:0.1226A:0.1099A:0.1040A:0.1040)
AAAAAAAAAAAAAAA
@ 1 train 10.1647 , allloss: 10.1647, norm:2.2334, dt: 1999.05ms, tok/sec: 65567.05, flops:28.40, batch-reuse:1
@ 2 train 10.0478 , allloss: 10.0478, norm:2.2124, dt: 1461.39ms, tok/sec: 89690.10, flops:38.85, batch-reuse:1
@ 3 train 9.8534 , allloss: 9.8534, norm:2.1770, dt: 1342.16ms, tok/sec: 97657.77, flops:42.30, batch-reuse:1
@ 4 train 9.5970 , allloss: 9.5970, norm:1.9774, dt: 1343.99ms, tok/sec: 97524.81, flops:42.24, batch-reuse:1
@ 5 train 9.3505 , allloss: 9.3505, norm:1.7282, dt: 1347.68ms, tok/sec: 97257.86, flops:42.13, batch-reuse:1
@ 6 train 9.0163 , allloss: 9.0163, norm:1.6867, dt: 1346.29ms, tok/sec: 97358.02, flops:42.17, batch-reuse:1
@ 7 train 8.7524 , allloss: 8.7524, norm:1.5102, dt: 1346.60ms, tok/sec: 97335.52, flops:42.16, batch-reuse:1
@ 8 train 8.4717 , allloss: 8.4717, norm:1.3324, dt: 1346.47ms, tok/sec: 97344.66, flops:42.16, batch-reuse:1
@ 9 train 8.2148 , allloss: 8.2148, norm:1.1114, dt: 1344.75ms, tok/sec: 97469.61, flops:42.22, batch-reuse:1
@ 10 train 8.0489 , allloss: 8.0489, norm:0.8997, dt: 1345.89ms, tok/sec: 97387.11, flops:42.18, batch-reuse:1
@ 11 train 7.9038 , allloss: 7.9038, norm:0.7653, dt: 1346.67ms, tok/sec: 97330.39, flops:42.16, batch-reuse:1
@ 12 train 7.7971 , allloss: 7.7971, norm:0.6086, dt: 1347.81ms, tok/sec: 97248.30, flops:42.12, batch-reuse:1
@ 13 train 7.7809 , allloss: 7.7809, norm:0.5538, dt: 1347.56ms, tok/sec: 97266.18, flops:42.13, batch-reuse:1
@ 14 train 7.7421 , allloss: 7.7421, norm:0.5180, dt: 1347.73ms, tok/sec: 97254.18, flops:42.12, batch-reuse:1
@ 15 train 7.7339 , allloss: 7.7339, norm:0.5660, dt: 1346.12ms, tok/sec: 97370.56, flops:42.17, batch-reuse:1
@ 16 train 7.7218 , allloss: 7.7218, norm:0.6344, dt: 1347.08ms, tok/sec: 97301.07, flops:42.14, batch-reuse:1
@ 17 train 7.7711 , allloss: 7.7711, norm:0.5709, dt: 1347.97ms, tok/sec: 97236.62, flops:42.12, batch-reuse:1
@ 18 train 7.7819 , allloss: 7.7819, norm:0.5731, dt: 1348.57ms, tok/sec: 97193.61, flops:42.10, batch-reuse:1
@ 19 train 7.6636 , allloss: 7.6636, norm:0.6828, dt: 1348.48ms, tok/sec: 97199.76, flops:42.10, batch-reuse:1
@ 20 train 7.7792 , allloss: 7.7792, norm:0.6865, dt: 1348.43ms, tok/sec: 97203.15, flops:42.10, batch-reuse:1
@ 21 train 7.7256 , allloss: 7.7256, norm:0.7182, dt: 1348.81ms, tok/sec: 97176.34, flops:42.09, batch-reuse:1
@ 22 train 7.6950 , allloss: 7.6950, norm:0.6031, dt: 1349.83ms, tok/sec: 97102.95, flops:42.06, batch-reuse:1
@ 23 train 7.9081 , allloss: 7.9081, norm:0.5028, dt: 1349.19ms, tok/sec: 97149.00, flops:42.08, batch-reuse:1
@ 24 train 7.8508 , allloss: 7.8508, norm:0.4395, dt: 1349.71ms, tok/sec: 97111.03, flops:42.06, batch-reuse:1
@ 25 train 7.6241 , allloss: 7.6241, norm:0.5035, dt: 1349.88ms, tok/sec: 97098.83, flops:42.06, batch-reuse:1
@ 26 train 7.7613 , allloss: 7.7613, norm:0.6404, dt: 1350.21ms, tok/sec: 97075.19, flops:42.05, batch-reuse:1
@ 27 train 7.7380 , allloss: 7.7380, norm:0.6658, dt: 1351.11ms, tok/sec: 97010.45, flops:42.02, batch-reuse:1
@ 28 train 7.7582 , allloss: 7.7582, norm:0.4887, dt: 1350.33ms, tok/sec: 97066.77, flops:42.04, batch-reuse:1
@ 29 train 7.8129 , allloss: 7.8129, norm:0.4188, dt: 1349.32ms, tok/sec: 97139.56, flops:42.07, batch-reuse:1
@ 30 train 7.7108 , allloss: 7.7108, norm:0.3857, dt: 1350.51ms, tok/sec: 97053.39, flops:42.04, batch-reuse:1
@ 31 train 7.7682 , allloss: 7.7682, norm:0.3829, dt: 1350.62ms, tok/sec: 97045.64, flops:42.03, batch-reuse:1
@ 32 train 7.7617 , allloss: 7.7617, norm:0.3951, dt: 1350.34ms, tok/sec: 97065.59, flops:42.04, batch-reuse:1
@ 33 train 7.7016 , allloss: 7.7016, norm:0.3623, dt: 1349.80ms, tok/sec: 97104.92, flops:42.06, batch-reuse:1
@ 34 train 7.7207 , allloss: 7.7207, norm:0.2787, dt: 1351.57ms, tok/sec: 96977.87, flops:42.00, batch-reuse:1
@ 35 train 7.6766 , allloss: 7.6766, norm:0.3919, dt: 1351.45ms, tok/sec: 96986.46, flops:42.01, batch-reuse:1
@ 36 train 7.6892 , allloss: 7.6892, norm:0.6196, dt: 1351.44ms, tok/sec: 96986.72, flops:42.01, batch-reuse:1
@ 37 train 7.7200 , allloss: 7.7200, norm:0.4004, dt: 1351.48ms, tok/sec: 96983.77, flops:42.01, batch-reuse:1
@ 38 train 7.6470 , allloss: 7.6470, norm:0.4828, dt: 1351.29ms, tok/sec: 96997.46, flops:42.01, batch-reuse:1
@ 39 train 7.7131 , allloss: 7.7131, norm:0.2640, dt: 1350.94ms, tok/sec: 97023.09, flops:42.02, batch-reuse:1
@ 40 train 7.7393 , allloss: 7.7393, norm:0.3284, dt: 1351.44ms, tok/sec: 96987.09, flops:42.01, batch-reuse:1
@ 41 train 7.7093 , allloss: 7.7093, norm:0.4177, dt: 1350.86ms, tok/sec: 97028.74, flops:42.03, batch-reuse:1
@ 42 train 7.6987 , allloss: 7.6987, norm:0.4793, dt: 1351.68ms, tok/sec: 96969.37, flops:42.00, batch-reuse:1
@ 43 train 7.7150 , allloss: 7.7150, norm:0.3625, dt: 1352.53ms, tok/sec: 96908.41, flops:41.97, batch-reuse:1
@ 44 train 7.7309 , allloss: 7.7309, norm:0.3468, dt: 1352.05ms, tok/sec: 96943.28, flops:41.99, batch-reuse:1
@ 45 train 7.6515 , allloss: 7.6515, norm:0.3315, dt: 1354.44ms, tok/sec: 96772.25, flops:41.91, batch-reuse:1
@ 46 train 7.7042 , allloss: 7.7042, norm:0.3149, dt: 1351.50ms, tok/sec: 96982.66, flops:42.01, batch-reuse:1
@ 47 train 7.6434 , allloss: 7.6434, norm:0.3353, dt: 1353.40ms, tok/sec: 96846.46, flops:41.95, batch-reuse:1
@ 48 train 7.6495 , allloss: 7.6495, norm:0.3469, dt: 1352.69ms, tok/sec: 96897.28, flops:41.97, batch-reuse:1
@ 49 train 7.6664 , allloss: 7.6664, norm:0.2588, dt: 1354.23ms, tok/sec: 96787.08, flops:41.92, batch-reuse:1
INFO nextres 15.593966484069824 attn*mlp 15.577953338623047 layernormed 0.9992988109588623
			attn_hist -40.5<tensor([  0.,   0.,   1.,  39., 347., 344.,  37.,   0.,   0.,   0.])>31.3125
			mlp_hist -43.75<tensor([  1.,  32.,  47.,  74., 240., 235.,  75.,  41.,  21.,   2.],
       dtype=torch.bfloat16)>40.75
			x_hist -3.2606215476989746<tensor([  0.,   0.,   0.,   0., 398., 370.,   0.,   0.,   0.,   0.])>3.1425158977508545
INFO nextres 36.93798065185547 attn*mlp 21.928754806518555 layernormed 0.999093770980835
			attn_hist -39.1875<tensor([  0.,   0.,   0.,  39., 359., 337.,  33.,   0.,   0.,   0.])>37.6875
			mlp_hist -31.375<tensor([  0.,   1.,  40.,  69., 280., 296.,  50.,  31.,   0.,   0.],
       dtype=torch.bfloat16)>29.5
			x_hist -3.1182613372802734<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>3.1952555179595947
INFO nextres 56.66511917114258 attn*mlp 19.98993492126465 layernormed 0.9990530610084534
			attn_hist -37.5<tensor([  0.,   0.,   0.,  45., 348., 341.,  34.,   0.,   0.,   0.])>38.25
			mlp_hist -27.875<tensor([  0.,   0.,  23.,  70., 300., 304.,  58.,  14.,   0.,   0.],
       dtype=torch.bfloat16)>26.25
			x_hist -3.170790910720825<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>3.2048866748809814
INFO nextres 76.00492095947266 attn*mlp 19.534189224243164 layernormed 0.9990344047546387
			attn_hist -38.0625<tensor([  0.,   0.,   0.,  46., 347., 341.,  34.,   0.,   0.,   0.])>38.4375
			mlp_hist -26.125<tensor([  0.,   0.,  15.,  69., 304., 314.,  56.,  11.,   0.,   0.],
       dtype=torch.bfloat16)>24.75
			x_hist -3.197603940963745<tensor([  0.,   0.,   0.,   0., 392., 376.,   0.,   0.,   0.,   0.])>3.208191156387329
INFO nextres 95.09929656982422 attn*mlp 19.25178337097168 layernormed 0.9990252256393433
			attn_hist -38.4375<tensor([  0.,   0.,   0.,  48., 344., 342.,  34.,   0.,   0.,   0.])>38.4375
			mlp_hist -25.0<tensor([  0.,   0.,  10.,  70., 306., 320.,  53.,  10.,   0.,   0.],
       dtype=torch.bfloat16)>23.75
			x_hist -3.2144668102264404<tensor([  0.,   0.,   0.,   0., 392., 376.,   0.,   0.,   0.,   0.])>3.2081522941589355
INFO nextres 114.01715087890625 attn*mlp 19.05164909362793 layernormed 0.9990202188491821
			attn_hist -38.625<tensor([  0.,   0.,   0.,  49., 343., 340.,  36.,   0.,   0.,   0.])>38.4375
			mlp_hist -24.25<tensor([  0.,   0.,   7.,  72., 304., 320.,  54.,   9.,   0.,   0.],
       dtype=torch.bfloat16)>23.0
			x_hist -3.226106643676758<tensor([  0.,   0.,   0.,   0., 392., 376.,   0.,   0.,   0.,   0.])>3.2067320346832275
INFO nextres 132.79937744140625 attn*mlp 18.899511337280273 layernormed 0.9990174174308777
			attn_hist -38.625<tensor([  0.,   0.,   0.,  50., 342., 339.,  37.,   0.,   0.,   0.])>38.4375
			mlp_hist -23.625<tensor([  0.,   0.,   4.,  74., 304., 324.,  55.,   7.,   0.,   0.],
       dtype=torch.bfloat16)>22.375
			x_hist -3.2331998348236084<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>3.205228567123413
INFO nextres 151.4708709716797 attn*mlp 18.776525497436523 layernormed 0.9990163445472717
			attn_hist -38.8125<tensor([  0.,   0.,   0.,  50., 343., 337.,  38.,   0.,   0.,   0.])>38.4375
			mlp_hist -23.125<tensor([  0.,   0.,   4.,  71., 308., 324.,  55.,   6.,   0.,   0.],
       dtype=torch.bfloat16)>22.0
			x_hist -3.239020347595215<tensor([  0.,   0.,   0.,   0., 390., 378.,   0.,   0.,   0.,   0.])>3.2040977478027344
INFO nextres 170.0533447265625 attn*mlp 18.678001403808594 layernormed 0.9990161061286926
			attn_hist -38.8125<tensor([  0.,   0.,   0.,  51., 339., 339.,  39.,   0.,   0.,   0.])>38.4375
			mlp_hist -22.75<tensor([  0.,   0.,   4.,  69., 308., 328.,  52.,   6.,   0.,   0.],
       dtype=torch.bfloat16)>21.625
			x_hist -3.243166208267212<tensor([  0.,   0.,   0.,   0., 387., 381.,   0.,   0.,   0.,   0.])>3.202756643295288
INFO nextres 188.55429077148438 attn*mlp 18.588876724243164 layernormed 0.999016523361206
			attn_hist -39.0<tensor([  0.,   0.,   0.,  51., 336., 342.,  39.,   0.,   0.,   0.])>38.4375
			mlp_hist -22.5<tensor([  0.,   0.,   3.,  68., 310., 330.,  52.,   5.,   0.,   0.],
       dtype=torch.bfloat16)>21.375
			x_hist -3.247563600540161<tensor([  0.,   0.,   0.,   0., 388., 380.,   0.,   0.,   0.,   0.])>3.20174241065979
INFO nextres 206.9907684326172 attn*mlp 18.51812171936035 layernormed 0.9990173578262329
			attn_hist -39.0<tensor([  0.,   0.,   0.,  53., 335., 341.,  39.,   0.,   0.,   0.])>38.4375
			mlp_hist -22.125<tensor([  0.,   0.,   3.,  67., 312., 332.,  52.,   4.,   0.,   0.],
       dtype=torch.bfloat16)>21.0
			x_hist -3.250561475753784<tensor([  0.,   0.,   0.,   0., 386., 382.,   0.,   0.,   0.,   0.])>3.200841188430786
INFO nextres 225.36727905273438 attn*mlp 18.452938079833984 layernormed 0.9990183115005493
			attn_hist -39.0<tensor([  0.,   0.,   0.,  54., 332., 341.,  41.,   0.,   0.,   0.])>38.4375
			mlp_hist -21.875<tensor([  0.,   0.,   3.,  67., 312., 330.,  53.,   3.,   0.,   0.],
       dtype=torch.bfloat16)>20.875
			x_hist -3.2528939247131348<tensor([  0.,   0.,   0.,   0., 386., 382.,   0.,   0.,   0.,   0.])>3.1998846530914307
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( of:0.1543 are:0.0923 are:0.0542,:0.0422,:0.1318,:0.2451 are:0.0669,:0.0576)
		(,:0.0457,:0.0408,:0.0435,:0.0396,:0.0576,:0.0669,:0.0498,:0.0498)
		(,:0.0391,:0.0374,:0.0388,:0.0366,:0.0457,:0.0508,:0.0425,:0.0437)
		(,:0.0361,:0.0344,:0.0361,:0.0347,:0.0415,:0.0449,:0.0393,:0.0393)
		(,:0.0339,:0.0332,:0.0339,:0.0334,:0.0381,:0.0400,:0.0369,:0.0369)
		(,:0.0327,:0.0320,:0.0327,:0.0320,:0.0354,:0.0376,:0.0347,:0.0356)
		(,:0.0312,:0.0315,:0.0312,:0.0315,:0.0342,:0.0359,:0.0342,:0.0342)
		(,:0.0308,:0.0299,:0.0308,:0.0299,:0.0325,:0.0344,:0.0325,:0.0325)
		(.:0.0302,:0.0294,:0.0302,:0.0294,:0.0320,:0.0327,:0.0320,:0.0320)
		(.:0.0295,:0.0287,:0.0295,:0.0287,:0.0312,:0.0320,:0.0303,:0.0312)
		(,:0.0289,:0.0281,:0.0289,:0.0281,:0.0306,:0.0315,:0.0297,:0.0305)
		(,:0.0282,:0.0283,:0.0282,:0.0283,:0.0299,:0.0308,:0.0299,:0.0299)
		(,:0.0282,:0.0283,:0.0282,:0.0283,:0.0299,:0.0308,:0.0299,:0.0299)
,
------
		( are:0.0923 are:0.0542,:0.0422,:0.1318,:0.2451 are:0.0669,:0.0576,:0.0452)
		(,:0.0408,:0.0435,:0.0396,:0.0576,:0.0669,:0.0498,:0.0498,:0.0449)
		(,:0.0374,:0.0388,:0.0366,:0.0457,:0.0508,:0.0425,:0.0437,:0.0408)
		(,:0.0344,:0.0361,:0.0347,:0.0415,:0.0449,:0.0393,:0.0393,:0.0386)
		(,:0.0332,:0.0339,:0.0334,:0.0381,:0.0400,:0.0369,:0.0369,:0.0361)
		(,:0.0320,:0.0327,:0.0320,:0.0354,:0.0376,:0.0347,:0.0356,:0.0347)
		(,:0.0315,:0.0312,:0.0315,:0.0342,:0.0359,:0.0342,:0.0342,:0.0332)
		(,:0.0299,:0.0308,:0.0299,:0.0325,:0.0344,:0.0325,:0.0325,:0.0327)
		(,:0.0294,:0.0302,:0.0294,:0.0320,:0.0327,:0.0320,:0.0320,:0.0310)
		(,:0.0287,:0.0295,:0.0287,:0.0312,:0.0320,:0.0303,:0.0312,:0.0304)
		(,:0.0281,:0.0289,:0.0281,:0.0306,:0.0315,:0.0297,:0.0305,:0.0297)
		(,:0.0283,:0.0282,:0.0283,:0.0299,:0.0308,:0.0299,:0.0299,:0.0291)
		(,:0.0283,:0.0282,:0.0283,:0.0299,:0.0308,:0.0299,:0.0299,:0.0291)
,,,,,,,,,,,,,,,
@ 50 train 7.7712 , allloss: 7.7712, norm:0.2958, dt: 1918.70ms, tok/sec: 68313.08, flops:29.59, batch-reuse:1
@ 51 train 7.8328 , allloss: 7.8328, norm:0.3970, dt: 1352.05ms, tok/sec: 96943.17, flops:41.99, batch-reuse:1
@ 52 train 7.6797 , allloss: 7.6797, norm:0.4699, dt: 1351.48ms, tok/sec: 96983.91, flops:42.01, batch-reuse:1
@ 53 train 7.6540 , allloss: 7.6540, norm:0.2549, dt: 1352.73ms, tok/sec: 96894.70, flops:41.97, batch-reuse:1
@ 54 train 7.7500 , allloss: 7.7500, norm:0.4053, dt: 1355.20ms, tok/sec: 96717.64, flops:41.89, batch-reuse:1
@ 55 train 7.7948 , allloss: 7.7948, norm:0.3438, dt: 1353.33ms, tok/sec: 96851.49, flops:41.95, batch-reuse:1
@ 56 train 7.6589 , allloss: 7.6589, norm:0.4141, dt: 1352.27ms, tok/sec: 96927.19, flops:41.98, batch-reuse:1
@ 57 train 7.5899 , allloss: 7.5899, norm:0.3299, dt: 1355.39ms, tok/sec: 96704.33, flops:41.89, batch-reuse:1
@ 58 train 7.6613 , allloss: 7.6613, norm:0.3860, dt: 1352.10ms, tok/sec: 96939.26, flops:41.99, batch-reuse:1
@ 59 train 7.4733 , allloss: 7.4733, norm:0.5096, dt: 1351.57ms, tok/sec: 96977.63, flops:42.00, batch-reuse:1
@ 60 train 7.6635 , allloss: 7.6635, norm:0.3561, dt: 1351.92ms, tok/sec: 96952.18, flops:41.99, batch-reuse:1
@ 61 train 7.6242 , allloss: 7.6242, norm:0.3546, dt: 1352.21ms, tok/sec: 96931.98, flops:41.98, batch-reuse:1
@ 62 train 7.5773 , allloss: 7.5773, norm:0.7557, dt: 1352.52ms, tok/sec: 96909.70, flops:41.97, batch-reuse:1
@ 63 train 7.6301 , allloss: 7.6301, norm:0.3376, dt: 1352.32ms, tok/sec: 96923.45, flops:41.98, batch-reuse:1
@ 64 train 7.6244 , allloss: 7.6244, norm:0.3208, dt: 1351.10ms, tok/sec: 97011.58, flops:42.02, batch-reuse:1
@ 65 train 7.7056 , allloss: 7.7056, norm:0.3723, dt: 1352.53ms, tok/sec: 96908.65, flops:41.97, batch-reuse:1
@ 66 train 7.7417 , allloss: 7.7417, norm:0.3243, dt: 1351.88ms, tok/sec: 96955.67, flops:41.99, batch-reuse:1
@ 67 train 7.6357 , allloss: 7.6357, norm:0.3445, dt: 1352.69ms, tok/sec: 96897.36, flops:41.97, batch-reuse:1
@ 68 train 7.6892 , allloss: 7.6892, norm:0.4182, dt: 1354.42ms, tok/sec: 96773.72, flops:41.92, batch-reuse:1
@ 69 train 7.5951 , allloss: 7.5951, norm:0.2785, dt: 1353.99ms, tok/sec: 96804.05, flops:41.93, batch-reuse:1
@ 70 train 7.6325 , allloss: 7.6325, norm:0.4109, dt: 1351.99ms, tok/sec: 96947.58, flops:41.99, batch-reuse:1
@ 71 train 7.5955 , allloss: 7.5955, norm:0.2656, dt: 1352.67ms, tok/sec: 96898.51, flops:41.97, batch-reuse:1
@ 72 train 7.6308 , allloss: 7.6308, norm:0.3072, dt: 1352.53ms, tok/sec: 96908.64, flops:41.97, batch-reuse:1
@ 73 train 7.6989 , allloss: 7.6989, norm:0.4733, dt: 1351.92ms, tok/sec: 96952.61, flops:41.99, batch-reuse:1
@ 74 train 7.6635 , allloss: 7.6635, norm:0.3232, dt: 1351.46ms, tok/sec: 96985.57, flops:42.01, batch-reuse:1
@ 75 train 8.1625 , allloss: 8.1625, norm:0.6699, dt: 1356.40ms, tok/sec: 96632.55, flops:41.85, batch-reuse:1
@ 76 train 7.6087 , allloss: 7.6087, norm:0.5453, dt: 1353.58ms, tok/sec: 96833.72, flops:41.94, batch-reuse:1
@ 77 train 8.0664 , allloss: 8.0664, norm:1.8063, dt: 1352.34ms, tok/sec: 96922.17, flops:41.98, batch-reuse:1
@ 78 train 7.7539 , allloss: 7.7539, norm:0.9404, dt: 1352.03ms, tok/sec: 96944.85, flops:41.99, batch-reuse:1
@ 79 train 7.5884 , allloss: 7.5884, norm:0.3314, dt: 1352.79ms, tok/sec: 96889.97, flops:41.97, batch-reuse:1
@ 80 train 7.6277 , allloss: 7.6277, norm:0.4520, dt: 1353.80ms, tok/sec: 96817.88, flops:41.93, batch-reuse:1
@ 81 train 7.6422 , allloss: 7.6422, norm:0.4208, dt: 1353.51ms, tok/sec: 96838.87, flops:41.94, batch-reuse:1
@ 82 train 7.6133 , allloss: 7.6133, norm:0.4327, dt: 1352.24ms, tok/sec: 96929.52, flops:41.98, batch-reuse:1
@ 83 train 7.6038 , allloss: 7.6038, norm:0.3525, dt: 1352.72ms, tok/sec: 96895.50, flops:41.97, batch-reuse:1
@ 84 train 7.6085 , allloss: 7.6085, norm:0.4480, dt: 1353.12ms, tok/sec: 96866.43, flops:41.96, batch-reuse:1
@ 85 train 7.5682 , allloss: 7.5682, norm:0.3783, dt: 1353.26ms, tok/sec: 96856.31, flops:41.95, batch-reuse:1
@ 86 train 7.5599 , allloss: 7.5599, norm:0.4869, dt: 1354.06ms, tok/sec: 96799.43, flops:41.93, batch-reuse:1
@ 87 train 7.5952 , allloss: 7.5952, norm:0.4419, dt: 1352.56ms, tok/sec: 96906.31, flops:41.97, batch-reuse:1
@ 88 train 7.5974 , allloss: 7.5974, norm:0.6497, dt: 1351.85ms, tok/sec: 96957.55, flops:42.00, batch-reuse:1
@ 89 train 7.5439 , allloss: 7.5439, norm:0.3762, dt: 1352.81ms, tok/sec: 96888.91, flops:41.97, batch-reuse:1
@ 90 train 7.4998 , allloss: 7.4998, norm:0.6218, dt: 1352.99ms, tok/sec: 96875.81, flops:41.96, batch-reuse:1
@ 91 train 7.5277 , allloss: 7.5277, norm:0.3656, dt: 1352.32ms, tok/sec: 96923.60, flops:41.98, batch-reuse:1
@ 92 train 7.5330 , allloss: 7.5330, norm:0.6344, dt: 1353.45ms, tok/sec: 96842.57, flops:41.95, batch-reuse:1
@ 93 train 7.4937 , allloss: 7.4937, norm:0.3718, dt: 1353.30ms, tok/sec: 96853.30, flops:41.95, batch-reuse:1
@ 94 train 7.6490 , allloss: 7.6490, norm:0.5652, dt: 1353.10ms, tok/sec: 96868.25, flops:41.96, batch-reuse:1
@ 95 train 7.5355 , allloss: 7.5355, norm:0.4618, dt: 1354.14ms, tok/sec: 96793.64, flops:41.92, batch-reuse:1
@ 96 train 7.4509 , allloss: 7.4509, norm:0.4216, dt: 1353.32ms, tok/sec: 96852.02, flops:41.95, batch-reuse:1
@ 97 train 7.4752 , allloss: 7.4752, norm:0.5541, dt: 1353.62ms, tok/sec: 96830.89, flops:41.94, batch-reuse:1
@ 98 train 7.4968 , allloss: 7.4968, norm:0.4448, dt: 1354.62ms, tok/sec: 96759.12, flops:41.91, batch-reuse:1
@ 99 train 7.4665 , allloss: 7.4665, norm:0.4536, dt: 1353.23ms, tok/sec: 96858.66, flops:41.95, batch-reuse:1
@ 100 train 7.4371 , allloss: 7.4371, norm:0.4331, dt: 1353.40ms, tok/sec: 96846.48, flops:41.95, batch-reuse:1
@ 101 train 7.3361 , allloss: 7.3361, norm:0.3744, dt: 1354.74ms, tok/sec: 96750.42, flops:41.91, batch-reuse:1
@ 102 train 7.4274 , allloss: 7.4274, norm:0.6141, dt: 1355.15ms, tok/sec: 96721.55, flops:41.89, batch-reuse:1
@ 103 train 7.3935 , allloss: 7.3935, norm:0.3508, dt: 1356.20ms, tok/sec: 96646.29, flops:41.86, batch-reuse:1
@ 104 train 7.4493 , allloss: 7.4493, norm:0.3734, dt: 1356.78ms, tok/sec: 96605.08, flops:41.84, batch-reuse:1
@ 105 train 7.4417 , allloss: 7.4417, norm:0.3941, dt: 1355.74ms, tok/sec: 96679.28, flops:41.87, batch-reuse:1
@ 106 train 7.3868 , allloss: 7.3868, norm:0.4428, dt: 1355.63ms, tok/sec: 96687.39, flops:41.88, batch-reuse:1
@ 107 train 7.3635 , allloss: 7.3635, norm:0.5983, dt: 1354.09ms, tok/sec: 96797.42, flops:41.93, batch-reuse:1
@ 108 train 7.3322 , allloss: 7.3322, norm:0.7409, dt: 1356.19ms, tok/sec: 96646.97, flops:41.86, batch-reuse:1
@ 109 train 7.2919 , allloss: 7.2919, norm:0.6300, dt: 1356.64ms, tok/sec: 96615.11, flops:41.85, batch-reuse:1
@ 110 train 7.3622 , allloss: 7.3622, norm:0.7370, dt: 1355.28ms, tok/sec: 96712.45, flops:41.89, batch-reuse:1
@ 111 train 7.3459 , allloss: 7.3459, norm:0.4705, dt: 1357.63ms, tok/sec: 96544.73, flops:41.82, batch-reuse:1
@ 112 train 7.3512 , allloss: 7.3512, norm:0.4747, dt: 1356.59ms, tok/sec: 96618.62, flops:41.85, batch-reuse:1
@ 113 train 7.3145 , allloss: 7.3145, norm:0.7522, dt: 1356.19ms, tok/sec: 96647.57, flops:41.86, batch-reuse:1
@ 114 train 7.2961 , allloss: 7.2961, norm:0.4250, dt: 1357.04ms, tok/sec: 96587.03, flops:41.83, batch-reuse:1
@ 115 train 7.2298 , allloss: 7.2298, norm:0.5149, dt: 1357.73ms, tok/sec: 96537.75, flops:41.81, batch-reuse:1
@ 116 train 7.2042 , allloss: 7.2042, norm:0.5419, dt: 1356.47ms, tok/sec: 96627.27, flops:41.85, batch-reuse:1
@ 117 train 7.2076 , allloss: 7.2076, norm:0.3086, dt: 1358.06ms, tok/sec: 96513.80, flops:41.80, batch-reuse:1
@ 118 train 7.2115 , allloss: 7.2115, norm:0.6035, dt: 1357.41ms, tok/sec: 96560.43, flops:41.82, batch-reuse:1
@ 119 train 7.1220 , allloss: 7.1220, norm:0.5966, dt: 1356.26ms, tok/sec: 96642.16, flops:41.86, batch-reuse:1
@ 120 train 7.2873 , allloss: 7.2873, norm:0.4657, dt: 1357.28ms, tok/sec: 96569.88, flops:41.83, batch-reuse:1
@ 121 train 7.3170 , allloss: 7.3170, norm:0.6436, dt: 1358.81ms, tok/sec: 96460.61, flops:41.78, batch-reuse:1
@ 122 train 7.2743 , allloss: 7.2743, norm:0.3778, dt: 1358.15ms, tok/sec: 96507.90, flops:41.80, batch-reuse:1
@ 123 train 7.1600 , allloss: 7.1600, norm:0.6486, dt: 1358.52ms, tok/sec: 96481.34, flops:41.79, batch-reuse:1
@ 124 train 7.1817 , allloss: 7.1817, norm:0.3228, dt: 1358.90ms, tok/sec: 96454.38, flops:41.78, batch-reuse:1
@ 125 train 7.1864 , allloss: 7.1864, norm:0.3406, dt: 1361.01ms, tok/sec: 96305.08, flops:41.71, batch-reuse:1
@ 126 train 7.1506 , allloss: 7.1506, norm:0.3881, dt: 1359.00ms, tok/sec: 96447.73, flops:41.77, batch-reuse:1
@ 127 train 7.1010 , allloss: 7.1010, norm:0.3616, dt: 1359.98ms, tok/sec: 96377.63, flops:41.74, batch-reuse:1
@ 128 train 7.0613 , allloss: 7.0613, norm:0.3911, dt: 1361.98ms, tok/sec: 96236.04, flops:41.68, batch-reuse:1
@ 129 train 7.0674 , allloss: 7.0674, norm:0.4356, dt: 1361.22ms, tok/sec: 96290.13, flops:41.71, batch-reuse:1
@ 130 train 7.1476 , allloss: 7.1476, norm:0.4956, dt: 1360.17ms, tok/sec: 96364.45, flops:41.74, batch-reuse:1
@ 131 train 7.1039 , allloss: 7.1039, norm:0.3234, dt: 1362.46ms, tok/sec: 96202.11, flops:41.67, batch-reuse:1
@ 132 train 7.1322 , allloss: 7.1322, norm:0.5916, dt: 1362.92ms, tok/sec: 96169.92, flops:41.65, batch-reuse:1
@ 133 train 7.0460 , allloss: 7.0460, norm:0.6087, dt: 1362.14ms, tok/sec: 96224.81, flops:41.68, batch-reuse:1
@ 134 train 7.0780 , allloss: 7.0780, norm:0.4562, dt: 1361.65ms, tok/sec: 96259.99, flops:41.69, batch-reuse:1
@ 135 train 6.9896 , allloss: 6.9896, norm:0.5368, dt: 1362.16ms, tok/sec: 96223.29, flops:41.68, batch-reuse:1
@ 136 train 7.0109 , allloss: 7.0109, norm:0.3891, dt: 1364.72ms, tok/sec: 96042.80, flops:41.60, batch-reuse:1
@ 137 train 6.9332 , allloss: 6.9332, norm:0.5471, dt: 1363.92ms, tok/sec: 96099.73, flops:41.62, batch-reuse:1
@ 138 train 6.9034 , allloss: 6.9034, norm:0.4357, dt: 1363.86ms, tok/sec: 96103.88, flops:41.63, batch-reuse:1
@ 139 train 6.8789 , allloss: 6.8789, norm:0.5166, dt: 1362.50ms, tok/sec: 96199.62, flops:41.67, batch-reuse:1
@ 140 train 6.8910 , allloss: 6.8910, norm:0.8037, dt: 1364.24ms, tok/sec: 96076.84, flops:41.61, batch-reuse:1
@ 141 train 6.8809 , allloss: 6.8809, norm:0.4322, dt: 1364.28ms, tok/sec: 96074.19, flops:41.61, batch-reuse:1
@ 142 train 6.9614 , allloss: 6.9614, norm:0.4457, dt: 1362.71ms, tok/sec: 96184.83, flops:41.66, batch-reuse:1
@ 143 train 6.9119 , allloss: 6.9119, norm:0.4133, dt: 1364.78ms, tok/sec: 96038.71, flops:41.60, batch-reuse:1
@ 144 train 6.8769 , allloss: 6.8769, norm:0.4814, dt: 1364.95ms, tok/sec: 96027.15, flops:41.59, batch-reuse:1
@ 145 train 6.8821 , allloss: 6.8821, norm:0.3610, dt: 1365.06ms, tok/sec: 96019.27, flops:41.59, batch-reuse:1
@ 146 train 7.0102 , allloss: 7.0102, norm:0.3554, dt: 1365.04ms, tok/sec: 96020.81, flops:41.59, batch-reuse:1
@ 147 train 6.8697 , allloss: 6.8697, norm:0.3954, dt: 1366.09ms, tok/sec: 95946.82, flops:41.56, batch-reuse:1
@ 148 train 6.8474 , allloss: 6.8474, norm:0.3399, dt: 1365.70ms, tok/sec: 95974.51, flops:41.57, batch-reuse:1
@ 149 train 6.7136 , allloss: 6.7136, norm:0.4575, dt: 1366.29ms, tok/sec: 95933.03, flops:41.55, batch-reuse:1
@ 150 train 6.9352 , allloss: 6.9352, norm:0.3859, dt: 1367.29ms, tok/sec: 95862.83, flops:41.52, batch-reuse:1
@ 151 train 6.7118 , allloss: 6.7118, norm:0.4577, dt: 1367.37ms, tok/sec: 95856.93, flops:41.52, batch-reuse:1
@ 152 train 6.7365 , allloss: 6.7365, norm:0.5798, dt: 1367.98ms, tok/sec: 95814.18, flops:41.50, batch-reuse:1
@ 153 train 6.6921 , allloss: 6.6921, norm:0.5304, dt: 1368.55ms, tok/sec: 95774.22, flops:41.48, batch-reuse:1
@ 154 train 6.8016 , allloss: 6.8016, norm:0.5418, dt: 1369.01ms, tok/sec: 95742.35, flops:41.47, batch-reuse:1
@ 155 train 6.9250 , allloss: 6.9250, norm:0.3904, dt: 1369.34ms, tok/sec: 95719.14, flops:41.46, batch-reuse:1
@ 156 train 6.7792 , allloss: 6.7792, norm:0.5589, dt: 1369.75ms, tok/sec: 95690.49, flops:41.45, batch-reuse:1
@ 157 train 6.8408 , allloss: 6.8408, norm:0.3546, dt: 1368.09ms, tok/sec: 95806.90, flops:41.50, batch-reuse:1
@ 158 train 6.8315 , allloss: 6.8315, norm:0.3659, dt: 1369.51ms, tok/sec: 95707.09, flops:41.45, batch-reuse:1
@ 159 train 6.8372 , allloss: 6.8372, norm:0.3762, dt: 1372.01ms, tok/sec: 95532.90, flops:41.38, batch-reuse:1
@ 160 train 6.7477 , allloss: 6.7477, norm:0.3300, dt: 1374.00ms, tok/sec: 95394.20, flops:41.32, batch-reuse:1
@ 161 train 6.7398 , allloss: 6.7398, norm:0.2810, dt: 1373.90ms, tok/sec: 95401.20, flops:41.32, batch-reuse:1
@ 162 train 6.8395 , allloss: 6.8395, norm:0.3888, dt: 1373.64ms, tok/sec: 95419.20, flops:41.33, batch-reuse:1
@ 163 train 6.7421 , allloss: 6.7421, norm:0.3126, dt: 1373.87ms, tok/sec: 95403.25, flops:41.32, batch-reuse:1
@ 164 train 6.6798 , allloss: 6.6798, norm:0.4242, dt: 1372.42ms, tok/sec: 95504.32, flops:41.37, batch-reuse:1
@ 165 train 6.7859 , allloss: 6.7859, norm:0.4357, dt: 1375.18ms, tok/sec: 95312.60, flops:41.28, batch-reuse:1
@ 166 train 6.7333 , allloss: 6.7333, norm:0.6348, dt: 1375.45ms, tok/sec: 95293.61, flops:41.27, batch-reuse:1
@ 167 train 6.7484 , allloss: 6.7484, norm:0.4750, dt: 1375.11ms, tok/sec: 95317.54, flops:41.28, batch-reuse:1
@ 168 train 6.6482 , allloss: 6.6482, norm:0.4525, dt: 1375.16ms, tok/sec: 95313.79, flops:41.28, batch-reuse:1
@ 169 train 6.7031 , allloss: 6.7031, norm:0.4359, dt: 1376.57ms, tok/sec: 95216.45, flops:41.24, batch-reuse:1
@ 170 train 6.6807 , allloss: 6.6807, norm:0.5682, dt: 1375.92ms, tok/sec: 95261.02, flops:41.26, batch-reuse:1
@ 171 train 6.6357 , allloss: 6.6357, norm:0.3706, dt: 1375.81ms, tok/sec: 95269.07, flops:41.26, batch-reuse:1
@ 172 train 6.6864 , allloss: 6.6864, norm:0.5200, dt: 1377.28ms, tok/sec: 95167.42, flops:41.22, batch-reuse:1
@ 173 train 6.6604 , allloss: 6.6604, norm:0.3647, dt: 1374.55ms, tok/sec: 95356.31, flops:41.30, batch-reuse:1
@ 174 train 6.7924 , allloss: 6.7924, norm:0.4668, dt: 1374.47ms, tok/sec: 95362.16, flops:41.30, batch-reuse:1
@ 175 train 6.5907 , allloss: 6.5907, norm:1.1248, dt: 1376.28ms, tok/sec: 95236.26, flops:41.25, batch-reuse:1
@ 176 train 6.6994 , allloss: 6.6994, norm:0.4080, dt: 1378.18ms, tok/sec: 95104.96, flops:41.19, batch-reuse:1
@ 177 train 6.6457 , allloss: 6.6457, norm:0.5199, dt: 1377.64ms, tok/sec: 95142.75, flops:41.21, batch-reuse:1
@ 178 train 6.6919 , allloss: 6.6919, norm:0.3036, dt: 1378.17ms, tok/sec: 95106.07, flops:41.19, batch-reuse:1
@ 179 train 6.5993 , allloss: 6.5993, norm:0.5434, dt: 1376.78ms, tok/sec: 95201.78, flops:41.23, batch-reuse:1
@ 180 train 6.6835 , allloss: 6.6835, norm:0.4419, dt: 1376.75ms, tok/sec: 95204.10, flops:41.24, batch-reuse:1
@ 181 train 6.5478 , allloss: 6.5478, norm:0.4777, dt: 1377.21ms, tok/sec: 95172.33, flops:41.22, batch-reuse:1
@ 182 train 6.6764 , allloss: 6.6764, norm:0.4494, dt: 1378.74ms, tok/sec: 95066.83, flops:41.18, batch-reuse:1
@ 183 train 6.6303 , allloss: 6.6303, norm:0.3460, dt: 1377.65ms, tok/sec: 95142.04, flops:41.21, batch-reuse:1
@ 184 train 6.5170 , allloss: 6.5170, norm:0.3760, dt: 1376.59ms, tok/sec: 95215.09, flops:41.24, batch-reuse:1
@ 185 train 6.5615 , allloss: 6.5615, norm:0.5238, dt: 1377.24ms, tok/sec: 95170.12, flops:41.22, batch-reuse:1
@ 186 train 6.7251 , allloss: 6.7251, norm:0.4038, dt: 1375.24ms, tok/sec: 95308.22, flops:41.28, batch-reuse:1
@ 187 train 6.8345 , allloss: 6.8345, norm:0.5324, dt: 1376.66ms, tok/sec: 95210.14, flops:41.24, batch-reuse:1
@ 188 train 6.6597 , allloss: 6.6597, norm:0.3566, dt: 1377.16ms, tok/sec: 95175.51, flops:41.22, batch-reuse:1
@ 189 train 6.7901 , allloss: 6.7901, norm:0.4299, dt: 1377.80ms, tok/sec: 95131.70, flops:41.20, batch-reuse:1
@ 190 train 6.7559 , allloss: 6.7559, norm:0.4511, dt: 1377.58ms, tok/sec: 95146.60, flops:41.21, batch-reuse:1
@ 191 train 6.7354 , allloss: 6.7354, norm:0.3907, dt: 1378.06ms, tok/sec: 95113.38, flops:41.20, batch-reuse:1
@ 192 train 6.7763 , allloss: 6.7763, norm:0.4348, dt: 1379.30ms, tok/sec: 95027.77, flops:41.16, batch-reuse:1
@ 193 train 6.6504 , allloss: 6.6504, norm:0.4741, dt: 1377.85ms, tok/sec: 95128.23, flops:41.20, batch-reuse:1
@ 194 train 6.7643 , allloss: 6.7643, norm:0.6502, dt: 1377.20ms, tok/sec: 95172.77, flops:41.22, batch-reuse:1
@ 195 train 6.7199 , allloss: 6.7199, norm:0.4296, dt: 1379.51ms, tok/sec: 95013.25, flops:41.15, batch-reuse:1
@ 196 train 6.6470 , allloss: 6.6470, norm:0.6842, dt: 1378.15ms, tok/sec: 95107.33, flops:41.19, batch-reuse:1
@ 197 train 6.6688 , allloss: 6.6688, norm:0.4856, dt: 1377.63ms, tok/sec: 95143.17, flops:41.21, batch-reuse:1
@ 198 train 6.6167 , allloss: 6.6167, norm:0.5566, dt: 1377.27ms, tok/sec: 95168.21, flops:41.22, batch-reuse:1
@ 199 train 6.6793 , allloss: 6.6793, norm:0.4917, dt: 1377.08ms, tok/sec: 95181.26, flops:41.23, batch-reuse:1
@ 200 train 6.6893 , allloss: 6.6893, norm:0.3632, dt: 1379.27ms, tok/sec: 95029.65, flops:41.16, batch-reuse:1
@ 201 train 6.6470 , allloss: 6.6470, norm:0.4645, dt: 1377.59ms, tok/sec: 95145.96, flops:41.21, batch-reuse:1
@ 202 train 6.7407 , allloss: 6.7407, norm:0.4206, dt: 1380.40ms, tok/sec: 94952.44, flops:41.13, batch-reuse:1
@ 203 train 6.6477 , allloss: 6.6477, norm:0.5128, dt: 1380.51ms, tok/sec: 94944.45, flops:41.12, batch-reuse:1
@ 204 train 6.6283 , allloss: 6.6283, norm:0.3736, dt: 1380.58ms, tok/sec: 94939.96, flops:41.12, batch-reuse:1
@ 205 train 6.6235 , allloss: 6.6235, norm:0.4757, dt: 1381.84ms, tok/sec: 94853.53, flops:41.08, batch-reuse:1
@ 206 train 6.6766 , allloss: 6.6766, norm:0.3335, dt: 1379.21ms, tok/sec: 95033.92, flops:41.16, batch-reuse:1
@ 207 train 6.5688 , allloss: 6.5688, norm:0.5998, dt: 1379.03ms, tok/sec: 95046.68, flops:41.17, batch-reuse:1
@ 208 train 6.6276 , allloss: 6.6276, norm:0.5101, dt: 1378.23ms, tok/sec: 95101.65, flops:41.19, batch-reuse:1
@ 209 train 6.6586 , allloss: 6.6586, norm:0.3652, dt: 1378.62ms, tok/sec: 95074.82, flops:41.18, batch-reuse:1
@ 210 train 6.6135 , allloss: 6.6135, norm:0.5277, dt: 1378.60ms, tok/sec: 95076.34, flops:41.18, batch-reuse:1
@ 211 train 6.5597 , allloss: 6.5597, norm:0.3200, dt: 1378.17ms, tok/sec: 95105.73, flops:41.19, batch-reuse:1
@ 212 train 6.6276 , allloss: 6.6276, norm:0.3957, dt: 1379.00ms, tok/sec: 95048.57, flops:41.17, batch-reuse:1
@ 213 train 6.5289 , allloss: 6.5289, norm:0.3626, dt: 1380.48ms, tok/sec: 94946.40, flops:41.12, batch-reuse:1
@ 214 train 6.5413 , allloss: 6.5413, norm:0.2923, dt: 1378.81ms, tok/sec: 95061.98, flops:41.17, batch-reuse:1
@ 215 train 6.4605 , allloss: 6.4605, norm:0.3445, dt: 1378.64ms, tok/sec: 95073.28, flops:41.18, batch-reuse:1
@ 216 train 6.4713 , allloss: 6.4713, norm:0.3187, dt: 1378.34ms, tok/sec: 95094.16, flops:41.19, batch-reuse:1
@ 217 train 6.5520 , allloss: 6.5520, norm:0.2916, dt: 1378.31ms, tok/sec: 95096.29, flops:41.19, batch-reuse:1
@ 218 train 6.4638 , allloss: 6.4638, norm:0.3647, dt: 1377.45ms, tok/sec: 95155.51, flops:41.21, batch-reuse:1
@ 219 train 6.6359 , allloss: 6.6359, norm:0.3600, dt: 1380.44ms, tok/sec: 94949.57, flops:41.13, batch-reuse:1
@ 220 train 6.5681 , allloss: 6.5681, norm:0.3333, dt: 1379.51ms, tok/sec: 95013.27, flops:41.15, batch-reuse:1
@ 221 train 6.5566 , allloss: 6.5566, norm:0.4784, dt: 1376.99ms, tok/sec: 95187.06, flops:41.23, batch-reuse:1
@ 222 train 6.5243 , allloss: 6.5243, norm:0.4338, dt: 1377.83ms, tok/sec: 95129.48, flops:41.20, batch-reuse:1
@ 223 train 6.5166 , allloss: 6.5166, norm:0.3685, dt: 1378.42ms, tok/sec: 95088.60, flops:41.19, batch-reuse:1
@ 224 train 6.4215 , allloss: 6.4215, norm:0.4327, dt: 1378.20ms, tok/sec: 95103.89, flops:41.19, batch-reuse:1
@ 225 train 6.5141 , allloss: 6.5141, norm:0.3318, dt: 1378.91ms, tok/sec: 95054.70, flops:41.17, batch-reuse:1
@ 226 train 6.5343 , allloss: 6.5343, norm:0.4109, dt: 1377.20ms, tok/sec: 95173.05, flops:41.22, batch-reuse:1
@ 227 train 6.5120 , allloss: 6.5120, norm:0.3489, dt: 1379.14ms, tok/sec: 95039.12, flops:41.16, batch-reuse:1
@ 228 train 6.5102 , allloss: 6.5102, norm:0.2772, dt: 1378.50ms, tok/sec: 95083.08, flops:41.18, batch-reuse:1
@ 229 train 6.5153 , allloss: 6.5153, norm:0.3307, dt: 1378.34ms, tok/sec: 95093.90, flops:41.19, batch-reuse:1
@ 230 train 6.5015 , allloss: 6.5015, norm:0.3435, dt: 1378.26ms, tok/sec: 95099.94, flops:41.19, batch-reuse:1
@ 231 train 6.4328 , allloss: 6.4328, norm:0.2924, dt: 1378.53ms, tok/sec: 95081.12, flops:41.18, batch-reuse:1
@ 232 train 6.3774 , allloss: 6.3774, norm:0.3476, dt: 1377.23ms, tok/sec: 95170.65, flops:41.22, batch-reuse:1
@ 233 train 6.4848 , allloss: 6.4848, norm:0.3589, dt: 1377.15ms, tok/sec: 95175.95, flops:41.22, batch-reuse:1
@ 234 train 6.3470 , allloss: 6.3470, norm:0.4008, dt: 1377.40ms, tok/sec: 95159.17, flops:41.22, batch-reuse:1
@ 235 train 6.4468 , allloss: 6.4468, norm:0.3642, dt: 1377.27ms, tok/sec: 95167.76, flops:41.22, batch-reuse:1
@ 236 train 6.3258 , allloss: 6.3258, norm:0.4252, dt: 1377.52ms, tok/sec: 95150.68, flops:41.21, batch-reuse:1
@ 237 train 6.4152 , allloss: 6.4152, norm:0.2795, dt: 1378.04ms, tok/sec: 95114.98, flops:41.20, batch-reuse:1
@ 238 train 6.3693 , allloss: 6.3693, norm:0.4207, dt: 1377.78ms, tok/sec: 95132.82, flops:41.20, batch-reuse:1
@ 239 train 6.3774 , allloss: 6.3774, norm:0.3919, dt: 1377.10ms, tok/sec: 95179.79, flops:41.23, batch-reuse:1
@ 240 train 6.4324 , allloss: 6.4324, norm:0.3622, dt: 1377.76ms, tok/sec: 95133.82, flops:41.21, batch-reuse:1
@ 241 train 6.4110 , allloss: 6.4110, norm:0.2995, dt: 1377.00ms, tok/sec: 95186.80, flops:41.23, batch-reuse:1
@ 242 train 6.3745 , allloss: 6.3745, norm:0.3499, dt: 1376.21ms, tok/sec: 95241.02, flops:41.25, batch-reuse:1
@ 243 train 6.2975 , allloss: 6.2975, norm:0.3438, dt: 1376.09ms, tok/sec: 95249.47, flops:41.26, batch-reuse:1
@ 244 train 6.4299 , allloss: 6.4299, norm:0.3193, dt: 1375.65ms, tok/sec: 95280.05, flops:41.27, batch-reuse:1
@ 245 train 6.1708 , allloss: 6.1708, norm:0.3678, dt: 1375.64ms, tok/sec: 95280.85, flops:41.27, batch-reuse:1
@ 246 train 6.4692 , allloss: 6.4692, norm:0.4241, dt: 1376.41ms, tok/sec: 95227.44, flops:41.25, batch-reuse:1
@ 247 train 6.3692 , allloss: 6.3692, norm:0.4145, dt: 1376.89ms, tok/sec: 95194.16, flops:41.23, batch-reuse:1
@ 248 train 6.3102 , allloss: 6.3102, norm:0.2820, dt: 1376.49ms, tok/sec: 95222.21, flops:41.24, batch-reuse:1
@ 249 train 6.2883 , allloss: 6.2883, norm:0.4274, dt: 1375.57ms, tok/sec: 95285.45, flops:41.27, batch-reuse:1
@ 250 train 6.3498 , allloss: 6.3498, norm:0.3271, dt: 1375.60ms, tok/sec: 95283.42, flops:41.27, batch-reuse:1
@ 251 train 6.2981 , allloss: 6.2981, norm:0.4008, dt: 1375.65ms, tok/sec: 95280.25, flops:41.27, batch-reuse:1
@ 252 train 6.4196 , allloss: 6.4196, norm:0.5272, dt: 1375.11ms, tok/sec: 95317.32, flops:41.28, batch-reuse:1
@ 253 train 6.2949 , allloss: 6.2949, norm:0.3070, dt: 1375.82ms, tok/sec: 95267.97, flops:41.26, batch-reuse:1
@ 254 train 6.2486 , allloss: 6.2486, norm:0.3522, dt: 1375.01ms, tok/sec: 95324.58, flops:41.29, batch-reuse:1
@ 255 train 6.2414 , allloss: 6.2414, norm:0.3164, dt: 1374.61ms, tok/sec: 95352.02, flops:41.30, batch-reuse:1
@ 256 train 6.2896 , allloss: 6.2896, norm:0.3613, dt: 1375.61ms, tok/sec: 95283.09, flops:41.27, batch-reuse:1
@ 257 train 6.2963 , allloss: 6.2963, norm:0.3875, dt: 1375.24ms, tok/sec: 95308.22, flops:41.28, batch-reuse:1
@ 258 train 6.2321 , allloss: 6.2321, norm:0.2694, dt: 1374.32ms, tok/sec: 95372.01, flops:41.31, batch-reuse:1
@ 259 train 6.2550 , allloss: 6.2550, norm:0.4192, dt: 1375.06ms, tok/sec: 95321.17, flops:41.29, batch-reuse:1
@ 260 train 6.3206 , allloss: 6.3206, norm:0.2948, dt: 1373.63ms, tok/sec: 95420.43, flops:41.33, batch-reuse:1
@ 261 train 6.3032 , allloss: 6.3032, norm:0.4035, dt: 1373.75ms, tok/sec: 95411.88, flops:41.33, batch-reuse:1
@ 262 train 6.3580 , allloss: 6.3580, norm:0.3329, dt: 1374.15ms, tok/sec: 95384.35, flops:41.31, batch-reuse:1
@ 263 train 6.3014 , allloss: 6.3014, norm:0.5188, dt: 1373.47ms, tok/sec: 95431.56, flops:41.33, batch-reuse:1
@ 264 train 6.2414 , allloss: 6.2414, norm:0.3716, dt: 1373.87ms, tok/sec: 95403.27, flops:41.32, batch-reuse:1
@ 265 train 6.2513 , allloss: 6.2513, norm:0.5485, dt: 1374.15ms, tok/sec: 95384.27, flops:41.31, batch-reuse:1
@ 266 train 6.2223 , allloss: 6.2223, norm:0.3408, dt: 1374.57ms, tok/sec: 95354.60, flops:41.30, batch-reuse:1
@ 267 train 6.2270 , allloss: 6.2270, norm:0.4130, dt: 1373.71ms, tok/sec: 95414.94, flops:41.33, batch-reuse:1
@ 268 train 6.2988 , allloss: 6.2988, norm:0.5118, dt: 1374.16ms, tok/sec: 95383.26, flops:41.31, batch-reuse:1
@ 269 train 6.2497 , allloss: 6.2497, norm:0.4190, dt: 1372.29ms, tok/sec: 95513.43, flops:41.37, batch-reuse:1
@ 270 train 6.3426 , allloss: 6.3426, norm:0.3404, dt: 1374.45ms, tok/sec: 95363.32, flops:41.30, batch-reuse:1
@ 271 train 6.2024 , allloss: 6.2024, norm:0.3685, dt: 1374.11ms, tok/sec: 95386.82, flops:41.31, batch-reuse:1
@ 272 train 6.2359 , allloss: 6.2359, norm:0.3536, dt: 1372.70ms, tok/sec: 95485.09, flops:41.36, batch-reuse:1
@ 273 train 6.2447 , allloss: 6.2447, norm:0.7554, dt: 1371.28ms, tok/sec: 95583.96, flops:41.40, batch-reuse:1
@ 274 train 6.2102 , allloss: 6.2102, norm:0.3185, dt: 1372.22ms, tok/sec: 95518.46, flops:41.37, batch-reuse:1
@ 275 train 6.2177 , allloss: 6.2177, norm:0.3776, dt: 1372.41ms, tok/sec: 95504.67, flops:41.37, batch-reuse:1
@ 276 train 6.1872 , allloss: 6.1872, norm:0.3378, dt: 1372.36ms, tok/sec: 95508.22, flops:41.37, batch-reuse:1
@ 277 train 6.0498 , allloss: 6.0498, norm:0.2810, dt: 1371.66ms, tok/sec: 95557.31, flops:41.39, batch-reuse:1
@ 278 train 6.1617 , allloss: 6.1617, norm:0.3859, dt: 1372.41ms, tok/sec: 95504.73, flops:41.37, batch-reuse:1
@ 279 train 6.1119 , allloss: 6.1119, norm:0.3392, dt: 1371.17ms, tok/sec: 95591.64, flops:41.40, batch-reuse:1
@ 280 train 6.2345 , allloss: 6.2345, norm:0.4380, dt: 1370.08ms, tok/sec: 95667.19, flops:41.44, batch-reuse:1
@ 281 train 6.1402 , allloss: 6.1402, norm:0.3130, dt: 1372.00ms, tok/sec: 95533.48, flops:41.38, batch-reuse:1
@ 282 train 6.1036 , allloss: 6.1036, norm:0.3301, dt: 1371.59ms, tok/sec: 95561.96, flops:41.39, batch-reuse:1
@ 283 train 6.0734 , allloss: 6.0734, norm:0.3368, dt: 1371.10ms, tok/sec: 95596.07, flops:41.41, batch-reuse:1
@ 284 train 6.1513 , allloss: 6.1513, norm:0.3321, dt: 1370.51ms, tok/sec: 95637.57, flops:41.42, batch-reuse:1
@ 285 train 6.1524 , allloss: 6.1524, norm:0.3599, dt: 1371.47ms, tok/sec: 95570.43, flops:41.39, batch-reuse:1
@ 286 train 6.1407 , allloss: 6.1407, norm:0.2923, dt: 1371.16ms, tok/sec: 95592.00, flops:41.40, batch-reuse:1
@ 287 train 6.1453 , allloss: 6.1453, norm:0.2564, dt: 1370.86ms, tok/sec: 95613.22, flops:41.41, batch-reuse:1
@ 288 train 6.0902 , allloss: 6.0902, norm:0.3576, dt: 1370.78ms, tok/sec: 95618.77, flops:41.42, batch-reuse:1
@ 289 train 6.1244 , allloss: 6.1244, norm:0.2988, dt: 1370.88ms, tok/sec: 95611.74, flops:41.41, batch-reuse:1
@ 290 train 6.1325 , allloss: 6.1325, norm:0.2756, dt: 1371.38ms, tok/sec: 95576.48, flops:41.40, batch-reuse:1
@ 291 train 6.0553 , allloss: 6.0553, norm:0.2883, dt: 1370.34ms, tok/sec: 95649.30, flops:41.43, batch-reuse:1
@ 292 train 6.0524 , allloss: 6.0524, norm:0.3378, dt: 1370.08ms, tok/sec: 95667.37, flops:41.44, batch-reuse:1
@ 293 train 6.1242 , allloss: 6.1242, norm:0.2927, dt: 1369.00ms, tok/sec: 95742.86, flops:41.47, batch-reuse:1
@ 294 train 6.1132 , allloss: 6.1132, norm:0.3075, dt: 1369.34ms, tok/sec: 95719.09, flops:41.46, batch-reuse:1
@ 295 train 6.1236 , allloss: 6.1236, norm:0.3338, dt: 1369.69ms, tok/sec: 95694.35, flops:41.45, batch-reuse:1
@ 296 train 6.0115 , allloss: 6.0115, norm:0.2989, dt: 1369.47ms, tok/sec: 95709.88, flops:41.45, batch-reuse:1
@ 297 train 5.9907 , allloss: 5.9907, norm:0.3450, dt: 1368.83ms, tok/sec: 95755.00, flops:41.47, batch-reuse:1
@ 298 train 6.0693 , allloss: 6.0693, norm:0.2522, dt: 1368.88ms, tok/sec: 95751.45, flops:41.47, batch-reuse:1
@ 299 train 6.0477 , allloss: 6.0477, norm:0.3485, dt: 1369.61ms, tok/sec: 95700.18, flops:41.45, batch-reuse:1
@ 300 train 6.0697 , allloss: 6.0697, norm:0.2992, dt: 1370.11ms, tok/sec: 95665.23, flops:41.44, batch-reuse:1
@ 301 train 6.1515 , allloss: 6.1515, norm:0.3452, dt: 1368.97ms, tok/sec: 95745.08, flops:41.47, batch-reuse:1
@ 302 train 6.0319 , allloss: 6.0319, norm:0.3873, dt: 1368.00ms, tok/sec: 95813.13, flops:41.50, batch-reuse:1
@ 303 train 6.0462 , allloss: 6.0462, norm:0.3273, dt: 1367.11ms, tok/sec: 95875.19, flops:41.53, batch-reuse:1
@ 304 train 5.9713 , allloss: 5.9713, norm:0.3198, dt: 1368.50ms, tok/sec: 95778.11, flops:41.48, batch-reuse:1
@ 305 train 5.9910 , allloss: 5.9910, norm:0.3422, dt: 1369.11ms, tok/sec: 95735.29, flops:41.47, batch-reuse:1
@ 306 train 5.9951 , allloss: 5.9951, norm:0.2544, dt: 1367.69ms, tok/sec: 95834.53, flops:41.51, batch-reuse:1
@ 307 train 6.0456 , allloss: 6.0456, norm:0.5834, dt: 1367.82ms, tok/sec: 95825.51, flops:41.50, batch-reuse:1
@ 308 train 5.9803 , allloss: 5.9803, norm:0.3094, dt: 1367.94ms, tok/sec: 95816.72, flops:41.50, batch-reuse:1
@ 309 train 6.0255 , allloss: 6.0255, norm:0.2709, dt: 1368.20ms, tok/sec: 95798.72, flops:41.49, batch-reuse:1
@ 310 train 6.0506 , allloss: 6.0506, norm:0.4034, dt: 1367.97ms, tok/sec: 95815.10, flops:41.50, batch-reuse:1
@ 311 train 6.1031 , allloss: 6.1031, norm:0.2989, dt: 1367.62ms, tok/sec: 95839.74, flops:41.51, batch-reuse:1
@ 312 train 6.1315 , allloss: 6.1315, norm:0.2867, dt: 1367.70ms, tok/sec: 95833.92, flops:41.51, batch-reuse:1
@ 313 train 5.9965 , allloss: 5.9965, norm:0.3356, dt: 1367.14ms, tok/sec: 95873.07, flops:41.53, batch-reuse:1
@ 314 train 5.9913 , allloss: 5.9913, norm:0.3383, dt: 1366.90ms, tok/sec: 95889.82, flops:41.53, batch-reuse:1
@ 315 train 5.9807 , allloss: 5.9807, norm:0.2771, dt: 1366.55ms, tok/sec: 95914.28, flops:41.54, batch-reuse:1
@ 316 train 5.9994 , allloss: 5.9994, norm:0.3649, dt: 1367.69ms, tok/sec: 95834.56, flops:41.51, batch-reuse:1
@ 317 train 6.0601 , allloss: 6.0601, norm:0.3685, dt: 1367.09ms, tok/sec: 95876.29, flops:41.53, batch-reuse:1
@ 318 train 5.9306 , allloss: 5.9306, norm:0.3178, dt: 1366.31ms, tok/sec: 95931.72, flops:41.55, batch-reuse:1
@ 319 train 5.8960 , allloss: 5.8960, norm:0.3248, dt: 1365.34ms, tok/sec: 95999.28, flops:41.58, batch-reuse:1
@ 320 train 5.9688 , allloss: 5.9688, norm:0.3039, dt: 1364.69ms, tok/sec: 96045.49, flops:41.60, batch-reuse:1
@ 321 train 6.0255 , allloss: 6.0255, norm:0.2821, dt: 1365.51ms, tok/sec: 95987.55, flops:41.57, batch-reuse:1
@ 322 train 5.9220 , allloss: 5.9220, norm:0.3032, dt: 1366.49ms, tok/sec: 95918.75, flops:41.55, batch-reuse:1
@ 323 train 5.9999 , allloss: 5.9999, norm:0.3530, dt: 1367.96ms, tok/sec: 95815.55, flops:41.50, batch-reuse:1
@ 324 train 5.8905 , allloss: 5.8905, norm:0.3621, dt: 1364.90ms, tok/sec: 96030.71, flops:41.59, batch-reuse:1
@ 325 train 5.9173 , allloss: 5.9173, norm:0.3009, dt: 1366.98ms, tok/sec: 95884.05, flops:41.53, batch-reuse:1
@ 326 train 5.9647 , allloss: 5.9647, norm:0.3277, dt: 1365.93ms, tok/sec: 95958.34, flops:41.56, batch-reuse:1
@ 327 train 6.0088 , allloss: 6.0088, norm:0.3450, dt: 1364.35ms, tok/sec: 96069.52, flops:41.61, batch-reuse:1
@ 328 train 5.9763 , allloss: 5.9763, norm:0.3146, dt: 1365.95ms, tok/sec: 95956.60, flops:41.56, batch-reuse:1
@ 329 train 5.9064 , allloss: 5.9064, norm:0.2927, dt: 1363.99ms, tok/sec: 96094.73, flops:41.62, batch-reuse:1
@ 330 train 5.9136 , allloss: 5.9136, norm:0.3209, dt: 1364.29ms, tok/sec: 96073.25, flops:41.61, batch-reuse:1
@ 331 train 5.8886 , allloss: 5.8886, norm:0.2756, dt: 1363.48ms, tok/sec: 96130.60, flops:41.64, batch-reuse:1
@ 332 train 5.8296 , allloss: 5.8296, norm:0.4178, dt: 1363.77ms, tok/sec: 96109.71, flops:41.63, batch-reuse:1
@ 333 train 5.9291 , allloss: 5.9291, norm:0.4090, dt: 1365.60ms, tok/sec: 95981.43, flops:41.57, batch-reuse:1
@ 334 train 5.8593 , allloss: 5.8593, norm:0.3375, dt: 1363.81ms, tok/sec: 96107.14, flops:41.63, batch-reuse:1
@ 335 train 5.8184 , allloss: 5.8184, norm:0.3477, dt: 1363.37ms, tok/sec: 96138.32, flops:41.64, batch-reuse:1
@ 336 train 5.8770 , allloss: 5.8770, norm:0.3335, dt: 1364.64ms, tok/sec: 96048.81, flops:41.60, batch-reuse:1
@ 337 train 5.8454 , allloss: 5.8454, norm:0.3530, dt: 1364.52ms, tok/sec: 96057.55, flops:41.61, batch-reuse:1
@ 338 train 5.8581 , allloss: 5.8581, norm:0.3309, dt: 1362.82ms, tok/sec: 96177.24, flops:41.66, batch-reuse:1
@ 339 train 5.9093 , allloss: 5.9093, norm:0.3538, dt: 1363.41ms, tok/sec: 96135.68, flops:41.64, batch-reuse:1
@ 340 train 5.8892 , allloss: 5.8892, norm:0.3258, dt: 1363.97ms, tok/sec: 96095.68, flops:41.62, batch-reuse:1
@ 341 train 5.8167 , allloss: 5.8167, norm:0.3667, dt: 1363.93ms, tok/sec: 96099.09, flops:41.62, batch-reuse:1
@ 342 train 5.7576 , allloss: 5.7576, norm:0.3315, dt: 1362.65ms, tok/sec: 96188.86, flops:41.66, batch-reuse:1
@ 343 train 5.8578 , allloss: 5.8578, norm:0.3653, dt: 1363.79ms, tok/sec: 96108.72, flops:41.63, batch-reuse:1
@ 344 train 5.8163 , allloss: 5.8163, norm:0.3712, dt: 1362.49ms, tok/sec: 96200.09, flops:41.67, batch-reuse:1
@ 345 train 5.8019 , allloss: 5.8019, norm:0.2779, dt: 1362.94ms, tok/sec: 96168.27, flops:41.65, batch-reuse:1
@ 346 train 5.8719 , allloss: 5.8719, norm:0.3284, dt: 1362.82ms, tok/sec: 96177.25, flops:41.66, batch-reuse:1
@ 347 train 5.8557 , allloss: 5.8557, norm:0.3335, dt: 1362.97ms, tok/sec: 96166.69, flops:41.65, batch-reuse:1
@ 348 train 5.8573 , allloss: 5.8573, norm:0.2853, dt: 1362.85ms, tok/sec: 96175.25, flops:41.66, batch-reuse:1
@ 349 train 5.8728 , allloss: 5.8728, norm:0.3287, dt: 1363.26ms, tok/sec: 96145.92, flops:41.64, batch-reuse:1
@ 350 train 5.8675 , allloss: 5.8675, norm:0.3715, dt: 1363.63ms, tok/sec: 96120.01, flops:41.63, batch-reuse:1
@ 351 train 5.8899 , allloss: 5.8899, norm:0.3092, dt: 1361.79ms, tok/sec: 96250.01, flops:41.69, batch-reuse:1
@ 352 train 5.8460 , allloss: 5.8460, norm:0.3469, dt: 1362.50ms, tok/sec: 96199.33, flops:41.67, batch-reuse:1
@ 353 train 5.8373 , allloss: 5.8373, norm:0.3501, dt: 1362.31ms, tok/sec: 96213.12, flops:41.67, batch-reuse:1
@ 354 train 5.8253 , allloss: 5.8253, norm:0.3809, dt: 1362.75ms, tok/sec: 96181.93, flops:41.66, batch-reuse:1
@ 355 train 5.8208 , allloss: 5.8208, norm:0.3066, dt: 1363.03ms, tok/sec: 96162.31, flops:41.65, batch-reuse:1
@ 356 train 5.8551 , allloss: 5.8551, norm:0.3449, dt: 1363.15ms, tok/sec: 96153.95, flops:41.65, batch-reuse:1
@ 357 train 5.8653 , allloss: 5.8653, norm:0.3235, dt: 1361.47ms, tok/sec: 96272.50, flops:41.70, batch-reuse:1
@ 358 train 5.7736 , allloss: 5.7736, norm:0.3444, dt: 1361.97ms, tok/sec: 96237.04, flops:41.68, batch-reuse:1
@ 359 train 5.9430 , allloss: 5.9430, norm:0.4298, dt: 1361.46ms, tok/sec: 96273.09, flops:41.70, batch-reuse:1
@ 360 train 5.7984 , allloss: 5.7984, norm:0.3697, dt: 1361.54ms, tok/sec: 96267.76, flops:41.70, batch-reuse:1
@ 361 train 5.8372 , allloss: 5.8372, norm:0.2736, dt: 1361.47ms, tok/sec: 96272.38, flops:41.70, batch-reuse:1
@ 362 train 5.7596 , allloss: 5.7596, norm:0.3551, dt: 1360.57ms, tok/sec: 96336.35, flops:41.73, batch-reuse:1
@ 363 train 5.8038 , allloss: 5.8038, norm:0.3574, dt: 1359.51ms, tok/sec: 96411.16, flops:41.76, batch-reuse:1
@ 364 train 5.8245 , allloss: 5.8245, norm:0.3824, dt: 1361.07ms, tok/sec: 96300.37, flops:41.71, batch-reuse:1
@ 365 train 5.8072 , allloss: 5.8072, norm:0.3494, dt: 1361.26ms, tok/sec: 96287.60, flops:41.70, batch-reuse:1
@ 366 train 5.8684 , allloss: 5.8684, norm:0.3021, dt: 1361.30ms, tok/sec: 96284.70, flops:41.70, batch-reuse:1
@ 367 train 5.7773 , allloss: 5.7773, norm:0.3696, dt: 1361.76ms, tok/sec: 96251.92, flops:41.69, batch-reuse:1
@ 368 train 5.8507 , allloss: 5.8507, norm:0.3230, dt: 1360.57ms, tok/sec: 96336.23, flops:41.73, batch-reuse:1
@ 369 train 5.7518 , allloss: 5.7518, norm:0.3545, dt: 1360.57ms, tok/sec: 96336.40, flops:41.73, batch-reuse:1
@ 370 train 5.9366 , allloss: 5.9366, norm:0.3856, dt: 1362.26ms, tok/sec: 96216.47, flops:41.67, batch-reuse:1
@ 371 train 5.9180 , allloss: 5.9180, norm:0.3627, dt: 1361.11ms, tok/sec: 96297.59, flops:41.71, batch-reuse:1
@ 372 train 5.9444 , allloss: 5.9444, norm:0.3188, dt: 1361.38ms, tok/sec: 96279.04, flops:41.70, batch-reuse:1
@ 373 train 5.8635 , allloss: 5.8635, norm:0.3211, dt: 1360.92ms, tok/sec: 96311.02, flops:41.72, batch-reuse:1
@ 374 train 5.9335 , allloss: 5.9335, norm:0.3546, dt: 1363.30ms, tok/sec: 96143.16, flops:41.64, batch-reuse:1
@ 375 train 6.0200 , allloss: 6.0200, norm:0.3709, dt: 1361.17ms, tok/sec: 96293.44, flops:41.71, batch-reuse:1
@ 376 train 6.0511 , allloss: 6.0511, norm:0.4602, dt: 1362.23ms, tok/sec: 96218.54, flops:41.68, batch-reuse:1
@ 377 train 5.9341 , allloss: 5.9341, norm:0.3561, dt: 1362.12ms, tok/sec: 96226.33, flops:41.68, batch-reuse:1
@ 378 train 5.9038 , allloss: 5.9038, norm:0.4479, dt: 1361.83ms, tok/sec: 96247.01, flops:41.69, batch-reuse:1
@ 379 train 5.9875 , allloss: 5.9875, norm:0.4075, dt: 1362.10ms, tok/sec: 96227.55, flops:41.68, batch-reuse:1
@ 380 train 6.0397 , allloss: 6.0397, norm:0.4001, dt: 1361.44ms, tok/sec: 96274.69, flops:41.70, batch-reuse:1
@ 381 train 5.9379 , allloss: 5.9379, norm:0.3540, dt: 1361.31ms, tok/sec: 96284.03, flops:41.70, batch-reuse:1
@ 382 train 5.8985 , allloss: 5.8985, norm:0.3538, dt: 1361.49ms, tok/sec: 96270.68, flops:41.70, batch-reuse:1
@ 383 train 5.9412 , allloss: 5.9412, norm:0.3488, dt: 1361.45ms, tok/sec: 96274.08, flops:41.70, batch-reuse:1
@ 384 train 5.8672 , allloss: 5.8672, norm:0.3274, dt: 1360.73ms, tok/sec: 96325.06, flops:41.72, batch-reuse:1
@ 385 train 5.8653 , allloss: 5.8653, norm:0.3059, dt: 1361.79ms, tok/sec: 96249.88, flops:41.69, batch-reuse:1
@ 386 train 5.9780 , allloss: 5.9780, norm:0.4303, dt: 1361.50ms, tok/sec: 96270.41, flops:41.70, batch-reuse:1
@ 387 train 5.9135 , allloss: 5.9135, norm:0.3888, dt: 1360.64ms, tok/sec: 96331.03, flops:41.72, batch-reuse:1
@ 388 train 5.8054 , allloss: 5.8054, norm:0.3806, dt: 1360.13ms, tok/sec: 96367.34, flops:41.74, batch-reuse:1
@ 389 train 5.8898 , allloss: 5.8898, norm:0.4505, dt: 1362.13ms, tok/sec: 96225.77, flops:41.68, batch-reuse:1
@ 390 train 5.9319 , allloss: 5.9319, norm:0.3489, dt: 1361.12ms, tok/sec: 96297.12, flops:41.71, batch-reuse:1
@ 391 train 5.8488 , allloss: 5.8488, norm:0.4120, dt: 1361.14ms, tok/sec: 96295.48, flops:41.71, batch-reuse:1
@ 392 train 5.9818 , allloss: 5.9818, norm:0.5884, dt: 1362.73ms, tok/sec: 96183.60, flops:41.66, batch-reuse:1
@ 393 train 5.9015 , allloss: 5.9015, norm:0.6726, dt: 1361.38ms, tok/sec: 96278.68, flops:41.70, batch-reuse:1
@ 394 train 5.8384 , allloss: 5.8384, norm:0.6097, dt: 1362.41ms, tok/sec: 96206.19, flops:41.67, batch-reuse:1
@ 395 train 5.9045 , allloss: 5.9045, norm:0.4139, dt: 1361.69ms, tok/sec: 96257.06, flops:41.69, batch-reuse:1
@ 396 train 5.7906 , allloss: 5.7906, norm:0.4323, dt: 1361.49ms, tok/sec: 96271.10, flops:41.70, batch-reuse:1
@ 397 train 5.9334 , allloss: 5.9334, norm:0.6138, dt: 1361.68ms, tok/sec: 96257.75, flops:41.69, batch-reuse:1
@ 398 train 5.9040 , allloss: 5.9040, norm:0.3473, dt: 1362.61ms, tok/sec: 96192.21, flops:41.66, batch-reuse:1
@ 399 train 5.8986 , allloss: 5.8986, norm:0.4822, dt: 1361.40ms, tok/sec: 96277.20, flops:41.70, batch-reuse:1
@ 400 train 5.9257 , allloss: 5.9257, norm:0.4902, dt: 1360.71ms, tok/sec: 96326.31, flops:41.72, batch-reuse:1
@ 401 train 5.8653 , allloss: 5.8653, norm:0.5297, dt: 1360.75ms, tok/sec: 96323.61, flops:41.72, batch-reuse:1
@ 402 train 5.8613 , allloss: 5.8613, norm:0.4296, dt: 1362.40ms, tok/sec: 96206.67, flops:41.67, batch-reuse:1
@ 403 train 5.9320 , allloss: 5.9320, norm:0.3769, dt: 1362.09ms, tok/sec: 96228.60, flops:41.68, batch-reuse:1
@ 404 train 5.8376 , allloss: 5.8376, norm:0.4668, dt: 1362.72ms, tok/sec: 96183.90, flops:41.66, batch-reuse:1
@ 405 train 5.8841 , allloss: 5.8841, norm:0.3320, dt: 1362.67ms, tok/sec: 96187.59, flops:41.66, batch-reuse:1
@ 406 train 5.8589 , allloss: 5.8589, norm:0.4977, dt: 1362.94ms, tok/sec: 96168.61, flops:41.65, batch-reuse:1
@ 407 train 5.8342 , allloss: 5.8342, norm:0.3226, dt: 1362.35ms, tok/sec: 96210.29, flops:41.67, batch-reuse:1
@ 408 train 5.8447 , allloss: 5.8447, norm:0.3828, dt: 1363.78ms, tok/sec: 96109.44, flops:41.63, batch-reuse:1
@ 409 train 5.8053 , allloss: 5.8053, norm:0.4387, dt: 1361.89ms, tok/sec: 96242.60, flops:41.69, batch-reuse:1
@ 410 train 5.8346 , allloss: 5.8346, norm:0.3207, dt: 1362.78ms, tok/sec: 96180.18, flops:41.66, batch-reuse:1
@ 411 train 5.8862 , allloss: 5.8862, norm:0.4236, dt: 1362.43ms, tok/sec: 96204.54, flops:41.67, batch-reuse:1
@ 412 train 5.8658 , allloss: 5.8658, norm:0.3453, dt: 1361.61ms, tok/sec: 96262.40, flops:41.69, batch-reuse:1
@ 413 train 5.5597 , allloss: 5.5597, norm:0.4919, dt: 1362.28ms, tok/sec: 96215.08, flops:41.67, batch-reuse:1
@ 414 train 5.8051 , allloss: 5.8051, norm:0.3135, dt: 1362.78ms, tok/sec: 96180.08, flops:41.66, batch-reuse:1
@ 415 train 5.7616 , allloss: 5.7616, norm:0.3706, dt: 1363.22ms, tok/sec: 96148.99, flops:41.64, batch-reuse:1
@ 416 train 5.9036 , allloss: 5.9036, norm:0.3929, dt: 1362.21ms, tok/sec: 96220.35, flops:41.68, batch-reuse:1
@ 417 train 5.7817 , allloss: 5.7817, norm:0.3646, dt: 1361.89ms, tok/sec: 96242.50, flops:41.69, batch-reuse:1
@ 418 train 5.8778 , allloss: 5.8778, norm:0.4090, dt: 1363.33ms, tok/sec: 96141.31, flops:41.64, batch-reuse:1
@ 419 train 5.8107 , allloss: 5.8107, norm:0.3564, dt: 1362.64ms, tok/sec: 96189.67, flops:41.66, batch-reuse:1
@ 420 train 5.7667 , allloss: 5.7667, norm:0.4110, dt: 1362.64ms, tok/sec: 96189.45, flops:41.66, batch-reuse:1
@ 421 train 5.8865 , allloss: 5.8865, norm:0.3216, dt: 1363.20ms, tok/sec: 96150.49, flops:41.65, batch-reuse:1
@ 422 train 5.7694 , allloss: 5.7694, norm:0.3944, dt: 1362.55ms, tok/sec: 96195.97, flops:41.67, batch-reuse:1
@ 423 train 5.7354 , allloss: 5.7354, norm:0.3223, dt: 1361.35ms, tok/sec: 96281.05, flops:41.70, batch-reuse:1
@ 424 train 5.8248 , allloss: 5.8248, norm:0.4020, dt: 1362.79ms, tok/sec: 96179.02, flops:41.66, batch-reuse:1
@ 425 train 5.8506 , allloss: 5.8506, norm:0.3147, dt: 1362.13ms, tok/sec: 96226.04, flops:41.68, batch-reuse:1
@ 426 train 5.7041 , allloss: 5.7041, norm:0.3638, dt: 1362.29ms, tok/sec: 96214.12, flops:41.67, batch-reuse:1
@ 427 train 5.7906 , allloss: 5.7906, norm:0.3739, dt: 1362.23ms, tok/sec: 96218.53, flops:41.68, batch-reuse:1
@ 428 train 5.7243 , allloss: 5.7243, norm:0.3423, dt: 1363.27ms, tok/sec: 96145.53, flops:41.64, batch-reuse:1
@ 429 train 5.7038 , allloss: 5.7038, norm:0.3691, dt: 1363.12ms, tok/sec: 96156.04, flops:41.65, batch-reuse:1
@ 430 train 5.7780 , allloss: 5.7780, norm:0.4005, dt: 1363.44ms, tok/sec: 96133.63, flops:41.64, batch-reuse:1
@ 431 train 5.7726 , allloss: 5.7726, norm:0.3721, dt: 1363.94ms, tok/sec: 96097.95, flops:41.62, batch-reuse:1
@ 432 train 5.8279 , allloss: 5.8279, norm:0.4308, dt: 1362.73ms, tok/sec: 96183.55, flops:41.66, batch-reuse:1
@ 433 train 5.7930 , allloss: 5.7930, norm:0.3855, dt: 1361.89ms, tok/sec: 96242.43, flops:41.69, batch-reuse:1
@ 434 train 5.8182 , allloss: 5.8182, norm:0.3237, dt: 1362.76ms, tok/sec: 96181.14, flops:41.66, batch-reuse:1
@ 435 train 5.7115 , allloss: 5.7115, norm:0.3988, dt: 1362.65ms, tok/sec: 96188.97, flops:41.66, batch-reuse:1
@ 436 train 5.8729 , allloss: 5.8729, norm:0.3126, dt: 1362.31ms, tok/sec: 96212.75, flops:41.67, batch-reuse:1
@ 437 train 5.6960 , allloss: 5.6960, norm:0.4186, dt: 1362.82ms, tok/sec: 96177.34, flops:41.66, batch-reuse:1
@ 438 train 5.8060 , allloss: 5.8060, norm:0.4311, dt: 1361.56ms, tok/sec: 96266.26, flops:41.70, batch-reuse:1
@ 439 train 5.6732 , allloss: 5.6732, norm:0.3180, dt: 1362.31ms, tok/sec: 96213.22, flops:41.67, batch-reuse:1
@ 440 train 5.6619 , allloss: 5.6619, norm:0.4843, dt: 1363.05ms, tok/sec: 96160.68, flops:41.65, batch-reuse:1
@ 441 train 5.7652 , allloss: 5.7652, norm:0.3037, dt: 1362.84ms, tok/sec: 96175.84, flops:41.66, batch-reuse:1
@ 442 train 5.7792 , allloss: 5.7792, norm:0.3343, dt: 1364.47ms, tok/sec: 96060.96, flops:41.61, batch-reuse:1
@ 443 train 5.7731 , allloss: 5.7731, norm:0.2868, dt: 1363.08ms, tok/sec: 96158.92, flops:41.65, batch-reuse:1
@ 444 train 5.7480 , allloss: 5.7480, norm:0.3184, dt: 1363.75ms, tok/sec: 96111.43, flops:41.63, batch-reuse:1
@ 445 train 5.7197 , allloss: 5.7197, norm:0.2713, dt: 1363.94ms, tok/sec: 96098.09, flops:41.62, batch-reuse:1
@ 446 train 5.6687 , allloss: 5.6687, norm:0.3227, dt: 1363.29ms, tok/sec: 96143.93, flops:41.64, batch-reuse:1
@ 447 train 5.7367 , allloss: 5.7367, norm:0.3348, dt: 1363.29ms, tok/sec: 96144.18, flops:41.64, batch-reuse:1
@ 448 train 5.6812 , allloss: 5.6812, norm:0.2742, dt: 1363.31ms, tok/sec: 96142.69, flops:41.64, batch-reuse:1
@ 449 train 5.7129 , allloss: 5.7129, norm:0.3223, dt: 1364.18ms, tok/sec: 96081.42, flops:41.62, batch-reuse:1
@ 450 train 5.7245 , allloss: 5.7245, norm:0.2704, dt: 1363.86ms, tok/sec: 96103.44, flops:41.63, batch-reuse:1
@ 451 train 5.7031 , allloss: 5.7031, norm:0.2991, dt: 1363.61ms, tok/sec: 96120.99, flops:41.63, batch-reuse:1
@ 452 train 5.6800 , allloss: 5.6800, norm:0.2609, dt: 1362.65ms, tok/sec: 96189.30, flops:41.66, batch-reuse:1
@ 453 train 5.6965 , allloss: 5.6965, norm:0.3218, dt: 1363.63ms, tok/sec: 96120.08, flops:41.63, batch-reuse:1
@ 454 train 5.7208 , allloss: 5.7208, norm:0.3247, dt: 1362.43ms, tok/sec: 96204.86, flops:41.67, batch-reuse:1
@ 455 train 5.7095 , allloss: 5.7095, norm:0.3922, dt: 1363.93ms, tok/sec: 96098.51, flops:41.62, batch-reuse:1
@ 456 train 5.7656 , allloss: 5.7656, norm:0.3034, dt: 1365.67ms, tok/sec: 95976.54, flops:41.57, batch-reuse:1
@ 457 train 5.7816 , allloss: 5.7816, norm:0.3799, dt: 1364.54ms, tok/sec: 96055.89, flops:41.60, batch-reuse:1
@ 458 train 5.7847 , allloss: 5.7847, norm:0.3566, dt: 1363.73ms, tok/sec: 96112.94, flops:41.63, batch-reuse:1
@ 459 train 5.7064 , allloss: 5.7064, norm:0.3526, dt: 1366.09ms, tok/sec: 95946.59, flops:41.56, batch-reuse:1
@ 460 train 5.6879 , allloss: 5.6879, norm:0.4574, dt: 1364.64ms, tok/sec: 96048.74, flops:41.60, batch-reuse:1
@ 461 train 5.6975 , allloss: 5.6975, norm:0.4746, dt: 1362.82ms, tok/sec: 96176.75, flops:41.66, batch-reuse:1
@ 462 train 5.7761 , allloss: 5.7761, norm:0.3550, dt: 1365.68ms, tok/sec: 95975.92, flops:41.57, batch-reuse:1
@ 463 train 5.7351 , allloss: 5.7351, norm:0.3735, dt: 1363.21ms, tok/sec: 96149.52, flops:41.65, batch-reuse:1
@ 464 train 5.7017 , allloss: 5.7017, norm:0.4403, dt: 1365.32ms, tok/sec: 96000.86, flops:41.58, batch-reuse:1
@ 465 train 5.6528 , allloss: 5.6528, norm:0.3117, dt: 1362.79ms, tok/sec: 96179.04, flops:41.66, batch-reuse:1
@ 466 train 5.7981 , allloss: 5.7981, norm:0.5550, dt: 1364.03ms, tok/sec: 96091.60, flops:41.62, batch-reuse:1
@ 467 train 5.6508 , allloss: 5.6508, norm:0.7096, dt: 1363.87ms, tok/sec: 96102.89, flops:41.62, batch-reuse:1
@ 468 train 5.6230 , allloss: 5.6230, norm:0.3483, dt: 1364.67ms, tok/sec: 96046.73, flops:41.60, batch-reuse:1
@ 469 train 5.6118 , allloss: 5.6118, norm:0.3670, dt: 1364.92ms, tok/sec: 96029.03, flops:41.59, batch-reuse:1
@ 470 train 5.6807 , allloss: 5.6807, norm:0.3903, dt: 1365.19ms, tok/sec: 96009.94, flops:41.58, batch-reuse:1
@ 471 train 5.5979 , allloss: 5.5979, norm:0.3688, dt: 1365.03ms, tok/sec: 96021.01, flops:41.59, batch-reuse:1
@ 472 train 5.7153 , allloss: 5.7153, norm:0.4033, dt: 1365.05ms, tok/sec: 96019.97, flops:41.59, batch-reuse:1
@ 473 train 5.6411 , allloss: 5.6411, norm:0.4457, dt: 1364.86ms, tok/sec: 96033.07, flops:41.59, batch-reuse:1
@ 474 train 5.5867 , allloss: 5.5867, norm:0.3291, dt: 1364.74ms, tok/sec: 96042.01, flops:41.60, batch-reuse:1
@ 475 train 5.6318 , allloss: 5.6318, norm:0.4432, dt: 1366.13ms, tok/sec: 95944.28, flops:41.56, batch-reuse:1
@ 476 train 5.6306 , allloss: 5.6306, norm:0.3714, dt: 1365.80ms, tok/sec: 95967.10, flops:41.57, batch-reuse:1
@ 477 train 5.6913 , allloss: 5.6913, norm:0.3449, dt: 1364.56ms, tok/sec: 96054.46, flops:41.60, batch-reuse:1
@ 478 train 5.6219 , allloss: 5.6219, norm:0.4239, dt: 1364.38ms, tok/sec: 96067.15, flops:41.61, batch-reuse:1
@ 479 train 5.6876 , allloss: 5.6876, norm:0.4291, dt: 1364.86ms, tok/sec: 96033.61, flops:41.59, batch-reuse:1
@ 480 train 5.5615 , allloss: 5.5615, norm:0.3422, dt: 1364.51ms, tok/sec: 96058.11, flops:41.61, batch-reuse:1
@ 481 train 5.6309 , allloss: 5.6309, norm:0.4318, dt: 1364.69ms, tok/sec: 96044.93, flops:41.60, batch-reuse:1
@ 482 train 5.6109 , allloss: 5.6109, norm:0.2899, dt: 1365.80ms, tok/sec: 95966.97, flops:41.57, batch-reuse:1
@ 483 train 5.7049 , allloss: 5.7049, norm:0.3448, dt: 1364.67ms, tok/sec: 96046.91, flops:41.60, batch-reuse:1
@ 484 train 5.5419 , allloss: 5.5419, norm:0.4228, dt: 1364.49ms, tok/sec: 96059.60, flops:41.61, batch-reuse:1
@ 485 train 5.5104 , allloss: 5.5104, norm:0.4132, dt: 1364.90ms, tok/sec: 96030.42, flops:41.59, batch-reuse:1
@ 486 train 5.6789 , allloss: 5.6789, norm:0.3107, dt: 1364.91ms, tok/sec: 96029.62, flops:41.59, batch-reuse:1
@ 487 train 5.6067 , allloss: 5.6067, norm:0.4338, dt: 1365.49ms, tok/sec: 95988.72, flops:41.58, batch-reuse:1
@ 488 train 5.5359 , allloss: 5.5359, norm:0.3979, dt: 1364.73ms, tok/sec: 96042.31, flops:41.60, batch-reuse:1
@ 489 train 5.5596 , allloss: 5.5596, norm:0.4697, dt: 1364.98ms, tok/sec: 96025.04, flops:41.59, batch-reuse:1
@ 490 train 5.6716 , allloss: 5.6716, norm:0.4750, dt: 1366.00ms, tok/sec: 95953.19, flops:41.56, batch-reuse:1
@ 491 train 5.6704 , allloss: 5.6704, norm:0.3344, dt: 1364.79ms, tok/sec: 96038.25, flops:41.60, batch-reuse:1
@ 492 train 5.5927 , allloss: 5.5927, norm:0.5122, dt: 1363.86ms, tok/sec: 96103.43, flops:41.63, batch-reuse:1
@ 493 train 5.6176 , allloss: 5.6176, norm:0.3313, dt: 1364.47ms, tok/sec: 96060.81, flops:41.61, batch-reuse:1
@ 494 train 5.5936 , allloss: 5.5936, norm:0.4734, dt: 1365.93ms, tok/sec: 95958.24, flops:41.56, batch-reuse:1
@ 495 train 5.5136 , allloss: 5.5136, norm:0.3161, dt: 1364.63ms, tok/sec: 96049.26, flops:41.60, batch-reuse:1
@ 496 train 5.6219 , allloss: 5.6219, norm:0.4624, dt: 1367.52ms, tok/sec: 95846.37, flops:41.51, batch-reuse:1
@ 497 train 5.5418 , allloss: 5.5418, norm:0.3840, dt: 1365.60ms, tok/sec: 95981.11, flops:41.57, batch-reuse:1
@ 498 train 5.6296 , allloss: 5.6296, norm:0.3841, dt: 1366.49ms, tok/sec: 95918.60, flops:41.55, batch-reuse:1
@ 499 train 5.6300 , allloss: 5.6300, norm:0.3883, dt: 1364.50ms, tok/sec: 96058.63, flops:41.61, batch-reuse:1
@ 500 train 5.5418 , allloss: 5.5418, norm:0.3161, dt: 1365.24ms, tok/sec: 96006.46, flops:41.58, batch-reuse:1
@ 501 train 5.6589 , allloss: 5.6589, norm:0.3501, dt: 1365.36ms, tok/sec: 95997.89, flops:41.58, batch-reuse:1
@ 502 train 5.6510 , allloss: 5.6510, norm:0.3402, dt: 1364.20ms, tok/sec: 96079.85, flops:41.61, batch-reuse:1
@ 503 train 5.5968 , allloss: 5.5968, norm:0.3277, dt: 1363.98ms, tok/sec: 96095.53, flops:41.62, batch-reuse:1
@ 504 train 5.6033 , allloss: 5.6033, norm:0.3702, dt: 1364.56ms, tok/sec: 96054.46, flops:41.60, batch-reuse:1
@ 505 train 5.5727 , allloss: 5.5727, norm:0.3635, dt: 1364.93ms, tok/sec: 96028.37, flops:41.59, batch-reuse:1
@ 506 train 5.6204 , allloss: 5.6204, norm:0.3369, dt: 1364.90ms, tok/sec: 96030.74, flops:41.59, batch-reuse:1
@ 507 train 5.5887 , allloss: 5.5887, norm:0.3712, dt: 1365.98ms, tok/sec: 95954.58, flops:41.56, batch-reuse:1
@ 508 train 5.5024 , allloss: 5.5024, norm:0.3012, dt: 1365.28ms, tok/sec: 96003.57, flops:41.58, batch-reuse:1
@ 509 train 5.5885 , allloss: 5.5885, norm:0.3393, dt: 1365.63ms, tok/sec: 95979.20, flops:41.57, batch-reuse:1
@ 510 train 5.5392 , allloss: 5.5392, norm:0.3565, dt: 1366.05ms, tok/sec: 95949.82, flops:41.56, batch-reuse:1
@ 511 train 5.5906 , allloss: 5.5906, norm:0.3324, dt: 1364.36ms, tok/sec: 96068.66, flops:41.61, batch-reuse:1
@ 512 train 5.5365 , allloss: 5.5365, norm:0.3404, dt: 1365.43ms, tok/sec: 95993.50, flops:41.58, batch-reuse:1
@ 513 train 5.5118 , allloss: 5.5118, norm:0.3090, dt: 1366.35ms, tok/sec: 95928.31, flops:41.55, batch-reuse:1
@ 514 train 5.5214 , allloss: 5.5214, norm:0.3181, dt: 1365.23ms, tok/sec: 96007.34, flops:41.58, batch-reuse:1
@ 515 train 5.4692 , allloss: 5.4692, norm:0.3889, dt: 1364.58ms, tok/sec: 96053.17, flops:41.60, batch-reuse:1
@ 516 train 5.4700 , allloss: 5.4700, norm:0.2636, dt: 1364.74ms, tok/sec: 96041.44, flops:41.60, batch-reuse:1
@ 517 train 5.4487 , allloss: 5.4487, norm:0.3599, dt: 1364.93ms, tok/sec: 96028.63, flops:41.59, batch-reuse:1
@ 518 train 5.5160 , allloss: 5.5160, norm:0.3982, dt: 1364.27ms, tok/sec: 96075.08, flops:41.61, batch-reuse:1
@ 519 train 5.4801 , allloss: 5.4801, norm:0.2937, dt: 1366.07ms, tok/sec: 95948.48, flops:41.56, batch-reuse:1
@ 520 train 5.5246 , allloss: 5.5246, norm:0.3290, dt: 1364.94ms, tok/sec: 96027.97, flops:41.59, batch-reuse:1
@ 521 train 5.6264 , allloss: 5.6264, norm:0.3692, dt: 1365.69ms, tok/sec: 95975.08, flops:41.57, batch-reuse:1
@ 522 train 5.4657 , allloss: 5.4657, norm:0.3082, dt: 1365.32ms, tok/sec: 96000.92, flops:41.58, batch-reuse:1
@ 523 train 5.5476 , allloss: 5.5476, norm:0.3714, dt: 1363.75ms, tok/sec: 96111.39, flops:41.63, batch-reuse:1
@ 524 train 5.4267 , allloss: 5.4267, norm:0.3065, dt: 1365.79ms, tok/sec: 95967.67, flops:41.57, batch-reuse:1
@ 525 train 5.4887 , allloss: 5.4887, norm:0.3230, dt: 1364.57ms, tok/sec: 96053.36, flops:41.60, batch-reuse:1
@ 526 train 5.5140 , allloss: 5.5140, norm:0.3764, dt: 1366.29ms, tok/sec: 95932.49, flops:41.55, batch-reuse:1
@ 527 train 5.5141 , allloss: 5.5141, norm:0.3090, dt: 1364.70ms, tok/sec: 96044.40, flops:41.60, batch-reuse:1
@ 528 train 5.4492 , allloss: 5.4492, norm:0.3105, dt: 1366.07ms, tok/sec: 95948.20, flops:41.56, batch-reuse:1
@ 529 train 5.4917 , allloss: 5.4917, norm:0.3650, dt: 1366.00ms, tok/sec: 95953.30, flops:41.56, batch-reuse:1
@ 530 train 5.5393 , allloss: 5.5393, norm:0.3059, dt: 1366.47ms, tok/sec: 95920.39, flops:41.55, batch-reuse:1
@ 531 train 5.7072 , allloss: 5.7072, norm:0.4240, dt: 1367.43ms, tok/sec: 95852.76, flops:41.52, batch-reuse:1
@ 532 train 5.3820 , allloss: 5.3820, norm:0.4580, dt: 1366.43ms, tok/sec: 95923.17, flops:41.55, batch-reuse:1
@ 533 train 5.4666 , allloss: 5.4666, norm:0.3231, dt: 1367.44ms, tok/sec: 95852.07, flops:41.52, batch-reuse:1
@ 534 train 5.4206 , allloss: 5.4206, norm:0.3989, dt: 1364.72ms, tok/sec: 96043.41, flops:41.60, batch-reuse:1
@ 535 train 5.5539 , allloss: 5.5539, norm:0.4600, dt: 1366.41ms, tok/sec: 95924.22, flops:41.55, batch-reuse:1
@ 536 train 5.4647 , allloss: 5.4647, norm:0.4578, dt: 1365.29ms, tok/sec: 96003.30, flops:41.58, batch-reuse:1
@ 537 train 5.4190 , allloss: 5.4190, norm:0.4893, dt: 1365.49ms, tok/sec: 95989.16, flops:41.58, batch-reuse:1
@ 538 train 5.5466 , allloss: 5.5466, norm:0.4162, dt: 1365.58ms, tok/sec: 95982.90, flops:41.57, batch-reuse:1
@ 539 train 5.4893 , allloss: 5.4893, norm:0.5720, dt: 1366.31ms, tok/sec: 95931.67, flops:41.55, batch-reuse:1
@ 540 train 5.5097 , allloss: 5.5097, norm:0.4027, dt: 1366.21ms, tok/sec: 95938.55, flops:41.55, batch-reuse:1
@ 541 train 5.4781 , allloss: 5.4781, norm:0.3896, dt: 1364.67ms, tok/sec: 96046.61, flops:41.60, batch-reuse:1
@ 542 train 5.4716 , allloss: 5.4716, norm:0.5097, dt: 1364.69ms, tok/sec: 96045.49, flops:41.60, batch-reuse:1
@ 543 train 5.5286 , allloss: 5.5286, norm:0.3675, dt: 1365.70ms, tok/sec: 95974.43, flops:41.57, batch-reuse:1
@ 544 train 5.4568 , allloss: 5.4568, norm:0.3951, dt: 1365.61ms, tok/sec: 95980.42, flops:41.57, batch-reuse:1
@ 545 train 5.4725 , allloss: 5.4725, norm:0.3211, dt: 1367.13ms, tok/sec: 95874.00, flops:41.53, batch-reuse:1
@ 546 train 5.3736 , allloss: 5.3736, norm:0.3591, dt: 1365.71ms, tok/sec: 95973.81, flops:41.57, batch-reuse:1
@ 547 train 5.5346 , allloss: 5.5346, norm:0.3855, dt: 1364.53ms, tok/sec: 96056.70, flops:41.60, batch-reuse:1
@ 548 train 5.5048 , allloss: 5.5048, norm:0.4227, dt: 1365.34ms, tok/sec: 95999.38, flops:41.58, batch-reuse:1
@ 549 train 5.4501 , allloss: 5.4501, norm:0.3577, dt: 1364.76ms, tok/sec: 96040.00, flops:41.60, batch-reuse:1
INFO nextres 10.106666564941406 attn*mlp 10.078983306884766 layernormed 0.9992984533309937
			attn_hist -39.375<tensor([  0.,   0.,   0.,  32., 351., 344.,  41.,   0.,   0.,   0.])>33.75
			mlp_hist -24.125<tensor([  0.,   0.,   3.,  58., 318., 332.,  51.,   6.,   0.,   0.],
       dtype=torch.bfloat16)>26.375
			x_hist -3.5554676055908203<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>4.172529697418213
INFO nextres 20.542442321777344 attn*mlp 10.81185531616211 layernormed 0.9993561506271362
			attn_hist -42.75<tensor([  0.,   0.,   1.,  33., 359., 340.,  34.,   1.,   0.,   0.])>50.25
			mlp_hist -12.125<tensor([  0.,   0.,   0.,   5., 342., 420.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>9.375
			x_hist -3.47764253616333<tensor([  0.,   0.,   0.,   0., 389., 379.,   0.,   0.,   0.,   0.])>4.232944965362549
INFO nextres 31.384571075439453 attn*mlp 11.256192207336426 layernormed 0.9994412064552307
			attn_hist -41.8125<tensor([  0.,   0.,   2.,  34., 353., 345.,  33.,   1.,   0.,   0.])>50.625
			mlp_hist -11.1875<tensor([  0.,   0.,   0.,   3., 348., 416.,   1.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.25
			x_hist -3.459852457046509<tensor([  0.,   0.,   0.,   0., 395., 373.,   0.,   0.,   0.,   0.])>4.280271053314209
INFO nextres 42.52428436279297 attn*mlp 11.576800346374512 layernormed 0.9994928240776062
			attn_hist -41.4375<tensor([  0.,   0.,   2.,  30., 363., 339.,  33.,   1.,   0.,   0.])>51.375
			mlp_hist -10.9375<tensor([  0.,   0.,   0.,   2., 376., 388.,   2.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.5
			x_hist -3.451382875442505<tensor([  0.,   0.,   0.,   0., 396., 372.,   0.,   0.,   0.,   0.])>4.315967559814453
INFO nextres 53.85200500488281 attn*mlp 11.791574478149414 layernormed 0.9995135068893433
			attn_hist -41.4375<tensor([  0.,   0.,   2.,  31., 363., 337.,  34.,   1.,   0.,   0.])>51.75
			mlp_hist -10.4375<tensor([  0.,   0.,   0.,   2., 380., 384.,   3.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.625
			x_hist -3.445652961730957<tensor([  0.,   0.,   0.,   0., 392., 376.,   0.,   0.,   0.,   0.])>4.335646152496338
INFO nextres 65.33404541015625 attn*mlp 11.964069366455078 layernormed 0.9995138049125671
			attn_hist -41.4375<tensor([  0.,   0.,   2.,  31., 359., 340.,  35.,   1.,   0.,   0.])>52.125
			mlp_hist -11.3125<tensor([  0.,   0.,   0.,   1., 380., 384.,   3.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.6875
			x_hist -3.439415454864502<tensor([  0.,   0.,   0.,   0., 385., 383.,   0.,   0.,   0.,   0.])>4.346719264984131
INFO nextres 76.94976806640625 attn*mlp 12.109638214111328 layernormed 0.9995042085647583
			attn_hist -41.25<tensor([  0.,   0.,   2.,  32., 351., 347.,  35.,   1.,   0.,   0.])>52.125
			mlp_hist -11.875<tensor([  0.,   0.,   0.,   1., 384., 380.,   3.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.625
			x_hist -3.4307918548583984<tensor([  0.,   0.,   0.,   0., 386., 382.,   0.,   0.,   0.,   0.])>4.349281311035156
INFO nextres 88.68592071533203 attn*mlp 12.23924732208252 layernormed 0.9994895458221436
			attn_hist -41.25<tensor([  0.,   0.,   2.,  30., 354., 347.,  34.,   1.,   0.,   0.])>52.125
			mlp_hist -12.25<tensor([  0.,   0.,   0.,   2., 384., 380.,   3.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.5625
			x_hist -3.4225475788116455<tensor([  0.,   0.,   0.,   0., 388., 380.,   0.,   0.,   0.,   0.])>4.346991062164307
INFO nextres 100.53083801269531 attn*mlp 12.355347633361816 layernormed 0.9994722604751587
			attn_hist -41.0625<tensor([  0.,   0.,   2.,  31., 355., 347.,  32.,   1.,   0.,   0.])>52.125
			mlp_hist -12.5<tensor([  0.,   0.,   0.,   2., 380., 384.,   1.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.5
			x_hist -3.412898063659668<tensor([  0.,   0.,   0.,   0., 386., 382.,   0.,   0.,   0.,   0.])>4.341824531555176
INFO nextres 112.4734115600586 attn*mlp 12.459683418273926 layernormed 0.9994537234306335
			attn_hist -40.875<tensor([  0.,   0.,   2.,  32., 352., 348.,  33.,   1.,   0.,   0.])>52.125
			mlp_hist -12.6875<tensor([  0.,   0.,   0.,   2., 380., 384.,   1.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.4375
			x_hist -3.402458906173706<tensor([  0.,   0.,   0.,   0., 387., 381.,   0.,   0.,   0.,   0.])>4.3351922035217285
INFO nextres 124.50640869140625 attn*mlp 12.556107521057129 layernormed 0.9994349479675293
			attn_hist -40.875<tensor([  0.,   0.,   1.,  33., 353., 345.,  35.,   1.,   0.,   0.])>52.125
			mlp_hist -12.8125<tensor([  0.,   0.,   0.,   3., 378., 386.,   1.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.375
			x_hist -3.392791986465454<tensor([  0.,   0.,   0.,   0., 388., 380.,   0.,   0.,   0.,   0.])>4.327638626098633
INFO nextres 136.6199493408203 attn*mlp 12.641912460327148 layernormed 0.9994159936904907
			attn_hist -40.6875<tensor([  0.,   0.,   1.,  34., 353., 344.,  35.,   1.,   0.,   0.])>51.75
			mlp_hist -12.875<tensor([  0.,   0.,   0.,   3., 380., 382.,   3.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>10.375
			x_hist -3.3828887939453125<tensor([  0.,   0.,   0.,   0., 389., 379.,   0.,   0.,   0.,   0.])>4.317470550537109
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		(,:0.1348 are:0.9219 red:0.6055 and:0.1426 Pot:0.2266,:0.3223 are:0.9688 :0.1167)
		(,:0.0630 are:0.6211 red:0.3887 and:0.2695 Pot:0.1895,:0.2539 are:0.7539 :0.1289)
		( Roses:0.0649 are:0.3535 red:0.2344 and:0.2080 Pot:0.1377,:0.2119 are:0.4883 Pot:0.1338)
		(,:0.0498 are:0.2168 red:0.1592 and:0.1670 Pot:0.0962,:0.1992 are:0.3223 Pot:0.1299)
		(,:0.0547 are:0.1494 red:0.1089 and:0.1377 Pot:0.0708,:0.1924 are:0.2305 Pot:0.1279)
		(�:0.0762 are:0.1094 red:0.0781 and:0.1113 Pot:0.0574,:0.1855 are:0.1729 Pot:0.1191)
		(�:0.0991 are:0.0869 red:0.0598 and:0.0962 Pot:0.0459,:0.1787 are:0.1406 Pot:0.1123)
		(�:0.1250 are:0.0737 red:0.0486 and:0.0830 Pot:0.0396,:0.1748 are:0.1177 Pot:0.1060)
		(�:0.1445 are:0.0659 red:0.0408 and:0.0752 Pot:0.0334,:0.1699 are:0.1030 Pot:0.1030)
		(�:0.1641 are:0.0581 red:0.0354 and:0.0684 Pot:0.0294,:0.1592 are:0.0889 Pot:0.0981)
		(�:0.1768 are:0.0525,:0.0306 and:0.0620 Pot:0.0269,:0.1562 are:0.0811 Pot:0.0938)
		(�:0.1904 are:0.0471,:0.0330 and:0.0581 Pot:0.0245,:0.1465 are:0.0713 Pot:0.0889)
		(�:0.1904 are:0.0471,:0.0330 and:0.0581 Pot:0.0245,:0.1465 are:0.0713 Pot:0.0889)
 Pot
------
		( are:0.9219 red:0.6055 and:0.1426 Pot:0.2266,:0.3223 are:0.9688 :0.1167 Pot:0.5469)
		( are:0.6211 red:0.3887 and:0.2695 Pot:0.1895,:0.2539 are:0.7539 :0.1289 Pot:0.4551)
		( are:0.3535 red:0.2344 and:0.2080 Pot:0.1377,:0.2119 are:0.4883 Pot:0.1338 Pot:0.3496)
		( are:0.2168 red:0.1592 and:0.1670 Pot:0.0962,:0.1992 are:0.3223 Pot:0.1299 Pot:0.2812)
		( are:0.1494 red:0.1089 and:0.1377 Pot:0.0708,:0.1924 are:0.2305 Pot:0.1279 Pot:0.2324)
		( are:0.1094 red:0.0781 and:0.1113 Pot:0.0574,:0.1855 are:0.1729 Pot:0.1191 Pot:0.1895)
		( are:0.0869 red:0.0598 and:0.0962 Pot:0.0459,:0.1787 are:0.1406 Pot:0.1123atoes:0.1963)
		( are:0.0737 red:0.0486 and:0.0830 Pot:0.0396,:0.1748 are:0.1177 Pot:0.1060atoes:0.1943)
		( are:0.0659 red:0.0408 and:0.0752 Pot:0.0334,:0.1699 are:0.1030 Pot:0.1030atoes:0.1934)
		( are:0.0581 red:0.0354 and:0.0684 Pot:0.0294,:0.1592 are:0.0889 Pot:0.0981atoes:0.1914)
		( are:0.0525,:0.0306 and:0.0620 Pot:0.0269,:0.1562 are:0.0811 Pot:0.0938atoes:0.1895)
		( are:0.0471,:0.0330 and:0.0581 Pot:0.0245,:0.1465 are:0.0713 Pot:0.0889atoes:0.1787)
		( are:0.0471,:0.0330 and:0.0581 Pot:0.0245,:0.1465 are:0.0713 Pot:0.0889atoes:0.1787)
atoes, and are are are also are also are also are also are also
@ 550 train 5.4704 , allloss: 5.4704, norm:0.3639, dt: 1933.60ms, tok/sec: 67786.35, flops:29.36, batch-reuse:1
@ 551 train 5.5432 , allloss: 5.5432, norm:0.3460, dt: 1365.84ms, tok/sec: 95964.04, flops:41.56, batch-reuse:1
@ 552 train 5.4132 , allloss: 5.4132, norm:0.3312, dt: 1365.74ms, tok/sec: 95971.24, flops:41.57, batch-reuse:1
@ 553 train 5.5496 , allloss: 5.5496, norm:0.3116, dt: 1366.11ms, tok/sec: 95945.16, flops:41.56, batch-reuse:1
@ 554 train 5.4410 , allloss: 5.4410, norm:0.3673, dt: 1366.12ms, tok/sec: 95944.46, flops:41.56, batch-reuse:1
@ 555 train 5.4849 , allloss: 5.4849, norm:0.3186, dt: 1365.81ms, tok/sec: 95966.72, flops:41.57, batch-reuse:1
@ 556 train 5.4038 , allloss: 5.4038, norm:0.4132, dt: 1366.01ms, tok/sec: 95952.57, flops:41.56, batch-reuse:1
