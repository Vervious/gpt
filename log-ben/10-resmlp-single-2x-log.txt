Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 10-resmlp-single-2x
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, y = res+2*attn(x), x = x + mlp(LN(res)), ELEMENTWISEAFFINE=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1num decayed parameter tensors: 10, with 53,575,680 parameters
num non-decayed parameter tensors: 8, with 13,824 parameters
