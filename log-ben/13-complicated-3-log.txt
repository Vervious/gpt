Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 13-complicated-3
MLPSCALE: 4
Experiment description: Transformer, max LR 6e-4
Setting:
========
y = self.ln_1(x)
newemb = self.attn(y)
mlp=self.mlp(self.ln_2(newemb))
x = mlp*newemb + x
======== 
VALUEMATRIX=True
REUSE_WEIGHTS=False
MLP_SCALE=4
Warmup steps: 100
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1{'block_size': 1024, 'vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768}
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
@ 0 train 11.5996 , allloss: 11.5996, norm:1.4327, dt: 1549.09ms, tok/sec: 84612.16, flops:74.11, batch-reuse:1
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Roses:0.5977 are:0.5391 red:0.2148,:0.4883 Pot:0.5391atoes:0.2617 are:0.6133 :0.5508)
 
------
		( are:0.5391 red:0.2148,:0.4883 Pot:0.5391atoes:0.2617 are:0.6133 :0.5508 :0.5039)
               
@ 1 train 11.5820 , allloss: 11.5820, norm:1.5540, dt: 753.76ms, tok/sec: 173889.83, flops:152.31, batch-reuse:1
@ 2 train 11.5447 , allloss: 11.5447, norm:1.7996, dt: 444.84ms, tok/sec: 294649.16, flops:258.09, batch-reuse:1
@ 3 train 11.4608 , allloss: 11.4608, norm:2.7947, dt: 447.37ms, tok/sec: 292985.74, flops:256.63, batch-reuse:1
@ 4 train 11.2097 , allloss: 11.2097, norm:6.3288, dt: 447.50ms, tok/sec: 292896.93, flops:256.55, batch-reuse:1
@ 5 train 10.6196 , allloss: 10.6196, norm:9.1438, dt: 450.63ms, tok/sec: 290861.32, flops:254.77, batch-reuse:1
@ 6 train 10.0177 , allloss: 10.0177, norm:4.1842, dt: 451.70ms, tok/sec: 290176.90, flops:254.17, batch-reuse:1
@ 7 train 9.8315 , allloss: 9.8315, norm:2.9795, dt: 448.47ms, tok/sec: 292261.63, flops:256.00, batch-reuse:1
@ 8 train 9.7224 , allloss: 9.7224, norm:2.4643, dt: 451.43ms, tok/sec: 290345.94, flops:254.32, batch-reuse:1
@ 9 train 9.5970 , allloss: 9.5970, norm:2.2286, dt: 451.59ms, tok/sec: 290244.16, flops:254.23, batch-reuse:1
@ 10 train 9.5457 , allloss: 9.5457, norm:2.0982, dt: 452.08ms, tok/sec: 289932.81, flops:253.96, batch-reuse:1
@ 11 train 9.5251 , allloss: 9.5251, norm:1.9949, dt: 449.86ms, tok/sec: 291360.31, flops:255.21, batch-reuse:1
@ 12 train 9.4259 , allloss: 9.4259, norm:2.1245, dt: 450.19ms, tok/sec: 291149.68, flops:255.02, batch-reuse:1
@ 13 train 9.4026 , allloss: 9.4026, norm:2.0913, dt: 449.16ms, tok/sec: 291818.40, flops:255.61, batch-reuse:1
@ 14 train 9.3347 , allloss: 9.3347, norm:2.0524, dt: 449.33ms, tok/sec: 291704.90, flops:255.51, batch-reuse:1
@ 15 train 9.2850 , allloss: 9.2850, norm:1.9936, dt: 447.35ms, tok/sec: 292998.70, flops:256.64, batch-reuse:1
@ 16 train 9.2197 , allloss: 9.2197, norm:1.9037, dt: 449.70ms, tok/sec: 291467.82, flops:255.30, batch-reuse:1
@ 17 train 9.1452 , allloss: 9.1452, norm:1.8596, dt: 449.50ms, tok/sec: 291592.73, flops:255.41, batch-reuse:1
@ 18 train 9.0888 , allloss: 9.0888, norm:1.8036, dt: 451.39ms, tok/sec: 290373.85, flops:254.34, batch-reuse:1
@ 19 train 8.9644 , allloss: 8.9644, norm:1.8441, dt: 458.90ms, tok/sec: 285619.93, flops:250.18, batch-reuse:1
@ 20 train 8.9089 , allloss: 8.9089, norm:1.8393, dt: 470.11ms, tok/sec: 278812.23, flops:244.22, batch-reuse:1
@ 21 train 8.8055 , allloss: 8.8055, norm:1.8237, dt: 453.01ms, tok/sec: 289337.09, flops:253.43, batch-reuse:1
@ 22 train 8.7341 , allloss: 8.7341, norm:1.6948, dt: 449.02ms, tok/sec: 291907.34, flops:255.69, batch-reuse:1
@ 23 train 8.7330 , allloss: 8.7330, norm:1.5910, dt: 448.54ms, tok/sec: 292220.62, flops:255.96, batch-reuse:1
@ 24 train 8.6383 , allloss: 8.6383, norm:1.4782, dt: 457.91ms, tok/sec: 286239.77, flops:250.72, batch-reuse:1
