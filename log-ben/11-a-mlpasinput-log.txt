Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 11-a-mlpasinput
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, 
Setting:
========
mlp = self.mlp(x)
attn = self.attn(mlp, mlp)
y = attn
x = res + y
newres = x
x = RMSNorm(x, ELEMENTWISEAFFINE=False), 
======== 
VALUEMATRIX=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1num decayed parameter tensors: 10, with 52,396,032 parameters
num non-decayed parameter tensors: 8, with 12,288 parameters
@ 0 train 10.9874 , allloss: 10.9874, norm:22.8366, dt: 2940.51ms, tok/sec: 44574.64, flops:19.31, batch-reuse:1
INFO nextres 0.5086966753005981 attn*mlp 0.5076587200164795 layernormed 1.0004918575286865
			attn_hist -3.2109375<tensor([  0.,   0.,   0.,   0., 395., 373.,   0.,   0.,   0.,   0.])>2.91796875
			mlp_hist -0.267578125<tensor([  0.,   0.,   0.,   0.,  28., 740.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2431640625
			x_hist -3.641479730606079<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>3.3590874671936035
INFO nextres 1.1883771419525146 attn*mlp 0.9197242259979248 layernormed 1.000279426574707
			attn_hist -2.63671875<tensor([  0.,   0.,   0.,   0., 392., 376.,   0.,   0.,   0.,   0.])>2.54296875
			mlp_hist -0.2197265625<tensor([  0.,   0.,   0.,   0.,  49., 720.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2119140625
			x_hist -2.7975785732269287<tensor([  0.,   0.,   0.,   0., 408., 360.,   0.,   0.,   0.,   0.])>2.968919038772583
INFO nextres 2.0434072017669678 attn*mlp 1.0239733457565308 layernormed 0.9996670484542847
			attn_hist -3.0<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>2.63671875
			mlp_hist -0.25<tensor([  0.,   0.,   0.,   0.,  64., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2197265625
			x_hist -2.884554862976074<tensor([  0.,   0.,   0.,   0., 409., 359.,   0.,   0.,   0.,   0.])>2.6525638103485107
INFO nextres 2.9890215396881104 attn*mlp 1.067815899848938 layernormed 0.9993542432785034
			attn_hist -2.80078125<tensor([  0.,   0.,   0.,   0., 404., 364.,   0.,   0.,   0.,   0.])>2.87109375
			mlp_hist -0.2333984375<tensor([  0.,   0.,   0.,   0.,  71., 696.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2392578125
			x_hist -2.896571636199951<tensor([  0.,   0.,   0.,   0., 403., 365.,   0.,   0.,   0.,   0.])>2.828737735748291
INFO nextres 3.9767653942108154 attn*mlp 1.0864695310592651 layernormed 0.9993258118629456
			attn_hist -2.89453125<tensor([  0.,   0.,   0.,   0., 396., 372.,   0.,   0.,   0.,   0.])>3.3046875
			mlp_hist -0.2412109375<tensor([  0.,   0.,   0.,   0.,  68., 700.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.275390625
			x_hist -2.873898983001709<tensor([  0.,   0.,   0.,   0., 397., 371.,   0.,   0.,   0.,   0.])>3.024406671524048
INFO nextres 4.985732078552246 attn*mlp 1.0922551155090332 layernormed 0.999409556388855
			attn_hist -2.91796875<tensor([  0.,   0.,   0.,   0., 391., 377.,   0.,   0.,   0.,   0.])>3.4921875
			mlp_hist -0.2431640625<tensor([  0.,   0.,   0.,   0.,  63., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.291015625
			x_hist -2.823951244354248<tensor([  0.,   0.,   0.,   0., 404., 364.,   0.,   0.,   0.,   0.])>3.1585769653320312
INFO nextres 6.004549980163574 attn*mlp 1.0911823511123657 layernormed 0.9995251297950745
			attn_hist -2.94140625<tensor([  0.,   0.,   0.,   0., 384., 384.,   0.,   0.,   0.,   0.])>3.515625
			mlp_hist -0.2451171875<tensor([  0.,   0.,   0.,   0.,  64., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.29296875
			x_hist -2.763237953186035<tensor([  0.,   0.,   0.,   0., 405., 363.,   0.,   0.,   0.,   0.])>3.239802598953247
INFO nextres 7.027956962585449 attn*mlp 1.0877294540405273 layernormed 0.999641478061676
			attn_hist -2.96484375<tensor([  0.,   0.,   0.,   0., 379., 389.,   0.,   0.,   0.,   0.])>3.4921875
			mlp_hist -0.2470703125<tensor([  0.,   0.,   0.,   0.,  66., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.291015625
			x_hist -2.698801040649414<tensor([  0.,   0.,   0.,   0., 399., 369.,   0.,   0.,   0.,   0.])>3.287428379058838
INFO nextres 8.05300521850586 attn*mlp 1.083275318145752 layernormed 0.999748706817627
			attn_hist -3.0<tensor([  0.,   0.,   0.,   0., 378., 390.,   0.,   0.,   0.,   0.])>3.4453125
			mlp_hist -0.25<tensor([  0.,   0.,   0.,   0.,  68., 700.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.287109375
			x_hist -2.634218692779541<tensor([  0.,   0.,   0.,   0., 394., 374.,   0.,   0.,   0.,   0.])>3.3132565021514893
INFO nextres 9.078205108642578 attn*mlp 1.078689694404602 layernormed 0.9998437762260437
			attn_hist -3.0234375<tensor([  0.,   0.,   0.,   0., 381., 387.,   0.,   0.,   0.,   0.])>3.375
			mlp_hist -0.251953125<tensor([  0.,   0.,   0.,   0.,  66., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.28125
			x_hist -2.6355316638946533<tensor([  0.,   0.,   0.,   0., 396., 372.,   0.,   0.,   0.,   0.])>3.323228120803833
INFO nextres 10.102691650390625 attn*mlp 1.0741596221923828 layernormed 0.9999271631240845
			attn_hist -3.046875<tensor([  0.,   0.,   0.,   0., 386., 382.,   0.,   0.,   0.,   0.])>3.328125
			mlp_hist -0.25390625<tensor([  0.,   0.,   0.,   0.,  63., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.27734375
			x_hist -2.6707170009613037<tensor([  0.,   0.,   0.,   0., 393., 375.,   0.,   0.,   0.,   0.])>3.3252336978912354
INFO nextres 11.126275062561035 attn*mlp 1.070141077041626 layernormed 1.000000238418579
			attn_hist -3.0703125<tensor([  0.,   0.,   0.,   0., 387., 381.,   0.,   0.,   0.,   0.])>3.28125
			mlp_hist -0.255859375<tensor([  0.,   0.,   0.,   0.,  64., 704.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2734375
			x_hist -2.6998186111450195<tensor([  0.,   0.,   0.,   0., 389., 379.,   0.,   0.,   0.,   0.])>3.321712017059326
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Kos:0.0003 Kos:0.0002 Kos:0.0002 Kos:0.0002Mary:0.0002 Kos:0.0002 nationalism:0.0002 reveal:0.0002)
		(Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0003.:0.0003.:0.0003.:0.0003)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0003.:0.0003.:0.0003.:0.0003)
.
------
		( Kos:0.0002 Kos:0.0002 Kos:0.0002Mary:0.0002 Kos:0.0002 nationalism:0.0002 reveal:0.0002 Kos:0.0002)
		(Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002Fighting:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002)
		( Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002 Sear:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0003)
		(.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0002.:0.0003)
		(.:0.0002.:0.0002.:0.0002.:0.0003.:0.0003.:0.0003.:0.0003.:0.0003)
		(.:0.0002.:0.0002.:0.0002.:0.0003.:0.0003.:0.0003.:0.0003.:0.0003)
...............
@ 1 train 10.5115 , allloss: 10.5115, norm:12.8877, dt: 2134.15ms, tok/sec: 61416.58, flops:26.60, batch-reuse:1
@ 2 train 10.1055 , allloss: 10.1055, norm:7.6463, dt: 1314.47ms, tok/sec: 99714.44, flops:43.19, batch-reuse:1
@ 3 train 9.7669 , allloss: 9.7669, norm:4.4612, dt: 1314.63ms, tok/sec: 99702.36, flops:43.18, batch-reuse:1
@ 4 train 9.5426 , allloss: 9.5426, norm:2.8503, dt: 1315.00ms, tok/sec: 99674.56, flops:43.17, batch-reuse:1
@ 5 train 9.4028 , allloss: 9.4028, norm:2.2126, dt: 1318.52ms, tok/sec: 99408.17, flops:43.06, batch-reuse:1
@ 6 train 9.0984 , allloss: 9.0984, norm:2.1082, dt: 1318.37ms, tok/sec: 99419.95, flops:43.06, batch-reuse:1
@ 7 train 8.8636 , allloss: 8.8636, norm:1.8422, dt: 1317.99ms, tok/sec: 99448.49, flops:43.07, batch-reuse:1
@ 8 train 8.6019 , allloss: 8.6019, norm:1.5817, dt: 1318.38ms, tok/sec: 99419.26, flops:43.06, batch-reuse:1
@ 9 train 8.3282 , allloss: 8.3282, norm:1.4064, dt: 1317.77ms, tok/sec: 99464.70, flops:43.08, batch-reuse:1
@ 10 train 8.1465 , allloss: 8.1465, norm:1.0997, dt: 1318.39ms, tok/sec: 99418.36, flops:43.06, batch-reuse:1
@ 11 train 7.9803 , allloss: 7.9803, norm:0.8750, dt: 1317.77ms, tok/sec: 99465.26, flops:43.08, batch-reuse:1
@ 12 train 7.8347 , allloss: 7.8347, norm:0.8112, dt: 1317.03ms, tok/sec: 99520.79, flops:43.11, batch-reuse:1
@ 13 train 7.7961 , allloss: 7.7961, norm:0.5947, dt: 1317.49ms, tok/sec: 99486.06, flops:43.09, batch-reuse:1
@ 14 train 7.7440 , allloss: 7.7440, norm:0.6261, dt: 1318.88ms, tok/sec: 99381.61, flops:43.05, batch-reuse:1
@ 15 train 7.7221 , allloss: 7.7221, norm:0.4961, dt: 1318.35ms, tok/sec: 99421.04, flops:43.06, batch-reuse:1
@ 16 train 7.7188 , allloss: 7.7188, norm:0.7354, dt: 1318.15ms, tok/sec: 99436.02, flops:43.07, batch-reuse:1
@ 17 train 7.7667 , allloss: 7.7667, norm:0.7825, dt: 1319.66ms, tok/sec: 99322.63, flops:43.02, batch-reuse:1
@ 18 train 7.7707 , allloss: 7.7707, norm:0.6269, dt: 1317.87ms, tok/sec: 99457.23, flops:43.08, batch-reuse:1
@ 19 train 7.6573 , allloss: 7.6573, norm:0.5964, dt: 1318.24ms, tok/sec: 99429.51, flops:43.07, batch-reuse:1
@ 20 train 7.7686 , allloss: 7.7686, norm:0.5389, dt: 1317.98ms, tok/sec: 99449.50, flops:43.07, batch-reuse:1
@ 21 train 7.7222 , allloss: 7.7222, norm:0.6306, dt: 1318.21ms, tok/sec: 99431.49, flops:43.07, batch-reuse:1
@ 22 train 7.6927 , allloss: 7.6927, norm:0.4215, dt: 1317.84ms, tok/sec: 99459.70, flops:43.08, batch-reuse:1
@ 23 train 7.9172 , allloss: 7.9172, norm:0.8412, dt: 1317.33ms, tok/sec: 99498.45, flops:43.10, batch-reuse:1
@ 24 train 7.8674 , allloss: 7.8674, norm:0.5942, dt: 1317.48ms, tok/sec: 99486.60, flops:43.09, batch-reuse:1
@ 25 train 7.6344 , allloss: 7.6344, norm:0.6651, dt: 1318.97ms, tok/sec: 99374.80, flops:43.04, batch-reuse:1
@ 26 train 7.7608 , allloss: 7.7608, norm:0.7920, dt: 1321.42ms, tok/sec: 99190.48, flops:42.96, batch-reuse:1
@ 27 train 7.7335 , allloss: 7.7335, norm:0.6843, dt: 1320.22ms, tok/sec: 99280.75, flops:43.00, batch-reuse:1
@ 28 train 7.7553 , allloss: 7.7553, norm:0.4525, dt: 1318.16ms, tok/sec: 99435.25, flops:43.07, batch-reuse:1
@ 29 train 7.8190 , allloss: 7.8190, norm:0.6321, dt: 1319.60ms, tok/sec: 99327.17, flops:43.02, batch-reuse:1
@ 30 train 7.7139 , allloss: 7.7139, norm:0.6632, dt: 1318.38ms, tok/sec: 99418.78, flops:43.06, batch-reuse:1
@ 31 train 7.7702 , allloss: 7.7702, norm:0.4400, dt: 1319.35ms, tok/sec: 99345.78, flops:43.03, batch-reuse:1
@ 32 train 7.7640 , allloss: 7.7640, norm:0.4201, dt: 1318.60ms, tok/sec: 99402.04, flops:43.05, batch-reuse:1
@ 33 train 7.7034 , allloss: 7.7034, norm:0.4662, dt: 1320.28ms, tok/sec: 99275.89, flops:43.00, batch-reuse:1
@ 34 train 7.7312 , allloss: 7.7312, norm:0.4350, dt: 1318.66ms, tok/sec: 99398.23, flops:43.05, batch-reuse:1
@ 35 train 7.6916 , allloss: 7.6916, norm:0.4651, dt: 1319.29ms, tok/sec: 99350.52, flops:43.03, batch-reuse:1
@ 36 train 7.6842 , allloss: 7.6842, norm:0.5553, dt: 1318.65ms, tok/sec: 99398.27, flops:43.05, batch-reuse:1
@ 37 train 7.7263 , allloss: 7.7263, norm:0.4392, dt: 1318.75ms, tok/sec: 99390.99, flops:43.05, batch-reuse:1
@ 38 train 7.6547 , allloss: 7.6547, norm:0.3619, dt: 1319.76ms, tok/sec: 99315.11, flops:43.02, batch-reuse:1
@ 39 train 7.7263 , allloss: 7.7263, norm:0.5309, dt: 1319.89ms, tok/sec: 99305.12, flops:43.01, batch-reuse:1
@ 40 train 7.7551 , allloss: 7.7551, norm:0.5321, dt: 1321.71ms, tok/sec: 99168.56, flops:42.95, batch-reuse:1
@ 41 train 7.7145 , allloss: 7.7145, norm:0.3773, dt: 1321.16ms, tok/sec: 99209.82, flops:42.97, batch-reuse:1
@ 42 train 7.7051 , allloss: 7.7051, norm:0.3812, dt: 1319.91ms, tok/sec: 99304.02, flops:43.01, batch-reuse:1
@ 43 train 7.7279 , allloss: 7.7279, norm:0.3268, dt: 1321.26ms, tok/sec: 99202.01, flops:42.97, batch-reuse:1
@ 44 train 7.7397 , allloss: 7.7397, norm:0.4099, dt: 1322.12ms, tok/sec: 99137.88, flops:42.94, batch-reuse:1
@ 45 train 7.6640 , allloss: 7.6640, norm:0.3529, dt: 1321.59ms, tok/sec: 99177.71, flops:42.96, batch-reuse:1
@ 46 train 7.7160 , allloss: 7.7160, norm:0.3889, dt: 1322.29ms, tok/sec: 99124.65, flops:42.93, batch-reuse:1
@ 47 train 7.6542 , allloss: 7.6542, norm:0.4007, dt: 1323.05ms, tok/sec: 99067.76, flops:42.91, batch-reuse:1
@ 48 train 7.6644 , allloss: 7.6644, norm:0.3464, dt: 1323.10ms, tok/sec: 99063.97, flops:42.91, batch-reuse:1
@ 49 train 7.6793 , allloss: 7.6793, norm:0.2563, dt: 1324.14ms, tok/sec: 98986.24, flops:42.87, batch-reuse:1
INFO nextres 14.161012649536133 attn*mlp 14.160792350769043 layernormed 1.0004165172576904
			attn_hist -68.25<tensor([  0.,   6.,  26.,  76., 273., 288.,  72.,  23.,   4.,   0.])>67.5
			mlp_hist -5.6875<tensor([  0.,   0.,   0.,   0., 344., 424.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>5.625
			x_hist -3.3357973098754883<tensor([  0.,   0.,   0.,   0., 379., 389.,   0.,   0.,   0.,   0.])>3.2982470989227295
INFO nextres 59.05318069458008 attn*mlp 46.80094909667969 layernormed 1.0003501176834106
			attn_hist -189.0<tensor([ 28.,  26.,  36.,  57., 182., 205.,  68.,  34.,  31.,  23.])>183.75
			mlp_hist -15.75<tensor([  0.,   0.,   0.,  22., 336., 388.,  21.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.3125
			x_hist -3.3545103073120117<tensor([  0.,   0.,   0.,   0., 377., 391.,   0.,   0.,   0.,   0.])>3.276151180267334
INFO nextres 114.72254180908203 attn*mlp 55.898773193359375 layernormed 1.0003818273544312
			attn_hist -187.5<tensor([ 28.,  28.,  34.,  58., 185., 204.,  68.,  35.,  31.,  23.])>182.25
			mlp_hist -15.625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.1875
			x_hist -3.3662331104278564<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>3.2809982299804688
INFO nextres 170.0947265625 attn*mlp 55.44535446166992 layernormed 1.000392198562622
			attn_hist -186.75<tensor([ 28.,  28.,  34.,  58., 185., 204.,  69.,  34.,  31.,  23.])>181.5
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.125
			x_hist -3.370572090148926<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.282439947128296
INFO nextres 225.3477325439453 attn*mlp 55.289581298828125 layernormed 1.0003974437713623
			attn_hist -186.75<tensor([ 28.,  28.,  34.,  58., 185., 204.,  69.,  34.,  31.,  23.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.374027967453003<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.281193733215332
INFO nextres 280.5312194824219 attn*mlp 55.205535888671875 layernormed 1.0004003047943115
			attn_hist -186.75<tensor([ 28.,  28.,  34.,  58., 185., 204.,  69.,  34.,  32.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3768043518066406<tensor([  0.,   0.,   0.,   0., 374., 394.,   0.,   0.,   0.,   0.])>3.2809953689575195
INFO nextres 335.6649475097656 attn*mlp 55.14851760864258 layernormed 1.0004023313522339
			attn_hist -186.75<tensor([ 28.,  28.,  34.,  58., 185., 204.,  69.,  34.,  32.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3790464401245117<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.2811832427978516
INFO nextres 390.7698059082031 attn*mlp 55.11555099487305 layernormed 1.0004037618637085
			attn_hist -186.75<tensor([ 28.,  28.,  34.,  58., 185., 204.,  69.,  34.,  32.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3808884620666504<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.281520366668701
INFO nextres 445.8603820800781 attn*mlp 55.098663330078125 layernormed 1.000404715538025
			attn_hist -186.75<tensor([ 28.,  27.,  35.,  58., 185., 204.,  69.,  34.,  32.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3823165893554688<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.2818009853363037
INFO nextres 500.933349609375 attn*mlp 55.07929611206055 layernormed 1.0004055500030518
			attn_hist -186.75<tensor([ 28.,  27.,  35.,  58., 185., 205.,  68.,  34.,  32.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3835268020629883<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.2821054458618164
INFO nextres 555.9945678710938 attn*mlp 55.0664176940918 layernormed 1.0004061460494995
			attn_hist -186.75<tensor([ 28.,  27.,  35.,  58., 184., 206.,  68.,  35.,  31.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.3845582008361816<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.2824034690856934
INFO nextres 611.046630859375 attn*mlp 55.05628204345703 layernormed 1.0004066228866577
			attn_hist -186.75<tensor([ 28.,  27.,  35.,  58., 184., 206.,  68.,  35.,  31.,  22.])>180.75
			mlp_hist -15.5625<tensor([  0.,   0.,   0.,  20., 334., 394.,  20.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>15.0625
			x_hist -3.385451078414917<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>3.2826902866363525
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		(,:0.0312SpaceEngineers:0.0003 the:0.0242"]:0.0002,:0.0275 the:0.0277"]:0.0002 the:0.0206)
		( the:0.0383 the:0.0332.:0.0339 the:0.0308.:0.0359.:0.0349 the:0.0317.:0.0320)
		( the:0.0393 the:0.0376,:0.0366 the:0.0359,:0.0369,:0.0369 the:0.0356 the:0.0361)
		( the:0.0405 the:0.0393,:0.0376 the:0.0371,:0.0378,:0.0378 the:0.0383 the:0.0381)
		( the:0.0403 the:0.0400 the:0.0388 the:0.0391 the:0.0391,:0.0378 the:0.0391 the:0.0381)
		( the:0.0403 the:0.0400 the:0.0391 the:0.0388 the:0.0391 the:0.0391 the:0.0388 the:0.0393)
		( the:0.0403 the:0.0396 the:0.0403 the:0.0400 the:0.0391 the:0.0391 the:0.0400 the:0.0391)
		( the:0.0403 the:0.0408 the:0.0403 the:0.0398 the:0.0391 the:0.0391 the:0.0398 the:0.0391)
		( the:0.0403 the:0.0408 the:0.0403 the:0.0396 the:0.0403 the:0.0391 the:0.0396 the:0.0403)
		( the:0.0403 the:0.0405 the:0.0403 the:0.0396 the:0.0403 the:0.0403 the:0.0396 the:0.0403)
		( the:0.0403 the:0.0405 the:0.0403 the:0.0396 the:0.0403 the:0.0403 the:0.0396 the:0.0403)
		( the:0.0403 the:0.0405 the:0.0403 the:0.0408 the:0.0403 the:0.0403 the:0.0408 the:0.0403)
		( the:0.0403 the:0.0405 the:0.0403 the:0.0408 the:0.0403 the:0.0403 the:0.0408 the:0.0403)
 the
------
		(SpaceEngineers:0.0003 the:0.0242"]:0.0002,:0.0275 the:0.0277"]:0.0002 the:0.0206"]:0.0002)
		( the:0.0332.:0.0339 the:0.0308.:0.0359.:0.0349 the:0.0317.:0.0320 the:0.0317)
		( the:0.0376,:0.0366 the:0.0359,:0.0369,:0.0369 the:0.0356 the:0.0361 the:0.0356)
		( the:0.0393,:0.0376 the:0.0371,:0.0378,:0.0378 the:0.0383 the:0.0381 the:0.0383)
		( the:0.0400 the:0.0388 the:0.0391 the:0.0391,:0.0378 the:0.0391 the:0.0381 the:0.0391)
		( the:0.0400 the:0.0391 the:0.0388 the:0.0391 the:0.0391 the:0.0388 the:0.0393 the:0.0388)
		( the:0.0396 the:0.0403 the:0.0400 the:0.0391 the:0.0391 the:0.0400 the:0.0391 the:0.0400)
		( the:0.0408 the:0.0403 the:0.0398 the:0.0391 the:0.0391 the:0.0398 the:0.0391 the:0.0398)
		( the:0.0408 the:0.0403 the:0.0396 the:0.0403 the:0.0391 the:0.0396 the:0.0403 the:0.0396)
		( the:0.0405 the:0.0403 the:0.0396 the:0.0403 the:0.0403 the:0.0396 the:0.0403 the:0.0396)
		( the:0.0405 the:0.0403 the:0.0396 the:0.0403 the:0.0403 the:0.0396 the:0.0403 the:0.0396)
		( the:0.0405 the:0.0403 the:0.0408 the:0.0403 the:0.0403 the:0.0408 the:0.0403 the:0.0408)
		( the:0.0405 the:0.0403 the:0.0408 the:0.0403 the:0.0403 the:0.0408 the:0.0403 the:0.0408)
 the the the the the the the the the the the the the the the
@ 50 train 7.7928 , allloss: 7.7928, norm:0.3598, dt: 1905.62ms, tok/sec: 68781.76, flops:29.79, batch-reuse:1
@ 51 train 7.8549 , allloss: 7.8549, norm:0.4281, dt: 1325.04ms, tok/sec: 98918.91, flops:42.84, batch-reuse:1
@ 52 train 7.6970 , allloss: 7.6970, norm:0.4625, dt: 1325.43ms, tok/sec: 98889.80, flops:42.83, batch-reuse:1
@ 53 train 7.6732 , allloss: 7.6732, norm:0.3133, dt: 1325.42ms, tok/sec: 98890.80, flops:42.83, batch-reuse:1
@ 54 train 7.7650 , allloss: 7.7650, norm:0.2713, dt: 1324.96ms, tok/sec: 98925.23, flops:42.85, batch-reuse:1
@ 55 train 7.8201 , allloss: 7.8201, norm:0.4009, dt: 1325.77ms, tok/sec: 98864.89, flops:42.82, batch-reuse:1
@ 56 train 7.6820 , allloss: 7.6820, norm:0.3672, dt: 1326.02ms, tok/sec: 98846.26, flops:42.81, batch-reuse:1
@ 57 train 7.6095 , allloss: 7.6095, norm:0.3950, dt: 1327.06ms, tok/sec: 98768.71, flops:42.78, batch-reuse:1
@ 58 train 7.6847 , allloss: 7.6847, norm:0.4852, dt: 1325.78ms, tok/sec: 98863.70, flops:42.82, batch-reuse:1
@ 59 train 7.4939 , allloss: 7.4939, norm:0.3838, dt: 1325.80ms, tok/sec: 98862.93, flops:42.82, batch-reuse:1
@ 60 train 7.6800 , allloss: 7.6800, norm:0.3365, dt: 1326.33ms, tok/sec: 98822.84, flops:42.80, batch-reuse:1
@ 61 train 7.6358 , allloss: 7.6358, norm:0.3058, dt: 1328.10ms, tok/sec: 98691.17, flops:42.75, batch-reuse:1
@ 62 train 7.5976 , allloss: 7.5976, norm:0.7980, dt: 1328.38ms, tok/sec: 98670.82, flops:42.74, batch-reuse:1
@ 63 train 7.6609 , allloss: 7.6609, norm:0.4588, dt: 1329.19ms, tok/sec: 98610.80, flops:42.71, batch-reuse:1
@ 64 train 7.6320 , allloss: 7.6320, norm:0.4022, dt: 1329.39ms, tok/sec: 98595.58, flops:42.70, batch-reuse:1
@ 65 train 7.7270 , allloss: 7.7270, norm:0.4371, dt: 1330.40ms, tok/sec: 98520.98, flops:42.67, batch-reuse:1
@ 66 train 7.7340 , allloss: 7.7340, norm:0.4416, dt: 1330.38ms, tok/sec: 98521.93, flops:42.67, batch-reuse:1
@ 67 train 7.6346 , allloss: 7.6346, norm:0.4753, dt: 1330.63ms, tok/sec: 98503.45, flops:42.66, batch-reuse:1
@ 68 train 7.6867 , allloss: 7.6867, norm:0.6287, dt: 1329.48ms, tok/sec: 98588.96, flops:42.70, batch-reuse:1
@ 69 train 7.5924 , allloss: 7.5924, norm:0.5472, dt: 1329.93ms, tok/sec: 98555.79, flops:42.69, batch-reuse:1
@ 70 train 7.6017 , allloss: 7.6017, norm:0.4817, dt: 1330.15ms, tok/sec: 98539.36, flops:42.68, batch-reuse:1
@ 71 train 7.5701 , allloss: 7.5701, norm:0.4638, dt: 1330.72ms, tok/sec: 98497.31, flops:42.66, batch-reuse:1
@ 72 train 7.6045 , allloss: 7.6045, norm:0.4913, dt: 1331.40ms, tok/sec: 98446.40, flops:42.64, batch-reuse:1
@ 73 train 7.6860 , allloss: 7.6860, norm:0.6797, dt: 1331.55ms, tok/sec: 98435.35, flops:42.64, batch-reuse:1
@ 74 train 7.6270 , allloss: 7.6270, norm:0.4538, dt: 1330.48ms, tok/sec: 98514.46, flops:42.67, batch-reuse:1
@ 75 train 8.1708 , allloss: 8.1708, norm:0.9649, dt: 1331.10ms, tok/sec: 98469.22, flops:42.65, batch-reuse:1
@ 76 train 7.6583 , allloss: 7.6583, norm:1.6012, dt: 1331.29ms, tok/sec: 98454.90, flops:42.64, batch-reuse:1
@ 77 train 8.4356 , allloss: 8.4356, norm:1.2789, dt: 1331.08ms, tok/sec: 98470.15, flops:42.65, batch-reuse:1
@ 78 train 7.9706 , allloss: 7.9706, norm:0.9570, dt: 1334.28ms, tok/sec: 98234.38, flops:42.55, batch-reuse:1
@ 79 train 7.5280 , allloss: 7.5280, norm:0.8946, dt: 1334.86ms, tok/sec: 98191.79, flops:42.53, batch-reuse:1
@ 80 train 7.5536 , allloss: 7.5536, norm:0.7228, dt: 1338.10ms, tok/sec: 97954.11, flops:42.43, batch-reuse:1
@ 81 train 7.5701 , allloss: 7.5701, norm:0.7715, dt: 1340.18ms, tok/sec: 97801.72, flops:42.36, batch-reuse:1
@ 82 train 7.5373 , allloss: 7.5373, norm:0.5946, dt: 1342.42ms, tok/sec: 97638.26, flops:42.29, batch-reuse:1
@ 83 train 7.5266 , allloss: 7.5266, norm:0.5969, dt: 1344.19ms, tok/sec: 97510.10, flops:42.23, batch-reuse:1
@ 84 train 7.5809 , allloss: 7.5809, norm:0.7237, dt: 1344.02ms, tok/sec: 97522.21, flops:42.24, batch-reuse:1
@ 85 train 7.5158 , allloss: 7.5158, norm:0.6221, dt: 1343.86ms, tok/sec: 97534.01, flops:42.24, batch-reuse:1
@ 86 train 7.4768 , allloss: 7.4768, norm:0.3677, dt: 1344.33ms, tok/sec: 97500.14, flops:42.23, batch-reuse:1
@ 87 train 7.5717 , allloss: 7.5717, norm:0.5824, dt: 1344.60ms, tok/sec: 97480.59, flops:42.22, batch-reuse:1
@ 88 train 7.5061 , allloss: 7.5061, norm:0.5680, dt: 1345.19ms, tok/sec: 97437.46, flops:42.20, batch-reuse:1
@ 89 train 7.4578 , allloss: 7.4578, norm:0.4899, dt: 1344.59ms, tok/sec: 97480.69, flops:42.22, batch-reuse:1
@ 90 train 7.4948 , allloss: 7.4948, norm:0.5653, dt: 1343.10ms, tok/sec: 97588.81, flops:42.27, batch-reuse:1
@ 91 train 7.4551 , allloss: 7.4551, norm:0.6197, dt: 1344.47ms, tok/sec: 97489.54, flops:42.23, batch-reuse:1
@ 92 train 7.4500 , allloss: 7.4500, norm:0.3703, dt: 1343.72ms, tok/sec: 97544.05, flops:42.25, batch-reuse:1
@ 93 train 7.4448 , allloss: 7.4448, norm:0.4370, dt: 1344.60ms, tok/sec: 97480.17, flops:42.22, batch-reuse:1
@ 94 train 7.7189 , allloss: 7.7189, norm:0.8016, dt: 1344.77ms, tok/sec: 97467.75, flops:42.22, batch-reuse:1
@ 95 train 7.4502 , allloss: 7.4502, norm:0.5860, dt: 1346.20ms, tok/sec: 97364.57, flops:42.17, batch-reuse:1
@ 96 train 7.3582 , allloss: 7.3582, norm:0.7165, dt: 1345.35ms, tok/sec: 97425.76, flops:42.20, batch-reuse:1
@ 97 train 7.3919 , allloss: 7.3919, norm:0.4843, dt: 1346.85ms, tok/sec: 97317.62, flops:42.15, batch-reuse:1
@ 98 train 7.4383 , allloss: 7.4383, norm:0.5345, dt: 1345.75ms, tok/sec: 97397.35, flops:42.19, batch-reuse:1
@ 99 train 7.3958 , allloss: 7.3958, norm:0.5445, dt: 1345.71ms, tok/sec: 97399.92, flops:42.19, batch-reuse:1
@ 100 train 7.3925 , allloss: 7.3925, norm:0.5234, dt: 1345.10ms, tok/sec: 97443.75, flops:42.21, batch-reuse:1
@ 101 train 7.2906 , allloss: 7.2906, norm:0.5204, dt: 1346.73ms, tok/sec: 97326.34, flops:42.15, batch-reuse:1
@ 102 train 7.3520 , allloss: 7.3520, norm:0.4077, dt: 1347.07ms, tok/sec: 97301.76, flops:42.14, batch-reuse:1
@ 103 train 7.3603 , allloss: 7.3603, norm:0.3326, dt: 1346.17ms, tok/sec: 97366.90, flops:42.17, batch-reuse:1
@ 104 train 7.4487 , allloss: 7.4487, norm:0.6029, dt: 1347.11ms, tok/sec: 97298.78, flops:42.14, batch-reuse:1
@ 105 train 7.4204 , allloss: 7.4204, norm:0.5091, dt: 1347.34ms, tok/sec: 97281.72, flops:42.14, batch-reuse:1
@ 106 train 7.3644 , allloss: 7.3644, norm:0.3978, dt: 1344.89ms, tok/sec: 97459.18, flops:42.21, batch-reuse:1
@ 107 train 7.3488 , allloss: 7.3488, norm:0.4070, dt: 1343.73ms, tok/sec: 97543.68, flops:42.25, batch-reuse:1
@ 108 train 7.3232 , allloss: 7.3232, norm:0.4658, dt: 1345.70ms, tok/sec: 97400.88, flops:42.19, batch-reuse:1
@ 109 train 7.2778 , allloss: 7.2778, norm:0.3740, dt: 1343.70ms, tok/sec: 97545.35, flops:42.25, batch-reuse:1
@ 110 train 7.3424 , allloss: 7.3424, norm:0.5305, dt: 1344.20ms, tok/sec: 97509.41, flops:42.23, batch-reuse:1
@ 111 train 7.3342 , allloss: 7.3342, norm:0.4566, dt: 1342.71ms, tok/sec: 97617.69, flops:42.28, batch-reuse:1
@ 112 train 7.3472 , allloss: 7.3472, norm:0.3304, dt: 1344.58ms, tok/sec: 97481.83, flops:42.22, batch-reuse:1
@ 113 train 7.2880 , allloss: 7.2880, norm:0.4674, dt: 1343.18ms, tok/sec: 97583.37, flops:42.27, batch-reuse:1
@ 114 train 7.3136 , allloss: 7.3136, norm:0.7062, dt: 1341.29ms, tok/sec: 97720.66, flops:42.33, batch-reuse:1
@ 115 train 7.2565 , allloss: 7.2565, norm:0.4568, dt: 1342.36ms, tok/sec: 97643.03, flops:42.29, batch-reuse:1
@ 116 train 7.2299 , allloss: 7.2299, norm:0.6754, dt: 1343.98ms, tok/sec: 97525.51, flops:42.24, batch-reuse:1
@ 117 train 7.2533 , allloss: 7.2533, norm:0.4686, dt: 1343.92ms, tok/sec: 97529.84, flops:42.24, batch-reuse:1
@ 118 train 7.2716 , allloss: 7.2716, norm:0.4745, dt: 1343.91ms, tok/sec: 97530.19, flops:42.24, batch-reuse:1
@ 119 train 7.2340 , allloss: 7.2340, norm:0.5875, dt: 1343.67ms, tok/sec: 97547.68, flops:42.25, batch-reuse:1
@ 120 train 7.3842 , allloss: 7.3842, norm:0.4397, dt: 1343.50ms, tok/sec: 97559.90, flops:42.26, batch-reuse:1
@ 121 train 7.4766 , allloss: 7.4766, norm:0.5939, dt: 1344.02ms, tok/sec: 97522.25, flops:42.24, batch-reuse:1
@ 122 train 7.3985 , allloss: 7.3985, norm:0.5120, dt: 1344.89ms, tok/sec: 97459.02, flops:42.21, batch-reuse:1
@ 123 train 7.2627 , allloss: 7.2627, norm:0.4320, dt: 1344.89ms, tok/sec: 97458.97, flops:42.21, batch-reuse:1
@ 124 train 7.2945 , allloss: 7.2945, norm:0.7571, dt: 1345.32ms, tok/sec: 97428.29, flops:42.20, batch-reuse:1
@ 125 train 7.3021 , allloss: 7.3021, norm:0.4378, dt: 1345.74ms, tok/sec: 97397.50, flops:42.19, batch-reuse:1
@ 126 train 7.3204 , allloss: 7.3204, norm:0.4721, dt: 1344.67ms, tok/sec: 97474.92, flops:42.22, batch-reuse:1
@ 127 train 7.2314 , allloss: 7.2314, norm:0.6223, dt: 1345.40ms, tok/sec: 97422.30, flops:42.20, batch-reuse:1
@ 128 train 7.1865 , allloss: 7.1865, norm:0.4143, dt: 1344.43ms, tok/sec: 97492.55, flops:42.23, batch-reuse:1
@ 129 train 7.4076 , allloss: 7.4076, norm:0.6008, dt: 1344.06ms, tok/sec: 97519.74, flops:42.24, batch-reuse:1
@ 130 train 7.3059 , allloss: 7.3059, norm:0.3215, dt: 1345.22ms, tok/sec: 97435.17, flops:42.20, batch-reuse:1
@ 131 train 7.2857 , allloss: 7.2857, norm:0.5622, dt: 1344.78ms, tok/sec: 97466.95, flops:42.22, batch-reuse:1
@ 132 train 7.3342 , allloss: 7.3342, norm:0.4980, dt: 1343.25ms, tok/sec: 97577.95, flops:42.26, batch-reuse:1
@ 133 train 7.2402 , allloss: 7.2402, norm:0.4339, dt: 1343.30ms, tok/sec: 97574.57, flops:42.26, batch-reuse:1
@ 134 train 7.3029 , allloss: 7.3029, norm:0.4249, dt: 1345.28ms, tok/sec: 97431.11, flops:42.20, batch-reuse:1
@ 135 train 7.1866 , allloss: 7.1866, norm:1.0000, dt: 1342.94ms, tok/sec: 97600.76, flops:42.27, batch-reuse:1
@ 136 train 7.2295 , allloss: 7.2295, norm:0.4773, dt: 1343.29ms, tok/sec: 97575.11, flops:42.26, batch-reuse:1
@ 137 train 7.1389 , allloss: 7.1389, norm:0.5486, dt: 1344.04ms, tok/sec: 97520.88, flops:42.24, batch-reuse:1
@ 138 train 7.1290 , allloss: 7.1290, norm:0.4261, dt: 1347.57ms, tok/sec: 97265.16, flops:42.13, batch-reuse:1
@ 139 train 7.1085 , allloss: 7.1085, norm:0.6288, dt: 1344.88ms, tok/sec: 97459.68, flops:42.21, batch-reuse:1
@ 140 train 7.1204 , allloss: 7.1204, norm:0.4719, dt: 1344.69ms, tok/sec: 97473.92, flops:42.22, batch-reuse:1
@ 141 train 7.1242 , allloss: 7.1242, norm:0.4672, dt: 1344.20ms, tok/sec: 97509.38, flops:42.23, batch-reuse:1
@ 142 train 7.2424 , allloss: 7.2424, norm:0.5280, dt: 1345.45ms, tok/sec: 97418.71, flops:42.19, batch-reuse:1
@ 143 train 7.1821 , allloss: 7.1821, norm:0.9162, dt: 1344.21ms, tok/sec: 97508.67, flops:42.23, batch-reuse:1
@ 144 train 7.1760 , allloss: 7.1760, norm:0.7761, dt: 1348.94ms, tok/sec: 97166.81, flops:42.09, batch-reuse:1
@ 145 train 7.1776 , allloss: 7.1776, norm:0.5668, dt: 1345.94ms, tok/sec: 97383.30, flops:42.18, batch-reuse:1
@ 146 train 7.3244 , allloss: 7.3244, norm:0.5848, dt: 1345.08ms, tok/sec: 97445.75, flops:42.21, batch-reuse:1
@ 147 train 7.1563 , allloss: 7.1563, norm:0.4226, dt: 1346.21ms, tok/sec: 97363.55, flops:42.17, batch-reuse:1
@ 148 train 7.1543 , allloss: 7.1543, norm:0.5386, dt: 1346.33ms, tok/sec: 97355.31, flops:42.17, batch-reuse:1
@ 149 train 7.0256 , allloss: 7.0256, norm:0.4402, dt: 1345.61ms, tok/sec: 97407.46, flops:42.19, batch-reuse:1
@ 150 train 7.2723 , allloss: 7.2723, norm:0.7332, dt: 1345.38ms, tok/sec: 97423.98, flops:42.20, batch-reuse:1
@ 151 train 7.0606 , allloss: 7.0606, norm:0.5704, dt: 1346.84ms, tok/sec: 97318.02, flops:42.15, batch-reuse:1
@ 152 train 7.0470 , allloss: 7.0470, norm:0.7782, dt: 1348.04ms, tok/sec: 97231.18, flops:42.11, batch-reuse:1
@ 153 train 7.0547 , allloss: 7.0547, norm:0.5287, dt: 1348.14ms, tok/sec: 97224.55, flops:42.11, batch-reuse:1
@ 154 train 7.1244 , allloss: 7.1244, norm:0.9297, dt: 1348.07ms, tok/sec: 97229.47, flops:42.11, batch-reuse:1
@ 155 train 7.2703 , allloss: 7.2703, norm:0.5262, dt: 1346.08ms, tok/sec: 97373.13, flops:42.18, batch-reuse:1
@ 156 train 7.1140 , allloss: 7.1140, norm:0.5336, dt: 1350.86ms, tok/sec: 97028.81, flops:42.03, batch-reuse:1
@ 157 train 7.2225 , allloss: 7.2225, norm:0.6183, dt: 1349.38ms, tok/sec: 97134.86, flops:42.07, batch-reuse:1
@ 158 train 7.1961 , allloss: 7.1961, norm:1.2836, dt: 1347.37ms, tok/sec: 97279.75, flops:42.13, batch-reuse:1
@ 159 train 7.2200 , allloss: 7.2200, norm:0.5816, dt: 1347.08ms, tok/sec: 97301.17, flops:42.14, batch-reuse:1
@ 160 train 7.1197 , allloss: 7.1197, norm:0.6284, dt: 1349.04ms, tok/sec: 97159.72, flops:42.08, batch-reuse:1
@ 161 train 7.1266 , allloss: 7.1266, norm:0.6839, dt: 1349.44ms, tok/sec: 97130.83, flops:42.07, batch-reuse:1
@ 162 train 7.2332 , allloss: 7.2332, norm:0.6739, dt: 1349.65ms, tok/sec: 97115.90, flops:42.06, batch-reuse:1
@ 163 train 7.1549 , allloss: 7.1549, norm:0.7381, dt: 1349.33ms, tok/sec: 97138.48, flops:42.07, batch-reuse:1
@ 164 train 7.0512 , allloss: 7.0512, norm:0.4973, dt: 1351.15ms, tok/sec: 97008.01, flops:42.02, batch-reuse:1
@ 165 train 7.1807 , allloss: 7.1807, norm:0.7235, dt: 1353.34ms, tok/sec: 96850.44, flops:41.95, batch-reuse:1
@ 166 train 7.0785 , allloss: 7.0785, norm:0.7726, dt: 1354.02ms, tok/sec: 96802.17, flops:41.93, batch-reuse:1
@ 167 train 7.1313 , allloss: 7.1313, norm:1.0478, dt: 1351.38ms, tok/sec: 96990.93, flops:42.01, batch-reuse:1
@ 168 train 7.0545 , allloss: 7.0545, norm:0.5905, dt: 1351.60ms, tok/sec: 96975.13, flops:42.00, batch-reuse:1
@ 169 train 7.1138 , allloss: 7.1138, norm:0.6805, dt: 1352.72ms, tok/sec: 96895.14, flops:41.97, batch-reuse:1
@ 170 train 7.0979 , allloss: 7.0979, norm:0.5377, dt: 1352.48ms, tok/sec: 96912.63, flops:41.98, batch-reuse:1
@ 171 train 7.0585 , allloss: 7.0585, norm:0.4515, dt: 1353.15ms, tok/sec: 96864.38, flops:41.95, batch-reuse:1
@ 172 train 7.1080 , allloss: 7.1080, norm:0.4579, dt: 1352.72ms, tok/sec: 96895.02, flops:41.97, batch-reuse:1
@ 173 train 7.0813 , allloss: 7.0813, norm:0.6012, dt: 1352.88ms, tok/sec: 96883.34, flops:41.96, batch-reuse:1
@ 174 train 7.2416 , allloss: 7.2416, norm:0.5276, dt: 1353.32ms, tok/sec: 96852.38, flops:41.95, batch-reuse:1
@ 175 train 7.1089 , allloss: 7.1089, norm:1.4889, dt: 1354.54ms, tok/sec: 96764.69, flops:41.91, batch-reuse:1
@ 176 train 7.1505 , allloss: 7.1505, norm:0.6678, dt: 1354.73ms, tok/sec: 96751.27, flops:41.91, batch-reuse:1
@ 177 train 7.1021 , allloss: 7.1021, norm:0.5992, dt: 1353.99ms, tok/sec: 96804.58, flops:41.93, batch-reuse:1
@ 178 train 7.1711 , allloss: 7.1711, norm:0.7458, dt: 1354.03ms, tok/sec: 96801.60, flops:41.93, batch-reuse:1
@ 179 train 7.0294 , allloss: 7.0294, norm:0.7512, dt: 1353.97ms, tok/sec: 96805.87, flops:41.93, batch-reuse:1
@ 180 train 7.1680 , allloss: 7.1680, norm:0.6287, dt: 1354.10ms, tok/sec: 96796.28, flops:41.93, batch-reuse:1
@ 181 train 7.0850 , allloss: 7.0850, norm:0.6615, dt: 1353.77ms, tok/sec: 96820.18, flops:41.94, batch-reuse:1
@ 182 train 7.1243 , allloss: 7.1243, norm:0.6965, dt: 1352.53ms, tok/sec: 96908.43, flops:41.97, batch-reuse:1
@ 183 train 7.0938 , allloss: 7.0938, norm:0.6565, dt: 1352.65ms, tok/sec: 96899.91, flops:41.97, batch-reuse:1
@ 184 train 6.9652 , allloss: 6.9652, norm:0.6508, dt: 1353.86ms, tok/sec: 96813.85, flops:41.93, batch-reuse:1
@ 185 train 7.0210 , allloss: 7.0210, norm:0.7150, dt: 1352.84ms, tok/sec: 96886.21, flops:41.96, batch-reuse:1
@ 186 train 7.1942 , allloss: 7.1942, norm:0.9266, dt: 1353.44ms, tok/sec: 96843.61, flops:41.95, batch-reuse:1
@ 187 train 7.2870 , allloss: 7.2870, norm:0.9557, dt: 1353.12ms, tok/sec: 96866.19, flops:41.96, batch-reuse:1
@ 188 train 7.0977 , allloss: 7.0977, norm:0.6191, dt: 1354.56ms, tok/sec: 96763.51, flops:41.91, batch-reuse:1
@ 189 train 7.2190 , allloss: 7.2190, norm:0.8945, dt: 1354.87ms, tok/sec: 96741.07, flops:41.90, batch-reuse:1
@ 190 train 7.1765 , allloss: 7.1765, norm:0.4632, dt: 1356.20ms, tok/sec: 96646.16, flops:41.86, batch-reuse:1
@ 191 train 7.1604 , allloss: 7.1604, norm:0.6213, dt: 1355.28ms, tok/sec: 96712.11, flops:41.89, batch-reuse:1
@ 192 train 7.2051 , allloss: 7.2051, norm:0.7088, dt: 1354.51ms, tok/sec: 96766.87, flops:41.91, batch-reuse:1
@ 193 train 7.1016 , allloss: 7.1016, norm:0.9210, dt: 1355.74ms, tok/sec: 96679.18, flops:41.87, batch-reuse:1
@ 194 train 7.2137 , allloss: 7.2137, norm:0.7465, dt: 1354.61ms, tok/sec: 96760.02, flops:41.91, batch-reuse:1
@ 195 train 7.2029 , allloss: 7.2029, norm:0.8224, dt: 1354.96ms, tok/sec: 96735.13, flops:41.90, batch-reuse:1
@ 196 train 7.0966 , allloss: 7.0966, norm:0.6037, dt: 1355.50ms, tok/sec: 96696.49, flops:41.88, batch-reuse:1
@ 197 train 7.1077 , allloss: 7.1077, norm:0.6276, dt: 1357.96ms, tok/sec: 96521.20, flops:41.81, batch-reuse:1
@ 198 train 7.0775 , allloss: 7.0775, norm:0.6701, dt: 1355.80ms, tok/sec: 96675.12, flops:41.87, batch-reuse:1
@ 199 train 7.1737 , allloss: 7.1737, norm:0.8501, dt: 1354.83ms, tok/sec: 96744.58, flops:41.90, batch-reuse:1
@ 200 train 7.1810 , allloss: 7.1810, norm:0.8523, dt: 1355.47ms, tok/sec: 96698.87, flops:41.88, batch-reuse:1
@ 201 train 7.1672 , allloss: 7.1672, norm:0.7356, dt: 1355.17ms, tok/sec: 96719.64, flops:41.89, batch-reuse:1
@ 202 train 7.2147 , allloss: 7.2147, norm:0.8111, dt: 1354.41ms, tok/sec: 96773.89, flops:41.92, batch-reuse:1
@ 203 train 7.1794 , allloss: 7.1794, norm:0.7620, dt: 1355.74ms, tok/sec: 96679.27, flops:41.87, batch-reuse:1
@ 204 train 7.1648 , allloss: 7.1648, norm:0.6423, dt: 1356.60ms, tok/sec: 96618.25, flops:41.85, batch-reuse:1
@ 205 train 7.1350 , allloss: 7.1350, norm:0.5793, dt: 1356.10ms, tok/sec: 96653.94, flops:41.86, batch-reuse:1
@ 206 train 7.1718 , allloss: 7.1718, norm:0.7035, dt: 1356.98ms, tok/sec: 96591.16, flops:41.84, batch-reuse:1
@ 207 train 7.0666 , allloss: 7.0666, norm:1.0807, dt: 1356.77ms, tok/sec: 96605.91, flops:41.84, batch-reuse:1
@ 208 train 7.1618 , allloss: 7.1618, norm:0.7807, dt: 1357.94ms, tok/sec: 96522.52, flops:41.81, batch-reuse:1
@ 209 train 7.1867 , allloss: 7.1867, norm:0.6541, dt: 1357.75ms, tok/sec: 96536.46, flops:41.81, batch-reuse:1
@ 210 train 7.1719 , allloss: 7.1719, norm:0.8740, dt: 1356.20ms, tok/sec: 96646.22, flops:41.86, batch-reuse:1
@ 211 train 7.0826 , allloss: 7.0826, norm:0.5678, dt: 1356.19ms, tok/sec: 96647.38, flops:41.86, batch-reuse:1
@ 212 train 7.1875 , allloss: 7.1875, norm:0.9450, dt: 1355.83ms, tok/sec: 96672.96, flops:41.87, batch-reuse:1
@ 213 train 7.0863 , allloss: 7.0863, norm:0.7123, dt: 1355.96ms, tok/sec: 96663.34, flops:41.87, batch-reuse:1
@ 214 train 7.1310 , allloss: 7.1310, norm:0.6454, dt: 1357.68ms, tok/sec: 96541.20, flops:41.81, batch-reuse:1
@ 215 train 7.0405 , allloss: 7.0405, norm:0.5492, dt: 1356.73ms, tok/sec: 96609.06, flops:41.84, batch-reuse:1
@ 216 train 7.0684 , allloss: 7.0684, norm:0.6740, dt: 1356.59ms, tok/sec: 96618.78, flops:41.85, batch-reuse:1
@ 217 train 7.1476 , allloss: 7.1476, norm:0.7873, dt: 1356.84ms, tok/sec: 96601.00, flops:41.84, batch-reuse:1
@ 218 train 7.1240 , allloss: 7.1240, norm:1.6394, dt: 1357.12ms, tok/sec: 96580.86, flops:41.83, batch-reuse:1
@ 219 train 7.2448 , allloss: 7.2448, norm:0.6182, dt: 1356.32ms, tok/sec: 96638.24, flops:41.86, batch-reuse:1
@ 220 train 7.1595 , allloss: 7.1595, norm:0.5503, dt: 1356.64ms, tok/sec: 96615.13, flops:41.85, batch-reuse:1
@ 221 train 7.1657 , allloss: 7.1657, norm:0.7888, dt: 1354.42ms, tok/sec: 96773.36, flops:41.92, batch-reuse:1
@ 222 train 7.1225 , allloss: 7.1225, norm:1.0875, dt: 1355.78ms, tok/sec: 96676.51, flops:41.87, batch-reuse:1
@ 223 train 7.1585 , allloss: 7.1585, norm:0.7746, dt: 1354.73ms, tok/sec: 96751.03, flops:41.91, batch-reuse:1
@ 224 train 7.0847 , allloss: 7.0847, norm:0.7697, dt: 1354.72ms, tok/sec: 96752.04, flops:41.91, batch-reuse:1
@ 225 train 7.1235 , allloss: 7.1235, norm:0.8211, dt: 1355.33ms, tok/sec: 96708.52, flops:41.89, batch-reuse:1
@ 226 train 7.1724 , allloss: 7.1724, norm:1.1340, dt: 1354.29ms, tok/sec: 96782.59, flops:41.92, batch-reuse:1
@ 227 train 7.1674 , allloss: 7.1674, norm:0.9654, dt: 1354.40ms, tok/sec: 96774.98, flops:41.92, batch-reuse:1
@ 228 train 7.1298 , allloss: 7.1298, norm:0.5928, dt: 1355.36ms, tok/sec: 96706.54, flops:41.89, batch-reuse:1
@ 229 train 7.1817 , allloss: 7.1817, norm:0.6996, dt: 1355.05ms, tok/sec: 96728.54, flops:41.90, batch-reuse:1
@ 230 train 7.1566 , allloss: 7.1566, norm:0.7763, dt: 1354.12ms, tok/sec: 96795.24, flops:41.92, batch-reuse:1
@ 231 train 7.0947 , allloss: 7.0947, norm:0.7454, dt: 1353.05ms, tok/sec: 96871.73, flops:41.96, batch-reuse:1
@ 232 train 7.0832 , allloss: 7.0832, norm:0.9362, dt: 1355.31ms, tok/sec: 96709.83, flops:41.89, batch-reuse:1
@ 233 train 7.1685 , allloss: 7.1685, norm:1.1866, dt: 1353.19ms, tok/sec: 96861.20, flops:41.95, batch-reuse:1
@ 234 train 7.0117 , allloss: 7.0117, norm:1.2871, dt: 1354.35ms, tok/sec: 96778.50, flops:41.92, batch-reuse:1
@ 235 train 7.1348 , allloss: 7.1348, norm:1.0770, dt: 1354.11ms, tok/sec: 96796.00, flops:41.93, batch-reuse:1
@ 236 train 7.0224 , allloss: 7.0224, norm:0.7475, dt: 1354.96ms, tok/sec: 96735.27, flops:41.90, batch-reuse:1
@ 237 train 7.1382 , allloss: 7.1382, norm:0.9874, dt: 1354.81ms, tok/sec: 96745.41, flops:41.90, batch-reuse:1
@ 238 train 7.0896 , allloss: 7.0896, norm:0.7302, dt: 1354.92ms, tok/sec: 96738.04, flops:41.90, batch-reuse:1
@ 239 train 7.1037 , allloss: 7.1037, norm:0.8650, dt: 1355.36ms, tok/sec: 96706.25, flops:41.89, batch-reuse:1
@ 240 train 7.1518 , allloss: 7.1518, norm:0.6938, dt: 1353.83ms, tok/sec: 96815.86, flops:41.93, batch-reuse:1
@ 241 train 7.1253 , allloss: 7.1253, norm:1.1839, dt: 1354.03ms, tok/sec: 96801.42, flops:41.93, batch-reuse:1
@ 242 train 7.1306 , allloss: 7.1306, norm:0.7252, dt: 1354.42ms, tok/sec: 96773.46, flops:41.92, batch-reuse:1
@ 243 train 6.9899 , allloss: 6.9899, norm:0.8182, dt: 1354.11ms, tok/sec: 96795.61, flops:41.92, batch-reuse:1
@ 244 train 7.1691 , allloss: 7.1691, norm:0.6833, dt: 1353.11ms, tok/sec: 96866.87, flops:41.96, batch-reuse:1
@ 245 train 7.0161 , allloss: 7.0161, norm:0.7669, dt: 1353.78ms, tok/sec: 96819.60, flops:41.94, batch-reuse:1
@ 246 train 7.2159 , allloss: 7.2159, norm:0.7620, dt: 1355.14ms, tok/sec: 96722.33, flops:41.89, batch-reuse:1
@ 247 train 7.1029 , allloss: 7.1029, norm:0.8375, dt: 1354.67ms, tok/sec: 96755.48, flops:41.91, batch-reuse:1
@ 248 train 7.0649 , allloss: 7.0649, norm:0.5529, dt: 1354.19ms, tok/sec: 96790.18, flops:41.92, batch-reuse:1
@ 249 train 7.0359 , allloss: 7.0359, norm:0.7253, dt: 1354.56ms, tok/sec: 96763.67, flops:41.91, batch-reuse:1
@ 250 train 7.0881 , allloss: 7.0881, norm:0.5842, dt: 1355.66ms, tok/sec: 96684.72, flops:41.88, batch-reuse:1
@ 251 train 7.0165 , allloss: 7.0165, norm:0.6573, dt: 1355.93ms, tok/sec: 96665.65, flops:41.87, batch-reuse:1
@ 252 train 7.1675 , allloss: 7.1675, norm:0.7616, dt: 1354.41ms, tok/sec: 96774.26, flops:41.92, batch-reuse:1
@ 253 train 7.0357 , allloss: 7.0357, norm:0.6430, dt: 1353.73ms, tok/sec: 96823.04, flops:41.94, batch-reuse:1
@ 254 train 6.9881 , allloss: 6.9881, norm:0.6282, dt: 1353.08ms, tok/sec: 96869.58, flops:41.96, batch-reuse:1
@ 255 train 7.0003 , allloss: 7.0003, norm:0.5710, dt: 1352.76ms, tok/sec: 96892.24, flops:41.97, batch-reuse:1
@ 256 train 7.0361 , allloss: 7.0361, norm:0.9635, dt: 1353.41ms, tok/sec: 96845.86, flops:41.95, batch-reuse:1
@ 257 train 7.0809 , allloss: 7.0809, norm:0.5940, dt: 1353.01ms, tok/sec: 96874.67, flops:41.96, batch-reuse:1
@ 258 train 7.0256 , allloss: 7.0256, norm:1.0311, dt: 1355.36ms, tok/sec: 96706.29, flops:41.89, batch-reuse:1
@ 259 train 7.0349 , allloss: 7.0349, norm:0.6481, dt: 1353.98ms, tok/sec: 96804.71, flops:41.93, batch-reuse:1
@ 260 train 7.1277 , allloss: 7.1277, norm:0.6817, dt: 1352.31ms, tok/sec: 96924.80, flops:41.98, batch-reuse:1
@ 261 train 7.0759 , allloss: 7.0759, norm:0.7579, dt: 1353.15ms, tok/sec: 96864.12, flops:41.95, batch-reuse:1
@ 262 train 7.1251 , allloss: 7.1251, norm:0.6264, dt: 1352.88ms, tok/sec: 96883.43, flops:41.96, batch-reuse:1
@ 263 train 7.0737 , allloss: 7.0737, norm:0.6499, dt: 1352.90ms, tok/sec: 96882.29, flops:41.96, batch-reuse:1
@ 264 train 7.0509 , allloss: 7.0509, norm:0.7370, dt: 1355.13ms, tok/sec: 96723.12, flops:41.89, batch-reuse:1
@ 265 train 7.0391 , allloss: 7.0391, norm:0.6916, dt: 1354.49ms, tok/sec: 96768.81, flops:41.91, batch-reuse:1
@ 266 train 7.0298 , allloss: 7.0298, norm:0.6283, dt: 1352.95ms, tok/sec: 96879.01, flops:41.96, batch-reuse:1
@ 267 train 7.0088 , allloss: 7.0088, norm:0.6045, dt: 1351.70ms, tok/sec: 96968.15, flops:42.00, batch-reuse:1
@ 268 train 7.0862 , allloss: 7.0862, norm:0.7594, dt: 1352.42ms, tok/sec: 96916.92, flops:41.98, batch-reuse:1
@ 269 train 7.0401 , allloss: 7.0401, norm:0.5645, dt: 1352.28ms, tok/sec: 96927.00, flops:41.98, batch-reuse:1
@ 270 train 7.1174 , allloss: 7.1174, norm:0.8679, dt: 1353.27ms, tok/sec: 96855.74, flops:41.95, batch-reuse:1
@ 271 train 7.0059 , allloss: 7.0059, norm:0.8752, dt: 1351.14ms, tok/sec: 97008.71, flops:42.02, batch-reuse:1
@ 272 train 7.0277 , allloss: 7.0277, norm:0.7531, dt: 1351.00ms, tok/sec: 97018.57, flops:42.02, batch-reuse:1
@ 273 train 7.1628 , allloss: 7.1628, norm:1.0626, dt: 1352.12ms, tok/sec: 96938.16, flops:41.99, batch-reuse:1
@ 274 train 7.0442 , allloss: 7.0442, norm:1.0763, dt: 1352.60ms, tok/sec: 96903.65, flops:41.97, batch-reuse:1
@ 275 train 7.0242 , allloss: 7.0242, norm:0.8402, dt: 1352.86ms, tok/sec: 96885.39, flops:41.96, batch-reuse:1
@ 276 train 7.0227 , allloss: 7.0227, norm:0.8196, dt: 1352.59ms, tok/sec: 96904.81, flops:41.97, batch-reuse:1
@ 277 train 6.9190 , allloss: 6.9190, norm:0.6232, dt: 1352.83ms, tok/sec: 96887.29, flops:41.96, batch-reuse:1
@ 278 train 7.0285 , allloss: 7.0285, norm:0.6927, dt: 1355.01ms, tok/sec: 96731.63, flops:41.90, batch-reuse:1
@ 279 train 6.9586 , allloss: 6.9586, norm:1.1436, dt: 1354.04ms, tok/sec: 96800.54, flops:41.93, batch-reuse:1
@ 280 train 7.0943 , allloss: 7.0943, norm:0.7789, dt: 1353.09ms, tok/sec: 96868.76, flops:41.96, batch-reuse:1
@ 281 train 6.9673 , allloss: 6.9673, norm:0.7799, dt: 1351.80ms, tok/sec: 96960.80, flops:42.00, batch-reuse:1
@ 282 train 6.9421 , allloss: 6.9421, norm:0.8829, dt: 1352.55ms, tok/sec: 96907.59, flops:41.97, batch-reuse:1
@ 283 train 6.9165 , allloss: 6.9165, norm:0.7816, dt: 1350.95ms, tok/sec: 97022.25, flops:42.02, batch-reuse:1
@ 284 train 7.0140 , allloss: 7.0140, norm:0.6781, dt: 1350.03ms, tok/sec: 97088.46, flops:42.05, batch-reuse:1
@ 285 train 7.0106 , allloss: 7.0106, norm:0.8531, dt: 1349.87ms, tok/sec: 97099.77, flops:42.06, batch-reuse:1
@ 286 train 7.0239 , allloss: 7.0239, norm:0.8583, dt: 1352.16ms, tok/sec: 96934.95, flops:41.99, batch-reuse:1
@ 287 train 7.0172 , allloss: 7.0172, norm:0.8259, dt: 1351.22ms, tok/sec: 97002.99, flops:42.01, batch-reuse:1
@ 288 train 7.0235 , allloss: 7.0235, norm:0.7164, dt: 1351.19ms, tok/sec: 97005.11, flops:42.02, batch-reuse:1
@ 289 train 7.0020 , allloss: 7.0020, norm:0.8834, dt: 1350.56ms, tok/sec: 97050.25, flops:42.04, batch-reuse:1
@ 290 train 6.9892 , allloss: 6.9892, norm:0.5875, dt: 1350.41ms, tok/sec: 97060.53, flops:42.04, batch-reuse:1
@ 291 train 6.8993 , allloss: 6.8993, norm:0.6125, dt: 1350.36ms, tok/sec: 97064.48, flops:42.04, batch-reuse:1
@ 292 train 6.9347 , allloss: 6.9347, norm:0.7177, dt: 1352.22ms, tok/sec: 96930.70, flops:41.98, batch-reuse:1
@ 293 train 7.0224 , allloss: 7.0224, norm:1.0012, dt: 1350.74ms, tok/sec: 97037.46, flops:42.03, batch-reuse:1
@ 294 train 6.9796 , allloss: 6.9796, norm:0.7370, dt: 1350.47ms, tok/sec: 97056.73, flops:42.04, batch-reuse:1
@ 295 train 7.0182 , allloss: 7.0182, norm:0.8948, dt: 1349.80ms, tok/sec: 97104.58, flops:42.06, batch-reuse:1
@ 296 train 6.8968 , allloss: 6.8968, norm:0.6435, dt: 1350.34ms, tok/sec: 97065.88, flops:42.04, batch-reuse:1
@ 297 train 6.8852 , allloss: 6.8852, norm:0.9410, dt: 1351.48ms, tok/sec: 96983.88, flops:42.01, batch-reuse:1
@ 298 train 6.9784 , allloss: 6.9784, norm:0.7725, dt: 1350.66ms, tok/sec: 97043.04, flops:42.03, batch-reuse:1
@ 299 train 6.9601 , allloss: 6.9601, norm:1.0495, dt: 1349.12ms, tok/sec: 97154.03, flops:42.08, batch-reuse:1
@ 300 train 6.9386 , allloss: 6.9386, norm:3.4712, dt: 1348.65ms, tok/sec: 97187.70, flops:42.09, batch-reuse:1
@ 301 train 7.1198 , allloss: 7.1198, norm:1.2715, dt: 1349.47ms, tok/sec: 97128.54, flops:42.07, batch-reuse:1
@ 302 train 6.9396 , allloss: 6.9396, norm:0.9263, dt: 1348.92ms, tok/sec: 97168.46, flops:42.09, batch-reuse:1
@ 303 train 6.9745 , allloss: 6.9745, norm:0.7781, dt: 1350.09ms, tok/sec: 97084.03, flops:42.05, batch-reuse:1
@ 304 train 6.9216 , allloss: 6.9216, norm:1.4302, dt: 1349.06ms, tok/sec: 97157.71, flops:42.08, batch-reuse:1
@ 305 train 6.8774 , allloss: 6.8774, norm:0.8382, dt: 1348.73ms, tok/sec: 97181.56, flops:42.09, batch-reuse:1
@ 306 train 6.8798 , allloss: 6.8798, norm:0.9206, dt: 1349.03ms, tok/sec: 97160.34, flops:42.08, batch-reuse:1
@ 307 train 7.0298 , allloss: 7.0298, norm:1.4542, dt: 1349.57ms, tok/sec: 97121.08, flops:42.07, batch-reuse:1
@ 308 train 6.9306 , allloss: 6.9306, norm:0.9681, dt: 1349.53ms, tok/sec: 97124.17, flops:42.07, batch-reuse:1
@ 309 train 6.9580 , allloss: 6.9580, norm:1.0254, dt: 1351.34ms, tok/sec: 96994.26, flops:42.01, batch-reuse:1
@ 310 train 7.0181 , allloss: 7.0181, norm:0.9693, dt: 1349.63ms, tok/sec: 97117.27, flops:42.06, batch-reuse:1
@ 311 train 7.0290 , allloss: 7.0290, norm:1.5460, dt: 1349.06ms, tok/sec: 97158.14, flops:42.08, batch-reuse:1
@ 312 train 7.1002 , allloss: 7.1002, norm:2.5107, dt: 1349.34ms, tok/sec: 97137.98, flops:42.07, batch-reuse:1
@ 313 train 6.9080 , allloss: 6.9080, norm:1.7763, dt: 1350.95ms, tok/sec: 97021.91, flops:42.02, batch-reuse:1
@ 314 train 6.9241 , allloss: 6.9241, norm:1.5019, dt: 1350.51ms, tok/sec: 97053.39, flops:42.04, batch-reuse:1
@ 315 train 6.9463 , allloss: 6.9463, norm:0.9536, dt: 1349.79ms, tok/sec: 97105.57, flops:42.06, batch-reuse:1
@ 316 train 6.9602 , allloss: 6.9602, norm:0.9843, dt: 1349.47ms, tok/sec: 97128.29, flops:42.07, batch-reuse:1
@ 317 train 7.0118 , allloss: 7.0118, norm:0.9527, dt: 1349.52ms, tok/sec: 97124.70, flops:42.07, batch-reuse:1
@ 318 train 6.8870 , allloss: 6.8870, norm:1.1510, dt: 1350.17ms, tok/sec: 97078.48, flops:42.05, batch-reuse:1
@ 319 train 6.8495 , allloss: 6.8495, norm:0.8231, dt: 1350.08ms, tok/sec: 97084.31, flops:42.05, batch-reuse:1
@ 320 train 6.9336 , allloss: 6.9336, norm:0.7869, dt: 1350.22ms, tok/sec: 97074.50, flops:42.05, batch-reuse:1
@ 321 train 6.9657 , allloss: 6.9657, norm:1.4205, dt: 1350.36ms, tok/sec: 97064.41, flops:42.04, batch-reuse:1
@ 322 train 6.8829 , allloss: 6.8829, norm:0.9426, dt: 1349.65ms, tok/sec: 97115.56, flops:42.06, batch-reuse:1
@ 323 train 6.9655 , allloss: 6.9655, norm:0.8664, dt: 1349.81ms, tok/sec: 97103.81, flops:42.06, batch-reuse:1
@ 324 train 6.8896 , allloss: 6.8896, norm:1.0036, dt: 1349.58ms, tok/sec: 97120.27, flops:42.07, batch-reuse:1
@ 325 train 6.9182 , allloss: 6.9182, norm:0.6448, dt: 1349.14ms, tok/sec: 97151.96, flops:42.08, batch-reuse:1
@ 326 train 6.9682 , allloss: 6.9682, norm:0.7303, dt: 1349.43ms, tok/sec: 97131.41, flops:42.07, batch-reuse:1
@ 327 train 7.0188 , allloss: 7.0188, norm:1.0635, dt: 1349.37ms, tok/sec: 97135.49, flops:42.07, batch-reuse:1
@ 328 train 6.9322 , allloss: 6.9322, norm:1.9157, dt: 1349.27ms, tok/sec: 97142.86, flops:42.08, batch-reuse:1
@ 329 train 6.9295 , allloss: 6.9295, norm:1.5501, dt: 1348.82ms, tok/sec: 97175.00, flops:42.09, batch-reuse:1
@ 330 train 6.9113 , allloss: 6.9113, norm:0.8606, dt: 1348.91ms, tok/sec: 97169.06, flops:42.09, batch-reuse:1
@ 331 train 6.8807 , allloss: 6.8807, norm:2.0897, dt: 1348.59ms, tok/sec: 97191.60, flops:42.10, batch-reuse:1
@ 332 train 6.8797 , allloss: 6.8797, norm:0.7781, dt: 1348.45ms, tok/sec: 97201.67, flops:42.10, batch-reuse:1
@ 333 train 6.9384 , allloss: 6.9384, norm:1.1186, dt: 1349.46ms, tok/sec: 97128.94, flops:42.07, batch-reuse:1
@ 334 train 6.8728 , allloss: 6.8728, norm:0.9565, dt: 1349.68ms, tok/sec: 97113.46, flops:42.06, batch-reuse:1
@ 335 train 6.8591 , allloss: 6.8591, norm:0.9350, dt: 1350.13ms, tok/sec: 97080.71, flops:42.05, batch-reuse:1
@ 336 train 6.9062 , allloss: 6.9062, norm:1.0292, dt: 1348.46ms, tok/sec: 97201.44, flops:42.10, batch-reuse:1
@ 337 train 6.9047 , allloss: 6.9047, norm:1.3771, dt: 1349.21ms, tok/sec: 97147.36, flops:42.08, batch-reuse:1
@ 338 train 6.8661 , allloss: 6.8661, norm:1.3972, dt: 1349.27ms, tok/sec: 97143.03, flops:42.08, batch-reuse:1
@ 339 train 6.9512 , allloss: 6.9512, norm:1.2993, dt: 1349.79ms, tok/sec: 97105.57, flops:42.06, batch-reuse:1
@ 340 train 6.9447 , allloss: 6.9447, norm:1.1185, dt: 1349.34ms, tok/sec: 97137.74, flops:42.07, batch-reuse:1
@ 341 train 6.8722 , allloss: 6.8722, norm:0.8675, dt: 1349.30ms, tok/sec: 97140.57, flops:42.07, batch-reuse:1
@ 342 train 6.8021 , allloss: 6.8021, norm:1.2588, dt: 1352.75ms, tok/sec: 96892.68, flops:41.97, batch-reuse:1
@ 343 train 6.8643 , allloss: 6.8643, norm:1.2596, dt: 1351.75ms, tok/sec: 96964.34, flops:42.00, batch-reuse:1
@ 344 train 6.8442 , allloss: 6.8442, norm:1.0041, dt: 1349.81ms, tok/sec: 97104.25, flops:42.06, batch-reuse:1
@ 345 train 6.8403 , allloss: 6.8403, norm:0.8837, dt: 1346.93ms, tok/sec: 97311.89, flops:42.15, batch-reuse:1
@ 346 train 6.9025 , allloss: 6.9025, norm:1.1475, dt: 1347.66ms, tok/sec: 97259.09, flops:42.13, batch-reuse:1
@ 347 train 6.9203 , allloss: 6.9203, norm:0.8678, dt: 1346.45ms, tok/sec: 97346.49, flops:42.16, batch-reuse:1
@ 348 train 6.8589 , allloss: 6.8589, norm:0.7632, dt: 1347.09ms, tok/sec: 97299.95, flops:42.14, batch-reuse:1
@ 349 train 6.9038 , allloss: 6.9038, norm:0.8323, dt: 1347.23ms, tok/sec: 97289.86, flops:42.14, batch-reuse:1
@ 350 train 6.9191 , allloss: 6.9191, norm:1.0716, dt: 1347.71ms, tok/sec: 97255.46, flops:42.12, batch-reuse:1
@ 351 train 6.9219 , allloss: 6.9219, norm:0.9069, dt: 1348.21ms, tok/sec: 97219.13, flops:42.11, batch-reuse:1
@ 352 train 6.9632 , allloss: 6.9632, norm:0.8864, dt: 1347.17ms, tok/sec: 97294.11, flops:42.14, batch-reuse:1
@ 353 train 6.9159 , allloss: 6.9159, norm:1.4441, dt: 1346.89ms, tok/sec: 97314.38, flops:42.15, batch-reuse:1
@ 354 train 6.8613 , allloss: 6.8613, norm:1.0018, dt: 1347.66ms, tok/sec: 97259.10, flops:42.13, batch-reuse:1
