Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 19-funexperiment-debug
MLPSCALE: 4
Experiment description: 
```
Transformer, max LR 0.0006 n_layer 12
Setting:
==details======
 machine_modules
        self.compiler = BenCompilerNoOp(config)
        self.execute = VanillaExecute(config)
----------------
 block_logic
        y = self.ln_1(x)
        attn, newKvCache = self.attn(y, y, print_weights=print_weights, kvCache=kvCache)
        program = self.compiler(x)
        machineOutput = self.execute(program, attn)
        newx = x + machineOutput
----------------
 attn_weights [TIE_ATTN_WEIGHTS]
                if TIE_ATTN_WEIGHTS:
                    # Tie model weights together
                    firstBlock = self.transformer.h[0]
                    for block in self.transformer.h:
                        block.attn.c_attn.weight = firstBlock.attn.c_attn.weight
                        # block.attn = firstBlock.attn
----------------
========
VALUEMATRIX=True
REUSE_WEIGHTS=False
MLP_SCALE=4
ATTENTION_SINK=False
TIE_ATTN_WEIGHTS=True
LOW_RANK_ATTN=True
```
![caption](img/19-funexperiment-debug.jpg)

Warmup steps: 100
total desired batch size: 163840
Mini-batch size: 1280*128
=> calculated gradient accumulation steps: 1
=> calculated gradient accumulation steps: 1
Training max steps: 300001Num GPUs: 1{'block_size': 128, 'vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768}
num decayed parameter tensors: 51, with 110,690,304 parameters
num non-decayed parameter tensors: 74, with 84,480 parameters
