Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 19-funexperiment
MLPSCALE: 4
Experiment description: 
```
Transformer, max LR 0.0006 n_layer 12
Setting:
==details======
 machine_modules
        self.compiler = BenCompilerNoOp(config)
        self.execute = VanillaExecute(config)
----------------
 block_logic
        y = self.ln_1(x)
        attn, newKvCache = self.attn(y, y, print_weights=print_weights, kvCache=kvCache)
        program = self.compiler(x)
        machineOutput = self.execute(program, attn)
        newx = x + machineOutput
----------------
 attn_weights [TIE_ATTN_WEIGHTS]
                if TIE_ATTN_WEIGHTS:
                    # Tie model weights together
                    firstBlock = self.transformer.h[0]
                    for block in self.transformer.h:
                        block.attn.c_attn.weight = firstBlock.attn.c_attn.weight
                        # block.attn = firstBlock.attn
----------------
========
VALUEMATRIX=True
REUSE_WEIGHTS=False
MLP_SCALE=4
ATTENTION_SINK=False
TIE_ATTN_WEIGHTS=True
LOW_RANK_ATTN=True
```
![caption](img/19-funexperiment.jpg)

Warmup steps: 100
total desired batch size: 4096
Mini-batch size: 64*64
=> calculated gradient accumulation steps: 1
=> calculated gradient accumulation steps: 1
Training max steps: 300001Num GPUs: 1{'block_size': 64, 'vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768}
num decayed parameter tensors: 51, with 110,641,152 parameters
num non-decayed parameter tensors: 74, with 84,480 parameters
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( depart:0.0002968:0.0002 plutonium:0.0001 elabor:0.0001COM:0.0001 institution:0.0001 wedge:0.0001)
 depart
------
		( depart:0.0004 plutonium:0.0002 surging:0.0001COM:0.0001 elabor:0.0001 Subjects:0.0001 machines:0.0001)
 depart depart depart depart depart depart depart depart depart depart depart depart depart depart depart
@ 0 train 11.0537 , allloss: 11.0537, dt: 11122.77ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 13.7710, norm(y): 64.0004, norm:26.3933, tok/sec: 368.25, flops:0.02, batch-reuse:1
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( depart:0.0002968:0.0002 Ves:0.0001 elabor:0.0001';:0.0001COM:0.0001 Info:0.0001)
 depart
------
		( depart:0.0004 plutonium:0.0001 Obama:0.0001 Sugar:0.0001iPhone:0.0001COM:0.0001 machines:0.0001)
 depart depart depart depart depart depart depart depart depart depart depart depart depart depart depart
@ 1 train 10.9531 , allloss: 10.9531, dt: 3914.20ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 13.7479, norm(y): 64.0003, norm:27.2006, tok/sec: 1046.45, flops:0.04, batch-reuse:1
@ 2 train 10.6650 , allloss: 10.6650, dt: 899.34ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 14.1239, norm(y): 64.0006, norm:24.9843, tok/sec: 4554.45, flops:0.20, batch-reuse:1
@ 3 train 10.7061 , allloss: 10.7061, dt: 1006.01ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 15.0392, norm(y): 64.0011, norm:21.7507, tok/sec: 4071.53, flops:0.17, batch-reuse:1
@ 4 train 10.2344 , allloss: 10.2344, dt: 910.34ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 16.5788, norm(y): 64.0017, norm:17.3805, tok/sec: 4499.42, flops:0.19, batch-reuse:1
@ 5 train 10.0747 , allloss: 10.0747, dt: 914.69ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 18.5868, norm(y): 64.0026, norm:14.5075, tok/sec: 4478.04, flops:0.19, batch-reuse:1
@ 6 train 10.0386 , allloss: 10.0386, dt: 926.69ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 20.9188, norm(y): 64.0032, norm:12.3893, tok/sec: 4420.05, flops:0.19, batch-reuse:1
@ 7 train 9.4941 , allloss: 9.4941, dt: 934.88ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 23.4421, norm(y): 64.0039, norm:12.4718, tok/sec: 4381.32, flops:0.19, batch-reuse:1
@ 8 train 9.8271 , allloss: 9.8271, dt: 926.52ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 26.1500, norm(y): 64.0046, norm:10.3442, tok/sec: 4420.82, flops:0.19, batch-reuse:1
@ 9 train 10.2368 , allloss: 10.2368, dt: 932.45ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 28.8145, norm(y): 64.0049, norm:8.1482, tok/sec: 4392.74, flops:0.19, batch-reuse:1
@ 10 train 9.8726 , allloss: 9.8726, dt: 931.27ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 31.8898, norm(y): 64.0052, norm:8.2742, tok/sec: 4398.32, flops:0.19, batch-reuse:1
@ 11 train 9.8921 , allloss: 9.8921, dt: 939.45ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 35.1990, norm(y): 64.0058, norm:6.5071, tok/sec: 4359.99, flops:0.19, batch-reuse:1
@ 12 train 9.8550 , allloss: 9.8550, dt: 956.51ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 38.4426, norm(y): 64.0059, norm:7.1276, tok/sec: 4282.25, flops:0.18, batch-reuse:1
@ 13 train 9.8276 , allloss: 9.8276, dt: 923.12ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 42.1966, norm(y): 64.0063, norm:6.0169, tok/sec: 4437.11, flops:0.19, batch-reuse:1
@ 14 train 9.9517 , allloss: 9.9517, dt: 1076.49ms, norm(attn): 2.0000, norm(output): 4.0000, norm(x): 46.1576, norm(y): 64.0065, norm:5.4625, tok/sec: 3804.94, flops:0.16, batch-reuse:1
@ 15 train 9.1687 , allloss: 9.1687, dt: 916.13ms, norm(attn): 2.0000, norm(output): 6.1875, norm(x): 50.4646, norm(y): 64.0069, norm:5.8530, tok/sec: 4471.00, flops:0.19, batch-reuse:1
@ 16 train 9.6621 , allloss: 9.6621, dt: 933.63ms, norm(attn): 2.0000, norm(output): 8.0000, norm(x): 54.8548, norm(y): 64.0070, norm:5.8694, tok/sec: 4387.20, flops:0.19, batch-reuse:1
@ 17 train 9.4126 , allloss: 9.4126, dt: 923.43ms, norm(attn): 2.0000, norm(output): 8.0000, norm(x): 59.5964, norm(y): 64.0073, norm:5.2629, tok/sec: 4435.63, flops:0.19, batch-reuse:1
