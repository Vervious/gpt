Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 16-principle-1
MLPSCALE: 4
Experiment description: Transformer, max LR 6e-4
Setting:
========
 y = self.ln_1(x)
attn, resx, scores = self.attn(y, y)
hiddenBias, fParams, bParams = self.compiler(y)
machineOutput = self.execute(attn, fParams, bParams, hiddenBias)
x = resx + machineOutput
======== 
VALUEMATRIX=True
REUSE_WEIGHTS=False
MLP_SCALE=4
MEASURE_SELF_CONTRIBUTION=True
NEW_ALL_LAYER_LOSS=False
MATRIX_NUM_PARAMS=4096
MLPMAT_INNER_SIZE=64
DELETE_SELF_CONTRIBUTION=False
EXTRACT_SELF_CONTRIBUTION=True

Warmup steps: 100
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1{'block_size': 1024, 'vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768}
num decayed parameter tensors: 98, with 428,703,744 parameters
num non-decayed parameter tensors: 98, with 219,648 parameters
@ 0 train 10.8155 , allloss: 10.8155, dt: 4176.16ms, perc(<0.5): 0.9771, perc(<5): 0.9980, perc(>5): 0.0020, norm:14.7793, tok/sec: 31385.79, flops:86.20, batch-reuse:1
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Soros:0.0002!]:0.0002mad:0.0002quant:0.0001 Source:0.0002 Rounds:0.0001 GA:0.0001 float:0.0001)
 float
------
		(!]:0.0002mad:0.0002quant:0.0001 Source:0.0002 Rounds:0.0001 GA:0.0001 float:0.0001 select:0.0001)
 select wow shaped Tor Syria quiitem Province persona"â€¦ Farrellests Pom aging enduring
@ 1 train 10.8146 , allloss: 10.8146, dt: 2699.80ms, perc(<0.5): 0.9772, perc(<5): 0.9980, perc(>5): 0.0020, norm:15.2266, tok/sec: 48548.74, flops:133.34, batch-reuse:1
@ 2 train 10.8131 , allloss: 10.8131, dt: 2053.44ms, perc(<0.5): 0.9772, perc(<5): 0.9980, perc(>5): 0.0020, norm:15.0348, tok/sec: 63830.60, flops:175.31, batch-reuse:1
@ 3 train 10.8042 , allloss: 10.8042, dt: 2054.16ms, perc(<0.5): 0.9772, perc(<5): 0.9980, perc(>5): 0.0020, norm:14.8954, tok/sec: 63808.01, flops:175.24, batch-reuse:1
@ 4 train 10.7927 , allloss: 10.7927, dt: 2051.03ms, perc(<0.5): 0.9772, perc(<5): 0.9980, perc(>5): 0.0020, norm:14.7672, tok/sec: 63905.40, flops:175.51, batch-reuse:1
@ 5 train 10.7836 , allloss: 10.7836, dt: 2058.84ms, perc(<0.5): 0.9771, perc(<5): 0.9980, perc(>5): 0.0020, norm:14.2973, tok/sec: 63663.05, flops:174.85, batch-reuse:1
@ 6 train 10.7620 , allloss: 10.7620, dt: 2059.26ms, perc(<0.5): 0.9771, perc(<5): 0.9980, perc(>5): 0.0020, norm:15.2979, tok/sec: 63649.99, flops:174.81, batch-reuse:1
@ 7 train 10.7377 , allloss: 10.7377, dt: 2060.13ms, perc(<0.5): 0.9770, perc(<5): 0.9980, perc(>5): 0.0020, norm:15.7855, tok/sec: 63623.08, flops:174.74, batch-reuse:1
@ 8 train 10.6970 , allloss: 10.6970, dt: 2058.09ms, perc(<0.5): 0.9769, perc(<5): 0.9980, perc(>5): 0.0020, norm:17.4424, tok/sec: 63686.36, flops:174.91, batch-reuse:1
@ 9 train 10.6283 , allloss: 10.6283, dt: 2057.49ms, perc(<0.5): 0.9768, perc(<5): 0.9980, perc(>5): 0.0020, norm:22.6751, tok/sec: 63704.65, flops:174.96, batch-reuse:1
@ 10 train 10.5144 , allloss: 10.5144, dt: 2058.74ms, perc(<0.5): 0.9768, perc(<5): 0.9980, perc(>5): 0.0020, norm:12.1053, tok/sec: 63666.26, flops:174.86, batch-reuse:1
@ 11 train 10.4571 , allloss: 10.4571, dt: 2059.51ms, perc(<0.5): 0.9767, perc(<5): 0.9980, perc(>5): 0.0020, norm:8.8765, tok/sec: 63642.19, flops:174.79, batch-reuse:1
@ 12 train 10.3936 , allloss: 10.3936, dt: 2059.26ms, perc(<0.5): 0.9765, perc(<5): 0.9980, perc(>5): 0.0020, norm:10.5307, tok/sec: 63650.01, flops:174.81, batch-reuse:1
@ 13 train 10.3130 , allloss: 10.3130, dt: 2058.39ms, perc(<0.5): 0.9764, perc(<5): 0.9980, perc(>5): 0.0020, norm:11.0795, tok/sec: 63676.83, flops:174.88, batch-reuse:1
@ 14 train 10.2671 , allloss: 10.2671, dt: 2058.62ms, perc(<0.5): 0.9762, perc(<5): 0.9980, perc(>5): 0.0020, norm:55.7246, tok/sec: 63669.71, flops:174.86, batch-reuse:1
@ 15 train 10.1459 , allloss: 10.1459, dt: 2058.54ms, perc(<0.5): 0.9761, perc(<5): 0.9980, perc(>5): 0.0020, norm:18.5904, tok/sec: 63672.38, flops:174.87, batch-reuse:1
@ 16 train 9.9690 , allloss: 9.9690, dt: 2057.15ms, perc(<0.5): 0.9760, perc(<5): 0.9980, perc(>5): 0.0020, norm:19.4928, tok/sec: 63715.26, flops:174.99, batch-reuse:1
@ 17 train 9.8768 , allloss: 9.8768, dt: 2060.47ms, perc(<0.5): 0.9758, perc(<5): 0.9980, perc(>5): 0.0020, norm:53.1475, tok/sec: 63612.80, flops:174.71, batch-reuse:1
@ 18 train 9.7678 , allloss: 9.7678, dt: 2058.90ms, perc(<0.5): 0.9755, perc(<5): 0.9980, perc(>5): 0.0020, norm:72.8613, tok/sec: 63661.06, flops:174.84, batch-reuse:1
@ 19 train 9.4975 , allloss: 9.4975, dt: 2061.60ms, perc(<0.5): 0.9755, perc(<5): 0.9980, perc(>5): 0.0020, norm:69.6526, tok/sec: 63577.75, flops:174.61, batch-reuse:1
