Threshold: 0.1
Enable layer loss: True
MAX LEARNING RATE: 0.0006
Experiment name: 10-resescape
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=True, y = res+attn(x), x = x + mlp(LN(y)), ELEMENTWISEAFFINE=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1num decayed parameter tensors: 10, with 53,575,680 parameters
num non-decayed parameter tensors: 8, with 13,824 parameters
@ 0 train 11.0280 , allloss: 135.3289, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-05, norm:23.3564, dt: 5165.80ms, tok/sec: 25373.01, flops:11.17, batch-reuse:1
SIZE COMPARISON prev 1.000056266784668 mid 0.04592081904411316 next 1.0006465911865234
SIZE COMPARISON prev 1.0090301036834717 mid 1.0008518695831299 next 1.0006468296051025
SIZE COMPARISON prev 1.0150115489959717 mid 1.0098850727081299 next 1.0006468296051025
SIZE COMPARISON prev 1.0207979679107666 mid 1.0159316062927246 next 1.0006468296051025
SIZE COMPARISON prev 1.026304006576538 mid 1.0217715501785278 next 1.0006468296051025
SIZE COMPARISON prev 1.0314621925354004 mid 1.0273292064666748 next 1.0006470680236816
SIZE COMPARISON prev 1.0362346172332764 mid 1.0325417518615723 next 1.0006470680236816
SIZE COMPARISON prev 1.0405796766281128 mid 1.0373649597167969 next 1.0006470680236816
SIZE COMPARISON prev 1.0444996356964111 mid 1.041761875152588 next 1.0006470680236816
SIZE COMPARISON prev 1.0480170249938965 mid 1.0457351207733154 next 1.0006470680236816
SIZE COMPARISON prev 1.0511159896850586 mid 1.0493030548095703 next 1.0006470680236816
SIZE COMPARISON prev 1.0538556575775146 mid 1.0524622201919556 next 1.0006470680236816
Final sample (0 word) 
			 ([0.0284423828125], ',')
Final sample (1 word) 
			 ([0.03955078125], ',')
rank 0 sample 0: Hello, I'm a language model,
------
		(Hello:0.6836,:0.4121 I:0.4902'm:0.3965 a:0.4746 language:0.4414 model:0.4746,:0.2871)
		(Hello:0.6680,:0.3965 I:0.4590'm:0.3672 a:0.4434 language:0.4121 model:0.4434,:0.2871)
		(Hello:0.6406,:0.3828 I:0.4297'm:0.3262 a:0.4121 language:0.3809 model:0.3984,:0.2617)
		(Hello:0.5977,:0.3398 I:0.3691'm:0.2734 a:0.3691 language:0.3379 model:0.3555,:0.2383)
		(Hello:0.5352,:0.2988 I:0.3125'm:0.2266 a:0.3125 language:0.2852 model:0.2871,:0.2061)
		(Hello:0.4570,:0.2617 I:0.2500'm:0.1768 a:0.2617 language:0.2373 model:0.2383,:0.1680)
		(Hello:0.3672,:0.2061 I:0.1865'm:0.1289 a:0.2061 language:0.1758 model:0.1865,:0.1289)
		(Hello:0.2734,:0.1592 I:0.1436'm:0.0923 a:0.1592 language:0.1279 model:0.1357,:0.1035)
		(Hello:0.2051,:0.1221 I:0.1035'm:0.0615 a:0.1157 language:0.0918 model:0.1035,:0.0737)
		(Hello:0.1357,:0.0923 I:0.0693'm:0.0417 a:0.0874 language:0.0613 model:0.0737,:0.0549)
		(Hello:0.0918,:0.0654 I:0.0488'm:0.0283 a:0.0618 language:0.0417 model:0.0518,:0.0396)
		(Hello:0.0615,:0.0474 I:0.0342'm:0.0190 a:0.0432 language:0.0282 model:0.0374,:0.0284)
		(Hello:0.0615,:0.0474 I:0.0342'm:0.0190 a:0.0432 language:0.0282 model:0.0374,:0.0284)
,
------
		(,:0.4121 I:0.4902'm:0.3965 a:0.4746 language:0.4414 model:0.4746,:0.2871,:0.4434)
		(,:0.3965 I:0.4590'm:0.3672 a:0.4434 language:0.4121 model:0.4434,:0.2871,:0.4277)
		(,:0.3828 I:0.4297'm:0.3262 a:0.4121 language:0.3809 model:0.3984,:0.2617,:0.3984)
		(,:0.3398 I:0.3691'm:0.2734 a:0.3691 language:0.3379 model:0.3555,:0.2383,:0.3535)
		(,:0.2988 I:0.3125'm:0.2266 a:0.3125 language:0.2852 model:0.2871,:0.2061,:0.2988)
		(,:0.2617 I:0.2500'm:0.1768 a:0.2617 language:0.2373 model:0.2383,:0.1680,:0.2383)
		(,:0.2061 I:0.1865'm:0.1289 a:0.2061 language:0.1758 model:0.1865,:0.1289,:0.1963)
		(,:0.1592 I:0.1436'm:0.0923 a:0.1592 language:0.1279 model:0.1357,:0.1035,:0.1436)
		(,:0.1221 I:0.1035'm:0.0615 a:0.1157 language:0.0918 model:0.1035,:0.0737,:0.1094)
		(,:0.0923 I:0.0693'm:0.0417 a:0.0874 language:0.0613 model:0.0737,:0.0549,:0.0781)
		(,:0.0654 I:0.0488'm:0.0283 a:0.0618 language:0.0417 model:0.0518,:0.0396,:0.0549)
		(,:0.0474 I:0.0342'm:0.0190 a:0.0432 language:0.0282 model:0.0374,:0.0284,:0.0396)
		(,:0.0474 I:0.0342'm:0.0190 a:0.0432 language:0.0282 model:0.0374,:0.0284,:0.0396)
,,,,,,,,,,,
@ 1 train 10.7596 , allloss: 133.2837, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:1.2000e-04, norm:26.6116, dt: 1841.56ms, tok/sec: 71174.44, flops:31.34, batch-reuse:1
@ 2 train 10.2727 , allloss: 129.0169, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:1.8000e-04, norm:29.3294, dt: 1600.84ms, tok/sec: 81877.03, flops:36.06, batch-reuse:1
@ 3 train 9.8120 , allloss: 123.2451, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:2.4000e-04, norm:28.6088, dt: 1601.48ms, tok/sec: 81844.20, flops:36.04, batch-reuse:1
@ 4 train 9.5705 , allloss: 117.8845, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:3.0000e-04, norm:26.3055, dt: 1598.80ms, tok/sec: 81981.62, flops:36.10, batch-reuse:1
@ 5 train 9.3892 , allloss: 113.8996, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:3.6000e-04, norm:23.5282, dt: 1596.57ms, tok/sec: 82096.14, flops:36.15, batch-reuse:1
@ 6 train 9.0521 , allloss: 109.1237, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:4.2000e-04, norm:23.1085, dt: 1590.34ms, tok/sec: 82417.43, flops:36.30, batch-reuse:1
@ 7 train 8.8021 , allloss: 105.8325, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:4.8000e-04, norm:20.3452, dt: 1586.47ms, tok/sec: 82618.90, flops:36.38, batch-reuse:1
@ 8 train 8.5259 , allloss: 102.4052, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:5.4000e-04, norm:17.1849, dt: 1584.89ms, tok/sec: 82700.96, flops:36.42, batch-reuse:1
@ 9 train 8.2679 , allloss: 99.2547, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:14.1070, dt: 1588.30ms, tok/sec: 82523.44, flops:36.34, batch-reuse:1
@ 10 train 8.0824 , allloss: 97.0105, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:10.5629, dt: 1583.48ms, tok/sec: 82774.73, flops:36.45, batch-reuse:1
@ 11 train 7.9377 , allloss: 95.2570, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:10.3711, dt: 1584.58ms, tok/sec: 82717.14, flops:36.43, batch-reuse:1
@ 12 train 7.8136 , allloss: 93.7678, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.2214, dt: 1583.71ms, tok/sec: 82762.70, flops:36.45, batch-reuse:1
@ 13 train 7.7882 , allloss: 93.4555, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.2040, dt: 1583.95ms, tok/sec: 82749.84, flops:36.44, batch-reuse:1
@ 14 train 7.7501 , allloss: 92.9964, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:7.4733, dt: 1585.30ms, tok/sec: 82679.58, flops:36.41, batch-reuse:1
@ 15 train 7.7372 , allloss: 92.8540, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:7.9292, dt: 1585.80ms, tok/sec: 82653.62, flops:36.40, batch-reuse:1
@ 16 train 7.7270 , allloss: 92.7342, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.1232, dt: 1585.97ms, tok/sec: 82644.48, flops:36.40, batch-reuse:1
@ 17 train 7.7950 , allloss: 93.5523, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.4374, dt: 1586.01ms, tok/sec: 82642.63, flops:36.39, batch-reuse:1
@ 18 train 7.8163 , allloss: 93.7982, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.4101, dt: 1586.69ms, tok/sec: 82607.04, flops:36.38, batch-reuse:1
@ 19 train 7.7008 , allloss: 92.4111, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.8340, dt: 1585.13ms, tok/sec: 82688.69, flops:36.41, batch-reuse:1
@ 20 train 7.8319 , allloss: 93.9906, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:8.7916, dt: 1585.63ms, tok/sec: 82662.35, flops:36.40, batch-reuse:1
@ 21 train 7.7709 , allloss: 93.2573, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:7.6111, dt: 1585.86ms, tok/sec: 82650.32, flops:36.40, batch-reuse:1
@ 22 train 7.7291 , allloss: 92.7542, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:6.3549, dt: 1587.81ms, tok/sec: 82548.95, flops:36.35, batch-reuse:1
@ 23 train 7.9511 , allloss: 95.4231, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:7.0173, dt: 1589.49ms, tok/sec: 82461.54, flops:36.31, batch-reuse:1
@ 24 train 7.8804 , allloss: 94.5694, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:5.0090, dt: 1589.13ms, tok/sec: 82480.42, flops:36.32, batch-reuse:1
@ 25 train 7.6333 , allloss: 91.5946, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:5.3187, dt: 1590.54ms, tok/sec: 82407.38, flops:36.29, batch-reuse:1
@ 26 train 7.7728 , allloss: 93.2670, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:4.0808, dt: 1591.45ms, tok/sec: 82360.24, flops:36.27, batch-reuse:1
@ 27 train 7.7425 , allloss: 92.9206, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:4.4685, dt: 1590.57ms, tok/sec: 82405.52, flops:36.29, batch-reuse:1
@ 28 train 7.7702 , allloss: 93.2413, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:4.6760, dt: 1591.84ms, tok/sec: 82339.69, flops:36.26, batch-reuse:1
@ 29 train 7.8236 , allloss: 93.8856, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:4.6510, dt: 1590.91ms, tok/sec: 82387.95, flops:36.28, batch-reuse:1
@ 30 train 7.7192 , allloss: 92.6270, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:3.4925, dt: 1592.24ms, tok/sec: 82319.23, flops:36.25, batch-reuse:1
@ 31 train 7.7730 , allloss: 93.2802, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:3.5285, dt: 1597.66ms, tok/sec: 82039.77, flops:36.13, batch-reuse:1
@ 32 train 7.7721 , allloss: 93.2639, confloss: 0.0000, targetloss: 0.0000, earlystop: 0.000, earlystopdict: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lr:6.0000e-04, norm:4.3818, dt: 1600.70ms, tok/sec: 81884.35, flops:36.06, batch-reuse:1
