Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 13-axm-novalue
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, 
Setting:
========
attn = self.attn(x, x)
mlp = self.mlp(x)
midx = mlp
y = mlp*attn
x = y + res
newres = x
x = self.ln(x)
======== 
VALUEMATRIX=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1{'block_size': 1024, 'vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768}
num decayed parameter tensors: 8, with 50,626,560 parameters
num non-decayed parameter tensors: 6, with 9,984 parameters
@ 0 train 10.8736 , allloss: 10.8736, norm:8.0980, dt: 2970.00ms, tok/sec: 44132.06, flops:18.63, batch-reuse:1
INFO nextres 0.40639182925224304 attn*mlp 0.07568359375 layernormed 0.9997008442878723
			attn_hist -36.75<tensor([  0.,   0.,   0.,  23., 347., 361.,  36.,   1.,   0.,   0.])>45.75
			mlp_hist -0.30078125<tensor([  0.,   0.,   0.,   0.,  37., 732.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2041015625
			x_hist -5.786170959472656<tensor([  0.,   0.,   0.,   0., 358., 409.,   1.,   0.,   0.,   0.])>11.102485656738281
INFO nextres 0.7053378820419312 attn*mlp 0.076171875 layernormed 1.0002028942108154
			attn_hist -69.375<tensor([  0.,   2.,   2.,  19., 335., 389.,  15.,   4.,   1.,   0.])>133.5
			mlp_hist -0.2177734375<tensor([  0.,   0.,   0.,   0.,  42., 728.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2734375
			x_hist -12.348860740661621<tensor([  0.,   0.,   0.,   1., 354., 413.,   0.,   0.,   0.,   0.])>8.05357551574707
INFO nextres 1.3084650039672852 attn*mlp 0.07666015625 layernormed 1.0002650022506714
			attn_hist -148.5<tensor([  0.,   2.,   2.,  22., 327., 393.,  17.,   2.,   0.,   1.])>96.75
			mlp_hist -0.2255859375<tensor([  0.,   0.,   0.,   0.,  42., 728.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.234375
			x_hist -17.501209259033203<tensor([  0.,   0.,   0.,   1., 363., 404.,   0.,   0.,   0.,   0.])>9.474621772766113
INFO nextres 2.215902805328369 attn*mlp 0.0771484375 layernormed 1.000304102897644
			attn_hist -210.0<tensor([  0.,   1.,   2.,  12., 347., 392.,  10.,   1.,   0.,   0.])>114.0
			mlp_hist -0.21484375<tensor([  0.,   0.,   0.,   0.,  40., 728.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.224609375
			x_hist -20.182533264160156<tensor([  0.,   0.,   1.,   0., 363., 404.,   0.,   0.,   0.,   0.])>9.990142822265625
INFO nextres 3.303868055343628 attn*mlp 0.078125 layernormed 1.000327706336975
			attn_hist -241.5<tensor([  0.,   1.,   1.,   8., 353., 395.,   7.,   1.,   0.,   0.])>120.0
			mlp_hist -0.2119140625<tensor([  0.,   0.,   0.,   0.,  46., 720.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.228515625
			x_hist -21.460121154785156<tensor([  0.,   0.,   1.,   0., 362., 404.,   1.,   0.,   0.,   0.])>10.26032543182373
INFO nextres 4.543030738830566 attn*mlp 0.078125 layernormed 1.0003418922424316
			attn_hist -258.0<tensor([  0.,   1.,   2.,   6., 354., 398.,   4.,   1.,   0.,   0.])>123.0
			mlp_hist -0.2158203125<tensor([  0.,   0.,   0.,   0.,  46., 720.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2314453125
			x_hist -22.121030807495117<tensor([  0.,   0.,   1.,   0., 362., 404.,   1.,   0.,   0.,   0.])>10.455002784729004
INFO nextres 5.9133782386779785 attn*mlp 0.07861328125 layernormed 1.000349998474121
			attn_hist -265.5<tensor([  0.,   0.,   3.,   4., 356., 399.,   3.,   1.,   0.,   0.])>125.25
			mlp_hist -0.2158203125<tensor([  0.,   0.,   0.,   0.,  44., 724.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2333984375
			x_hist -22.4898624420166<tensor([  0.,   0.,   1.,   0., 361., 405.,   1.,   0.,   0.,   0.])>10.625472068786621
INFO nextres 7.399190902709961 attn*mlp 0.0791015625 layernormed 1.0003526210784912
			attn_hist -270.0<tensor([  0.,   0.,   3.,   4., 356., 399.,   3.,   1.,   0.,   0.])>127.5
			mlp_hist -0.21484375<tensor([  0.,   0.,   0.,   0.,  45., 724.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2333984375
			x_hist -22.702617645263672<tensor([  0.,   0.,   1.,   0., 360., 406.,   1.,   0.,   0.,   0.])>10.79665756225586
INFO nextres 8.987886428833008 attn*mlp 0.0791015625 layernormed 1.0003516674041748
			attn_hist -273.0<tensor([  0.,   0.,   2.,   5., 355., 401.,   2.,   1.,   0.,   0.])>129.75
			mlp_hist -0.212890625<tensor([  0.,   0.,   0.,   0.,  44., 724.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.234375
			x_hist -22.82545280456543<tensor([  0.,   0.,   1.,   0., 360., 406.,   1.,   0.,   0.,   0.])>10.962486267089844
INFO nextres 10.656027793884277 attn*mlp 0.0791015625 layernormed 1.0003492832183838
			attn_hist -274.5<tensor([  0.,   0.,   2.,   4., 356., 401.,   2.,   1.,   0.,   0.])>131.25
			mlp_hist -0.2109375<tensor([  0.,   0.,   0.,   0.,  46., 720.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2353515625
			x_hist -22.894746780395508<tensor([  0.,   0.,   1.,   0., 360., 406.,   1.,   0.,   0.,   0.])>11.119250297546387
INFO nextres 12.391672134399414 attn*mlp 0.0791015625 layernormed 1.0003467798233032
			attn_hist -274.5<tensor([  0.,   0.,   1.,   4., 357., 401.,   2.,   1.,   0.,   0.])>133.5
			mlp_hist -0.208984375<tensor([  0.,   0.,   0.,   0.,  46., 720.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2353515625
			x_hist -22.923885345458984<tensor([  0.,   0.,   1.,   0., 360., 406.,   1.,   0.,   0.,   0.])>11.282454490661621
INFO nextres 14.176145553588867 attn*mlp 0.0791015625 layernormed 1.0003448724746704
			attn_hist -274.5<tensor([  0.,   0.,   1.,   4., 357., 401.,   2.,   1.,   0.,   0.])>135.75
			mlp_hist -0.20703125<tensor([  0.,   0.,   0.,   0.,  44., 724.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>0.2353515625
			x_hist -22.92630386352539<tensor([  0.,   0.,   1.,   0., 360., 406.,   1.,   0.,   0.,   0.])>11.445168495178223
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( teammate:0.0001ille:0.0001 Clancy:0.0002onial:0.0002 softer:0.0002 rocket:0.0002ib:0.0002 Historical:0.0001)
		(City:0.0002 kidnapping:0.0001block:0.0002VERSION:0.0002gence:0.0002 point:0.0002"></:0.0002 Historical:0.0002)
		(City:0.0002 kidnapping:0.0001 forgiveness:0.0002VERSION:0.0002ynamic:0.0002 Nic:0.0002 Memorial:0.0002 Clancy:0.0002)
		( Maintenance:0.0002bon:0.0002bon:0.0001 neighbors:0.0002ynamic:0.0002 Nic:0.0002 Memorial:0.0001Network:0.0002)
		( Maintenance:0.0002bon:0.0002bon:0.0002 neighbors:0.0001 Commons:0.0001 Maintenance:0.0002 addition:0.0002Network:0.0002)
		(Network:0.0002bon:0.0002bon:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002)
		(Network:0.0002bon:0.0002 diverted:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002)
		(Network:0.0002bon:0.0002 diverted:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002)
		(Network:0.0002bon:0.0002 diverted:0.0002 diverted:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002bon:0.0002)
		(Network:0.0002 reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 swore:0.0002 reviews:0.0002)
		( nationalism:0.0002 reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 reviews:0.0002)
		(usp:0.0002 reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002)
		(usp:0.0002 reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002)
 diverted
------
		(ille:0.0001 Clancy:0.0002onial:0.0002 softer:0.0002 rocket:0.0002ib:0.0002 Historical:0.0001 backstage:0.0001)
		( kidnapping:0.0001block:0.0002VERSION:0.0002gence:0.0002 point:0.0002"></:0.0002 Historical:0.0002 IRA:0.0002)
		( kidnapping:0.0001 forgiveness:0.0002VERSION:0.0002ynamic:0.0002 Nic:0.0002 Memorial:0.0002 Clancy:0.0002 opportun:0.0002)
		(bon:0.0002bon:0.0001 neighbors:0.0002ynamic:0.0002 Nic:0.0002 Memorial:0.0001Network:0.0002 opportun:0.0002)
		(bon:0.0002bon:0.0002 neighbors:0.0001 Commons:0.0001 Maintenance:0.0002 addition:0.0002Network:0.0002 opportun:0.0002)
		(bon:0.0002bon:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002iley:0.0002)
		(bon:0.0002 diverted:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002 addition:0.0002)
		(bon:0.0002 diverted:0.0002bon:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002Network:0.0002 addition:0.0002)
		(bon:0.0002 diverted:0.0002 diverted:0.0002 addition:0.0002 Maintenance:0.0002 addition:0.0002bon:0.0002 addition:0.0002)
		( reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 swore:0.0002 reviews:0.0002 addition:0.0002)
		( reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 reviews:0.0002 addition:0.0002)
		( reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002)
		( reviews:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002 diverted:0.0002)
 diverted addition reviews diverted additioniley swore addition swore addition addition addition addition addition addition
@ 1 train 10.7550 , allloss: 10.7550, norm:8.9465, dt: 1974.21ms, tok/sec: 66392.03, flops:28.03, batch-reuse:1
@ 2 train 10.6338 , allloss: 10.6338, norm:8.0490, dt: 1478.01ms, tok/sec: 88681.69, flops:37.45, batch-reuse:1
@ 3 train 10.5539 , allloss: 10.5539, norm:18.0053, dt: 1355.57ms, tok/sec: 96691.71, flops:40.83, batch-reuse:1
@ 4 train 10.4354 , allloss: 10.4354, norm:6.3222, dt: 1355.67ms, tok/sec: 96684.06, flops:40.82, batch-reuse:1
@ 5 train 10.3059 , allloss: 10.3059, norm:5.8523, dt: 1357.59ms, tok/sec: 96547.21, flops:40.77, batch-reuse:1
@ 6 train 10.1314 , allloss: 10.1314, norm:5.0848, dt: 1359.58ms, tok/sec: 96405.92, flops:40.71, batch-reuse:1
@ 7 train 9.9563 , allloss: 9.9563, norm:4.0397, dt: 1358.24ms, tok/sec: 96501.13, flops:40.75, batch-reuse:1
@ 8 train 9.7634 , allloss: 9.7634, norm:4.0646, dt: 1358.16ms, tok/sec: 96506.78, flops:40.75, batch-reuse:1
@ 9 train 9.5510 , allloss: 9.5510, norm:4.0958, dt: 1356.36ms, tok/sec: 96634.93, flops:40.80, batch-reuse:1
@ 10 train 9.3272 , allloss: 9.3272, norm:3.6356, dt: 1359.27ms, tok/sec: 96428.34, flops:40.72, batch-reuse:1
@ 11 train 9.0902 , allloss: 9.0902, norm:3.1690, dt: 1358.75ms, tok/sec: 96465.09, flops:40.73, batch-reuse:1
@ 12 train 8.8302 , allloss: 8.8302, norm:2.9889, dt: 1359.02ms, tok/sec: 96445.93, flops:40.72, batch-reuse:1
@ 13 train 8.6355 , allloss: 8.6355, norm:2.6246, dt: 1358.42ms, tok/sec: 96488.24, flops:40.74, batch-reuse:1
@ 14 train 8.4321 , allloss: 8.4321, norm:2.3847, dt: 1358.98ms, tok/sec: 96449.00, flops:40.72, batch-reuse:1
@ 15 train 8.2599 , allloss: 8.2599, norm:2.1963, dt: 1356.96ms, tok/sec: 96592.51, flops:40.79, batch-reuse:1
@ 16 train 8.0920 , allloss: 8.0920, norm:1.8605, dt: 1360.54ms, tok/sec: 96338.49, flops:40.68, batch-reuse:1
@ 17 train 7.9758 , allloss: 7.9758, norm:1.6029, dt: 1358.41ms, tok/sec: 96489.03, flops:40.74, batch-reuse:1
@ 18 train 7.8941 , allloss: 7.8941, norm:1.2497, dt: 1358.92ms, tok/sec: 96452.84, flops:40.73, batch-reuse:1
@ 19 train 7.7091 , allloss: 7.7091, norm:1.0342, dt: 1359.36ms, tok/sec: 96422.08, flops:40.71, batch-reuse:1
@ 20 train 7.7579 , allloss: 7.7579, norm:0.6660, dt: 1357.91ms, tok/sec: 96524.83, flops:40.76, batch-reuse:1
@ 21 train 7.7758 , allloss: 7.7758, norm:3.8829, dt: 1358.96ms, tok/sec: 96450.08, flops:40.73, batch-reuse:1
@ 22 train 7.6724 , allloss: 7.6724, norm:0.5999, dt: 1358.90ms, tok/sec: 96454.68, flops:40.73, batch-reuse:1
@ 23 train 7.8770 , allloss: 7.8770, norm:0.4632, dt: 1359.14ms, tok/sec: 96437.46, flops:40.72, batch-reuse:1
@ 24 train 7.8289 , allloss: 7.8289, norm:0.5751, dt: 1358.83ms, tok/sec: 96459.56, flops:40.73, batch-reuse:1
@ 25 train 7.6099 , allloss: 7.6099, norm:0.5829, dt: 1358.03ms, tok/sec: 96516.07, flops:40.75, batch-reuse:1
@ 26 train 7.7412 , allloss: 7.7412, norm:0.5519, dt: 1358.94ms, tok/sec: 96451.91, flops:40.73, batch-reuse:1
@ 27 train 7.7086 , allloss: 7.7086, norm:0.3383, dt: 1357.57ms, tok/sec: 96548.80, flops:40.77, batch-reuse:1
@ 28 train 7.7375 , allloss: 7.7375, norm:0.4281, dt: 1360.13ms, tok/sec: 96367.09, flops:40.69, batch-reuse:1
@ 29 train 7.7907 , allloss: 7.7907, norm:0.3765, dt: 1357.85ms, tok/sec: 96528.96, flops:40.76, batch-reuse:1
@ 30 train 7.6872 , allloss: 7.6872, norm:0.2958, dt: 1359.03ms, tok/sec: 96445.26, flops:40.72, batch-reuse:1
@ 31 train 7.7430 , allloss: 7.7430, norm:0.3415, dt: 1359.20ms, tok/sec: 96432.89, flops:40.72, batch-reuse:1
@ 32 train 7.7321 , allloss: 7.7321, norm:0.3917, dt: 1358.52ms, tok/sec: 96481.45, flops:40.74, batch-reuse:1
@ 33 train 7.6745 , allloss: 7.6745, norm:0.2349, dt: 1358.31ms, tok/sec: 96496.59, flops:40.75, batch-reuse:1
@ 34 train 7.6993 , allloss: 7.6993, norm:0.2803, dt: 1357.95ms, tok/sec: 96521.80, flops:40.76, batch-reuse:1
@ 35 train 7.6554 , allloss: 7.6554, norm:0.4125, dt: 1358.66ms, tok/sec: 96471.86, flops:40.73, batch-reuse:1
@ 36 train 7.6552 , allloss: 7.6552, norm:0.3391, dt: 1358.13ms, tok/sec: 96509.12, flops:40.75, batch-reuse:1
@ 37 train 7.7006 , allloss: 7.7006, norm:0.2842, dt: 1358.57ms, tok/sec: 96478.14, flops:40.74, batch-reuse:1
@ 38 train 7.6186 , allloss: 7.6186, norm:0.3779, dt: 1359.29ms, tok/sec: 96427.12, flops:40.72, batch-reuse:1
@ 39 train 7.6896 , allloss: 7.6896, norm:0.2822, dt: 1359.17ms, tok/sec: 96435.58, flops:40.72, batch-reuse:1
@ 40 train 7.7114 , allloss: 7.7114, norm:0.2735, dt: 1358.51ms, tok/sec: 96481.89, flops:40.74, batch-reuse:1
@ 41 train 7.6850 , allloss: 7.6850, norm:0.2568, dt: 1357.72ms, tok/sec: 96538.47, flops:40.76, batch-reuse:1
@ 42 train 7.6701 , allloss: 7.6701, norm:0.2980, dt: 1359.24ms, tok/sec: 96430.25, flops:40.72, batch-reuse:1
@ 43 train 7.6854 , allloss: 7.6854, norm:0.2938, dt: 1359.82ms, tok/sec: 96389.30, flops:40.70, batch-reuse:1
@ 44 train 7.6991 , allloss: 7.6991, norm:0.2766, dt: 1358.32ms, tok/sec: 96495.40, flops:40.74, batch-reuse:1
@ 45 train 7.6217 , allloss: 7.6217, norm:0.3189, dt: 1359.14ms, tok/sec: 96437.37, flops:40.72, batch-reuse:1
@ 46 train 7.6750 , allloss: 7.6750, norm:0.2709, dt: 1358.98ms, tok/sec: 96449.12, flops:40.72, batch-reuse:1
@ 47 train 7.6188 , allloss: 7.6188, norm:0.2450, dt: 1359.37ms, tok/sec: 96421.19, flops:40.71, batch-reuse:1
@ 48 train 7.6275 , allloss: 7.6275, norm:0.2438, dt: 1359.86ms, tok/sec: 96386.35, flops:40.70, batch-reuse:1
@ 49 train 7.6450 , allloss: 7.6450, norm:0.3145, dt: 1359.88ms, tok/sec: 96384.88, flops:40.70, batch-reuse:1
INFO nextres 16.83034896850586 attn*mlp 1.7109375 layernormed 1.0003751516342163
			attn_hist -28.125<tensor([  0.,   0.,   0.,  28., 344., 351.,  44.,   1.,   0.,   0.])>43.5
			mlp_hist -4.0<tensor([  0.,   0.,   0.,   0., 300., 468.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.046875
			x_hist -3.8628060817718506<tensor([  0.,   0.,   0.,   0., 368., 400.,   0.,   0.,   0.,   0.])>6.217241287231445
INFO nextres 75.27013397216797 attn*mlp 3.53125 layernormed 1.000566840171814
			attn_hist -46.3125<tensor([  0.,   0.,   6.,  28., 334., 361.,  33.,   5.,   1.,   0.])>74.625
			mlp_hist -11.25<tensor([  0.,   0.,   0.,   2., 280., 484.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>7.5
			x_hist -5.191766738891602<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.809775352478027
INFO nextres 135.98135375976562 attn*mlp 3.40625 layernormed 1.000633716583252
			attn_hist -62.25<tensor([  0.,   1.,   7.,  29., 335., 359.,  27.,   8.,   2.,   0.])>69.75
			mlp_hist -9.875<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.40625
			x_hist -5.915077209472656<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>6.077080249786377
INFO nextres 202.46865844726562 attn*mlp 3.375 layernormed 1.000641107559204
			attn_hist -70.875<tensor([  0.,   2.,   6.,  25., 340., 361.,  22.,   9.,   3.,   0.])>72.75
			mlp_hist -9.8125<tensor([  0.,   0.,   0.,   0., 282., 486.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.40625
			x_hist -6.354573726654053<tensor([  0.,   0.,   0.,   0., 373., 395.,   0.,   0.,   0.,   0.])>6.601627826690674
INFO nextres 268.771240234375 attn*mlp 3.265625 layernormed 1.000641107559204
			attn_hist -76.125<tensor([  0.,   2.,   6.,  26., 339., 360.,  23.,   9.,   3.,   0.])>79.125
			mlp_hist -9.5625<tensor([  0.,   0.,   0.,   0., 284., 484.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.28125
			x_hist -6.673903465270996<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>6.996831893920898
INFO nextres 334.14642333984375 attn*mlp 3.171875 layernormed 1.0006402730941772
			attn_hist -80.25<tensor([  1.,   1.,   6.,  26., 339., 361.,  22.,  10.,   1.,   1.])>84.0
			mlp_hist -9.375<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.1875
			x_hist -6.925779342651367<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>7.321129322052002
INFO nextres 398.6474914550781 attn*mlp 3.09375 layernormed 1.0006393194198608
			attn_hist -83.25<tensor([  1.,   1.,   6.,  25., 340., 362.,  21.,  10.,   1.,   1.])>87.75
			mlp_hist -9.25<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.09375
			x_hist -7.129546165466309<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>7.587750434875488
INFO nextres 462.392822265625 attn*mlp 3.03125 layernormed 1.0006383657455444
			attn_hist -85.5<tensor([  1.,   1.,   6.,  24., 341., 362.,  21.,  10.,   1.,   1.])>91.125
			mlp_hist -9.125<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>6.0
			x_hist -7.295510292053223<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>7.815609455108643
INFO nextres 525.4628295898438 attn*mlp 2.984375 layernormed 1.000637412071228
			attn_hist -87.375<tensor([  1.,   1.,   6.,  23., 342., 362.,  21.,  10.,   1.,   1.])>93.75
			mlp_hist -9.0<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>5.9375
			x_hist -7.437760829925537<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>8.014561653137207
INFO nextres 587.9447631835938 attn*mlp 2.9375 layernormed 1.0006365776062012
			attn_hist -89.25<tensor([  1.,   2.,   4.,  23., 343., 362.,  21.,  10.,   1.,   1.])>96.0
			mlp_hist -8.875<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>5.90625
			x_hist -7.562601566314697<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>8.188425064086914
INFO nextres 649.8915405273438 attn*mlp 2.90625 layernormed 1.0006358623504639
			attn_hist -90.75<tensor([  1.,   2.,   4.,  23., 343., 362.,  21.,  10.,   1.,   1.])>98.25
			mlp_hist -8.8125<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>5.84375
			x_hist -7.6746721267700195<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>8.347723007202148
INFO nextres 711.3803100585938 attn*mlp 2.875 layernormed 1.0006351470947266
			attn_hist -92.25<tensor([  1.,   2.,   4.,  21., 345., 362.,  22.,   9.,   1.,   0.])>100.5
			mlp_hist -8.75<tensor([  0.,   0.,   0.,   0., 280., 488.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>5.8125
			x_hist -7.77346658706665<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>8.492046356201172
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Roses:0.0007 the:0.0569 the:0.0601 the:0.0869 the:0.0293 the:0.0640 the:0.0679 the:0.0845)
		(.:0.0596 you:0.0806.:0.0752,:0.1494,:0.1348,:0.1396,:0.1221,:0.1016)
		(.:0.0420.:0.0613.:0.0649,:0.1250,:0.1113,:0.1143,:0.0986,:0.0942)
		( the:0.0427.:0.0542.:0.0559,:0.0947,:0.0859,:0.0894,:0.0806,:0.0801)
		( the:0.0413.:0.0503.:0.0503,:0.0811,:0.0732,:0.0762,:0.0708,:0.0708)
		( the:0.0410.:0.0461.:0.0476,:0.0708,:0.0649,:0.0684,:0.0649,:0.0654)
		( the:0.0393.:0.0435.:0.0447,:0.0645,:0.0579,:0.0623,:0.0591,:0.0598)
		( the:0.0376.:0.0417.:0.0417,:0.0588,:0.0540,:0.0566,:0.0552,:0.0559)
		( the:0.0369.:0.0388.:0.0400,:0.0549,:0.0503,:0.0527,:0.0515,:0.0522)
		( the:0.0352 the:0.0383.:0.0383,:0.0510,:0.0481,:0.0505,:0.0491,:0.0500)
		( the:0.0344.:0.0366.:0.0366,:0.0488,:0.0444,:0.0483,:0.0469,:0.0479)
		( the:0.0337 the:0.0359.:0.0359,:0.0454,:0.0425,:0.0447,:0.0447,:0.0457)
		( the:0.0337 the:0.0359.:0.0359,:0.0454,:0.0425,:0.0447,:0.0447,:0.0457)
,
------
		( the:0.0569 the:0.0601 the:0.0869 the:0.0293 the:0.0640 the:0.0679 the:0.0845 the:0.1064)
		( you:0.0806.:0.0752,:0.1494,:0.1348,:0.1396,:0.1221,:0.1016,:0.1611)
		(.:0.0613.:0.0649,:0.1250,:0.1113,:0.1143,:0.0986,:0.0942,:0.1260)
		(.:0.0542.:0.0559,:0.0947,:0.0859,:0.0894,:0.0806,:0.0801,:0.1030)
		(.:0.0503.:0.0503,:0.0811,:0.0732,:0.0762,:0.0708,:0.0708,:0.0889)
		(.:0.0461.:0.0476,:0.0708,:0.0649,:0.0684,:0.0649,:0.0654,:0.0801)
		(.:0.0435.:0.0447,:0.0645,:0.0579,:0.0623,:0.0591,:0.0598,:0.0737)
		(.:0.0417.:0.0417,:0.0588,:0.0540,:0.0566,:0.0552,:0.0559,:0.0674)
		(.:0.0388.:0.0400,:0.0549,:0.0503,:0.0527,:0.0515,:0.0522,:0.0630)
		( the:0.0383.:0.0383,:0.0510,:0.0481,:0.0505,:0.0491,:0.0500,:0.0586)
		(.:0.0366.:0.0366,:0.0488,:0.0444,:0.0483,:0.0469,:0.0479,:0.0562)
		( the:0.0359.:0.0359,:0.0454,:0.0425,:0.0447,:0.0447,:0.0457,:0.0535)
		( the:0.0359.:0.0359,:0.0454,:0.0425,:0.0447,:0.0447,:0.0457,:0.0535)
,,,,,,,,,,,,,,,
@ 50 train 7.7308 , allloss: 7.7308, norm:0.3483, dt: 1892.22ms, tok/sec: 69268.96, flops:29.25, batch-reuse:1
@ 51 train 7.7844 , allloss: 7.7844, norm:0.2874, dt: 1359.58ms, tok/sec: 96406.04, flops:40.71, batch-reuse:1
@ 52 train 7.6366 , allloss: 7.6366, norm:0.3334, dt: 1361.06ms, tok/sec: 96301.08, flops:40.66, batch-reuse:1
@ 53 train 7.6208 , allloss: 7.6208, norm:0.3701, dt: 1359.61ms, tok/sec: 96404.28, flops:40.71, batch-reuse:1
@ 54 train 7.7150 , allloss: 7.7150, norm:0.3153, dt: 1359.15ms, tok/sec: 96437.00, flops:40.72, batch-reuse:1
@ 55 train 7.7376 , allloss: 7.7376, norm:0.3428, dt: 1360.76ms, tok/sec: 96322.95, flops:40.67, batch-reuse:1
@ 56 train 7.5944 , allloss: 7.5944, norm:0.4465, dt: 1359.51ms, tok/sec: 96410.89, flops:40.71, batch-reuse:1
@ 57 train 7.5171 , allloss: 7.5171, norm:0.3487, dt: 1360.07ms, tok/sec: 96371.44, flops:40.69, batch-reuse:1
@ 58 train 7.5929 , allloss: 7.5929, norm:0.8780, dt: 1361.64ms, tok/sec: 96260.49, flops:40.65, batch-reuse:1
@ 59 train 7.3818 , allloss: 7.3818, norm:0.4660, dt: 1360.90ms, tok/sec: 96312.50, flops:40.67, batch-reuse:1
@ 60 train 7.5819 , allloss: 7.5819, norm:0.4198, dt: 1361.22ms, tok/sec: 96289.75, flops:40.66, batch-reuse:1
@ 61 train 7.5323 , allloss: 7.5323, norm:0.4748, dt: 1360.29ms, tok/sec: 96355.63, flops:40.69, batch-reuse:1
@ 62 train 7.4777 , allloss: 7.4777, norm:0.7335, dt: 1361.54ms, tok/sec: 96267.17, flops:40.65, batch-reuse:1
@ 63 train 7.5159 , allloss: 7.5159, norm:0.5951, dt: 1361.93ms, tok/sec: 96240.02, flops:40.64, batch-reuse:1
@ 64 train 7.4950 , allloss: 7.4950, norm:0.6070, dt: 1361.52ms, tok/sec: 96268.92, flops:40.65, batch-reuse:1
@ 65 train 7.5677 , allloss: 7.5677, norm:0.7259, dt: 1363.04ms, tok/sec: 96161.36, flops:40.60, batch-reuse:1
@ 66 train 7.5912 , allloss: 7.5912, norm:0.5963, dt: 1362.12ms, tok/sec: 96226.56, flops:40.63, batch-reuse:1
@ 67 train 7.4653 , allloss: 7.4653, norm:0.4973, dt: 1361.83ms, tok/sec: 96246.89, flops:40.64, batch-reuse:1
@ 68 train 7.5178 , allloss: 7.5178, norm:0.4834, dt: 1361.83ms, tok/sec: 96246.79, flops:40.64, batch-reuse:1
@ 69 train 7.4004 , allloss: 7.4004, norm:0.4577, dt: 1361.81ms, tok/sec: 96248.66, flops:40.64, batch-reuse:1
@ 70 train 7.4188 , allloss: 7.4188, norm:0.5126, dt: 1360.20ms, tok/sec: 96362.14, flops:40.69, batch-reuse:1
@ 71 train 7.3705 , allloss: 7.3705, norm:0.4854, dt: 1362.03ms, tok/sec: 96232.54, flops:40.63, batch-reuse:1
@ 72 train 7.3893 , allloss: 7.3893, norm:0.4289, dt: 1361.11ms, tok/sec: 96297.91, flops:40.66, batch-reuse:1
@ 73 train 7.4595 , allloss: 7.4595, norm:0.7024, dt: 1361.88ms, tok/sec: 96243.61, flops:40.64, batch-reuse:1
@ 74 train 7.3943 , allloss: 7.3943, norm:0.4376, dt: 1361.91ms, tok/sec: 96241.39, flops:40.64, batch-reuse:1
@ 75 train 7.9233 , allloss: 7.9233, norm:0.7974, dt: 1361.51ms, tok/sec: 96269.60, flops:40.65, batch-reuse:1
@ 76 train 7.3271 , allloss: 7.3271, norm:0.7424, dt: 1362.15ms, tok/sec: 96224.52, flops:40.63, batch-reuse:1
@ 77 train 7.9590 , allloss: 7.9590, norm:1.3546, dt: 1362.10ms, tok/sec: 96227.55, flops:40.63, batch-reuse:1
@ 78 train 7.5334 , allloss: 7.5334, norm:1.0964, dt: 1361.77ms, tok/sec: 96251.04, flops:40.64, batch-reuse:1
@ 79 train 7.2562 , allloss: 7.2562, norm:0.4498, dt: 1361.64ms, tok/sec: 96260.29, flops:40.65, batch-reuse:1
@ 80 train 7.2907 , allloss: 7.2907, norm:0.5027, dt: 1361.98ms, tok/sec: 96236.31, flops:40.64, batch-reuse:1
@ 81 train 7.3011 , allloss: 7.3011, norm:0.4399, dt: 1361.25ms, tok/sec: 96287.64, flops:40.66, batch-reuse:1
@ 82 train 7.2532 , allloss: 7.2532, norm:0.4574, dt: 1361.33ms, tok/sec: 96282.60, flops:40.65, batch-reuse:1
@ 83 train 7.2323 , allloss: 7.2323, norm:0.4386, dt: 1361.32ms, tok/sec: 96283.05, flops:40.65, batch-reuse:1
@ 84 train 7.2553 , allloss: 7.2553, norm:0.5766, dt: 1362.52ms, tok/sec: 96198.12, flops:40.62, batch-reuse:1
@ 85 train 7.1838 , allloss: 7.1838, norm:0.3937, dt: 1362.23ms, tok/sec: 96218.63, flops:40.63, batch-reuse:1
@ 86 train 7.1492 , allloss: 7.1492, norm:0.4091, dt: 1361.89ms, tok/sec: 96242.87, flops:40.64, batch-reuse:1
@ 87 train 7.1908 , allloss: 7.1908, norm:0.8546, dt: 1363.93ms, tok/sec: 96099.03, flops:40.58, batch-reuse:1
@ 88 train 7.1706 , allloss: 7.1706, norm:0.5011, dt: 1362.17ms, tok/sec: 96222.92, flops:40.63, batch-reuse:1
@ 89 train 7.1111 , allloss: 7.1111, norm:0.5643, dt: 1362.73ms, tok/sec: 96183.39, flops:40.61, batch-reuse:1
@ 90 train 7.0831 , allloss: 7.0831, norm:0.5612, dt: 1362.25ms, tok/sec: 96217.40, flops:40.63, batch-reuse:1
@ 91 train 7.0868 , allloss: 7.0868, norm:0.6553, dt: 1362.79ms, tok/sec: 96178.90, flops:40.61, batch-reuse:1
@ 92 train 7.0582 , allloss: 7.0582, norm:0.6071, dt: 1362.14ms, tok/sec: 96225.26, flops:40.63, batch-reuse:1
@ 93 train 7.0301 , allloss: 7.0301, norm:0.4850, dt: 1362.51ms, tok/sec: 96198.86, flops:40.62, batch-reuse:1
@ 94 train 7.2719 , allloss: 7.2719, norm:0.7724, dt: 1362.47ms, tok/sec: 96201.89, flops:40.62, batch-reuse:1
@ 95 train 7.0348 , allloss: 7.0348, norm:0.6947, dt: 1361.29ms, tok/sec: 96284.87, flops:40.66, batch-reuse:1
@ 96 train 6.9244 , allloss: 6.9244, norm:0.4191, dt: 1361.56ms, tok/sec: 96265.74, flops:40.65, batch-reuse:1
@ 97 train 6.9643 , allloss: 6.9643, norm:0.6053, dt: 1361.52ms, tok/sec: 96269.19, flops:40.65, batch-reuse:1
@ 98 train 7.0052 , allloss: 7.0052, norm:0.5432, dt: 1361.72ms, tok/sec: 96254.63, flops:40.64, batch-reuse:1
@ 99 train 6.9323 , allloss: 6.9323, norm:0.5725, dt: 1362.01ms, tok/sec: 96234.33, flops:40.63, batch-reuse:1
@ 100 train 6.9059 , allloss: 6.9059, norm:0.4237, dt: 1362.20ms, tok/sec: 96221.07, flops:40.63, batch-reuse:1
@ 101 train 6.7729 , allloss: 6.7729, norm:0.4740, dt: 1361.51ms, tok/sec: 96269.73, flops:40.65, batch-reuse:1
@ 102 train 6.8599 , allloss: 6.8599, norm:0.4061, dt: 1362.05ms, tok/sec: 96231.56, flops:40.63, batch-reuse:1
@ 103 train 6.8365 , allloss: 6.8365, norm:0.4074, dt: 1362.07ms, tok/sec: 96230.00, flops:40.63, batch-reuse:1
@ 104 train 6.9019 , allloss: 6.9019, norm:0.4344, dt: 1362.07ms, tok/sec: 96230.18, flops:40.63, batch-reuse:1
@ 105 train 6.8685 , allloss: 6.8685, norm:0.4006, dt: 1361.83ms, tok/sec: 96247.25, flops:40.64, batch-reuse:1
@ 106 train 6.7790 , allloss: 6.7790, norm:0.3303, dt: 1362.41ms, tok/sec: 96205.90, flops:40.62, batch-reuse:1
@ 107 train 6.7762 , allloss: 6.7762, norm:0.4296, dt: 1362.42ms, tok/sec: 96205.31, flops:40.62, batch-reuse:1
@ 108 train 6.7408 , allloss: 6.7408, norm:0.4291, dt: 1362.30ms, tok/sec: 96213.54, flops:40.63, batch-reuse:1
@ 109 train 6.7059 , allloss: 6.7059, norm:0.4061, dt: 1362.35ms, tok/sec: 96210.41, flops:40.62, batch-reuse:1
@ 110 train 6.7243 , allloss: 6.7243, norm:0.3012, dt: 1361.68ms, tok/sec: 96257.83, flops:40.64, batch-reuse:1
@ 111 train 6.7282 , allloss: 6.7282, norm:0.3554, dt: 1362.06ms, tok/sec: 96230.52, flops:40.63, batch-reuse:1
@ 112 train 6.7359 , allloss: 6.7359, norm:0.3422, dt: 1361.59ms, tok/sec: 96263.60, flops:40.65, batch-reuse:1
@ 113 train 6.6870 , allloss: 6.6870, norm:0.3257, dt: 1361.80ms, tok/sec: 96249.35, flops:40.64, batch-reuse:1
@ 114 train 6.6876 , allloss: 6.6876, norm:0.3663, dt: 1361.60ms, tok/sec: 96263.53, flops:40.65, batch-reuse:1
@ 115 train 6.6059 , allloss: 6.6059, norm:0.3002, dt: 1361.87ms, tok/sec: 96244.45, flops:40.64, batch-reuse:1
@ 116 train 6.5782 , allloss: 6.5782, norm:0.3574, dt: 1362.56ms, tok/sec: 96195.45, flops:40.62, batch-reuse:1
@ 117 train 6.5920 , allloss: 6.5920, norm:0.3971, dt: 1361.95ms, tok/sec: 96238.82, flops:40.64, batch-reuse:1
@ 118 train 6.5876 , allloss: 6.5876, norm:0.5565, dt: 1362.91ms, tok/sec: 96170.64, flops:40.61, batch-reuse:1
@ 119 train 6.5175 , allloss: 6.5175, norm:0.6202, dt: 1361.85ms, tok/sec: 96245.38, flops:40.64, batch-reuse:1
@ 120 train 6.6876 , allloss: 6.6876, norm:0.5179, dt: 1361.95ms, tok/sec: 96238.67, flops:40.64, batch-reuse:1
@ 121 train 6.7300 , allloss: 6.7300, norm:0.4631, dt: 1361.91ms, tok/sec: 96241.15, flops:40.64, batch-reuse:1
@ 122 train 6.6795 , allloss: 6.6795, norm:0.5686, dt: 1361.85ms, tok/sec: 96245.66, flops:40.64, batch-reuse:1
@ 123 train 6.5448 , allloss: 6.5448, norm:0.3996, dt: 1361.90ms, tok/sec: 96241.97, flops:40.64, batch-reuse:1
@ 124 train 6.5766 , allloss: 6.5766, norm:0.4220, dt: 1361.87ms, tok/sec: 96244.42, flops:40.64, batch-reuse:1
@ 125 train 6.5944 , allloss: 6.5944, norm:0.5042, dt: 1362.12ms, tok/sec: 96226.68, flops:40.63, batch-reuse:1
@ 126 train 6.5506 , allloss: 6.5506, norm:0.3397, dt: 1361.91ms, tok/sec: 96241.08, flops:40.64, batch-reuse:1
@ 127 train 6.4990 , allloss: 6.4990, norm:0.4849, dt: 1362.03ms, tok/sec: 96232.84, flops:40.63, batch-reuse:1
@ 128 train 6.4556 , allloss: 6.4556, norm:0.4262, dt: 1362.02ms, tok/sec: 96233.65, flops:40.63, batch-reuse:1
@ 129 train 6.5481 , allloss: 6.5481, norm:0.5046, dt: 1361.94ms, tok/sec: 96238.99, flops:40.64, batch-reuse:1
@ 130 train 6.5483 , allloss: 6.5483, norm:0.3511, dt: 1361.73ms, tok/sec: 96253.97, flops:40.64, batch-reuse:1
@ 131 train 6.5271 , allloss: 6.5271, norm:0.4845, dt: 1361.95ms, tok/sec: 96238.57, flops:40.64, batch-reuse:1
@ 132 train 6.5553 , allloss: 6.5553, norm:0.4256, dt: 1362.40ms, tok/sec: 96206.74, flops:40.62, batch-reuse:1
@ 133 train 6.4619 , allloss: 6.4619, norm:0.3936, dt: 1361.38ms, tok/sec: 96279.00, flops:40.65, batch-reuse:1
@ 134 train 6.5220 , allloss: 6.5220, norm:0.3526, dt: 1362.02ms, tok/sec: 96233.32, flops:40.63, batch-reuse:1
@ 135 train 6.3926 , allloss: 6.3926, norm:0.4040, dt: 1362.06ms, tok/sec: 96230.65, flops:40.63, batch-reuse:1
@ 136 train 6.4271 , allloss: 6.4271, norm:0.4071, dt: 1362.21ms, tok/sec: 96220.21, flops:40.63, batch-reuse:1
@ 137 train 6.3243 , allloss: 6.3243, norm:0.3502, dt: 1362.04ms, tok/sec: 96231.80, flops:40.63, batch-reuse:1
@ 138 train 6.3184 , allloss: 6.3184, norm:0.4436, dt: 1361.99ms, tok/sec: 96235.51, flops:40.63, batch-reuse:1
@ 139 train 6.3096 , allloss: 6.3096, norm:0.4108, dt: 1361.99ms, tok/sec: 96235.34, flops:40.63, batch-reuse:1
@ 140 train 6.2984 , allloss: 6.2984, norm:0.5380, dt: 1362.29ms, tok/sec: 96214.20, flops:40.63, batch-reuse:1
@ 141 train 6.2804 , allloss: 6.2804, norm:0.4773, dt: 1361.73ms, tok/sec: 96253.74, flops:40.64, batch-reuse:1
@ 142 train 6.3744 , allloss: 6.3744, norm:0.4640, dt: 1361.91ms, tok/sec: 96241.49, flops:40.64, batch-reuse:1
@ 143 train 6.3172 , allloss: 6.3172, norm:0.3143, dt: 1362.22ms, tok/sec: 96219.62, flops:40.63, batch-reuse:1
@ 144 train 6.2920 , allloss: 6.2920, norm:0.4201, dt: 1362.33ms, tok/sec: 96211.61, flops:40.62, batch-reuse:1
@ 145 train 6.2704 , allloss: 6.2704, norm:0.3217, dt: 1362.53ms, tok/sec: 96197.57, flops:40.62, batch-reuse:1
@ 146 train 6.4356 , allloss: 6.4356, norm:0.4135, dt: 1362.12ms, tok/sec: 96226.19, flops:40.63, batch-reuse:1
@ 147 train 6.2844 , allloss: 6.2844, norm:0.3875, dt: 1362.37ms, tok/sec: 96208.88, flops:40.62, batch-reuse:1
@ 148 train 6.2448 , allloss: 6.2448, norm:0.3232, dt: 1362.38ms, tok/sec: 96207.94, flops:40.62, batch-reuse:1
@ 149 train 6.1038 , allloss: 6.1038, norm:0.3215, dt: 1362.19ms, tok/sec: 96221.74, flops:40.63, batch-reuse:1
@ 150 train 6.3380 , allloss: 6.3380, norm:0.3429, dt: 1362.12ms, tok/sec: 96226.24, flops:40.63, batch-reuse:1
@ 151 train 6.1237 , allloss: 6.1237, norm:0.3487, dt: 1362.21ms, tok/sec: 96219.91, flops:40.63, batch-reuse:1
@ 152 train 6.1569 , allloss: 6.1569, norm:0.5179, dt: 1362.31ms, tok/sec: 96213.22, flops:40.63, batch-reuse:1
@ 153 train 6.0874 , allloss: 6.0874, norm:0.4858, dt: 1362.59ms, tok/sec: 96193.46, flops:40.62, batch-reuse:1
@ 154 train 6.2124 , allloss: 6.2124, norm:0.4265, dt: 1362.58ms, tok/sec: 96194.23, flops:40.62, batch-reuse:1
@ 155 train 6.3433 , allloss: 6.3433, norm:0.3224, dt: 1362.14ms, tok/sec: 96225.03, flops:40.63, batch-reuse:1
@ 156 train 6.1749 , allloss: 6.1749, norm:0.4223, dt: 1361.46ms, tok/sec: 96272.78, flops:40.65, batch-reuse:1
@ 157 train 6.2582 , allloss: 6.2582, norm:0.4404, dt: 1362.17ms, tok/sec: 96222.64, flops:40.63, batch-reuse:1
@ 158 train 6.2319 , allloss: 6.2319, norm:0.3455, dt: 1361.97ms, tok/sec: 96237.19, flops:40.64, batch-reuse:1
@ 159 train 6.2504 , allloss: 6.2504, norm:0.3705, dt: 1362.39ms, tok/sec: 96207.63, flops:40.62, batch-reuse:1
@ 160 train 6.1537 , allloss: 6.1537, norm:0.4720, dt: 1362.33ms, tok/sec: 96211.30, flops:40.62, batch-reuse:1
@ 161 train 6.1350 , allloss: 6.1350, norm:0.3655, dt: 1362.23ms, tok/sec: 96218.91, flops:40.63, batch-reuse:1
@ 162 train 6.2570 , allloss: 6.2570, norm:0.3467, dt: 1362.54ms, tok/sec: 96196.76, flops:40.62, batch-reuse:1
@ 163 train 6.1516 , allloss: 6.1516, norm:0.3620, dt: 1362.19ms, tok/sec: 96221.64, flops:40.63, batch-reuse:1
@ 164 train 6.0900 , allloss: 6.0900, norm:0.3238, dt: 1362.09ms, tok/sec: 96228.67, flops:40.63, batch-reuse:1
@ 165 train 6.1987 , allloss: 6.1987, norm:0.3226, dt: 1361.99ms, tok/sec: 96235.54, flops:40.63, batch-reuse:1
@ 166 train 6.1746 , allloss: 6.1746, norm:0.4749, dt: 1362.15ms, tok/sec: 96224.32, flops:40.63, batch-reuse:1
@ 167 train 6.2057 , allloss: 6.2057, norm:0.5344, dt: 1362.15ms, tok/sec: 96224.35, flops:40.63, batch-reuse:1
@ 168 train 6.0621 , allloss: 6.0621, norm:0.5413, dt: 1361.67ms, tok/sec: 96258.52, flops:40.64, batch-reuse:1
@ 169 train 6.1085 , allloss: 6.1085, norm:0.3804, dt: 1361.63ms, tok/sec: 96260.93, flops:40.65, batch-reuse:1
@ 170 train 6.0850 , allloss: 6.0850, norm:0.4750, dt: 1361.25ms, tok/sec: 96287.99, flops:40.66, batch-reuse:1
@ 171 train 6.0614 , allloss: 6.0614, norm:0.4973, dt: 1362.08ms, tok/sec: 96229.15, flops:40.63, batch-reuse:1
@ 172 train 6.0979 , allloss: 6.0979, norm:0.4114, dt: 1361.96ms, tok/sec: 96237.61, flops:40.64, batch-reuse:1
@ 173 train 6.0784 , allloss: 6.0784, norm:0.4561, dt: 1361.31ms, tok/sec: 96284.01, flops:40.66, batch-reuse:1
@ 174 train 6.2149 , allloss: 6.2149, norm:0.3562, dt: 1362.53ms, tok/sec: 96197.43, flops:40.62, batch-reuse:1
@ 175 train 6.0970 , allloss: 6.0970, norm:1.4156, dt: 1362.03ms, tok/sec: 96232.59, flops:40.63, batch-reuse:1
@ 176 train 6.1296 , allloss: 6.1296, norm:0.4854, dt: 1362.35ms, tok/sec: 96210.48, flops:40.62, batch-reuse:1
@ 177 train 6.0813 , allloss: 6.0813, norm:0.5013, dt: 1362.49ms, tok/sec: 96200.61, flops:40.62, batch-reuse:1
@ 178 train 6.1145 , allloss: 6.1145, norm:0.3928, dt: 1362.53ms, tok/sec: 96197.40, flops:40.62, batch-reuse:1
@ 179 train 6.0018 , allloss: 6.0018, norm:0.3502, dt: 1361.94ms, tok/sec: 96239.09, flops:40.64, batch-reuse:1
@ 180 train 6.1056 , allloss: 6.1056, norm:0.5117, dt: 1361.71ms, tok/sec: 96255.20, flops:40.64, batch-reuse:1
@ 181 train 5.9791 , allloss: 5.9791, norm:0.4967, dt: 1362.48ms, tok/sec: 96200.88, flops:40.62, batch-reuse:1
@ 182 train 6.0966 , allloss: 6.0966, norm:0.3496, dt: 1361.97ms, tok/sec: 96237.24, flops:40.64, batch-reuse:1
@ 183 train 6.0538 , allloss: 6.0538, norm:0.4085, dt: 1362.54ms, tok/sec: 96197.04, flops:40.62, batch-reuse:1
@ 184 train 5.9411 , allloss: 5.9411, norm:0.3727, dt: 1362.49ms, tok/sec: 96200.01, flops:40.62, batch-reuse:1
@ 185 train 5.9760 , allloss: 5.9760, norm:0.4003, dt: 1362.03ms, tok/sec: 96232.66, flops:40.63, batch-reuse:1
@ 186 train 6.1332 , allloss: 6.1332, norm:0.3483, dt: 1361.66ms, tok/sec: 96258.96, flops:40.64, batch-reuse:1
@ 187 train 6.2725 , allloss: 6.2725, norm:0.4699, dt: 1361.62ms, tok/sec: 96261.94, flops:40.65, batch-reuse:1
@ 188 train 6.1081 , allloss: 6.1081, norm:0.4016, dt: 1361.32ms, tok/sec: 96283.36, flops:40.65, batch-reuse:1
@ 189 train 6.2246 , allloss: 6.2246, norm:0.3281, dt: 1362.18ms, tok/sec: 96222.55, flops:40.63, batch-reuse:1
@ 190 train 6.1934 , allloss: 6.1934, norm:0.4443, dt: 1361.80ms, tok/sec: 96249.03, flops:40.64, batch-reuse:1
@ 191 train 6.1752 , allloss: 6.1752, norm:0.4045, dt: 1362.20ms, tok/sec: 96221.09, flops:40.63, batch-reuse:1
@ 192 train 6.2230 , allloss: 6.2230, norm:0.3262, dt: 1362.22ms, tok/sec: 96219.69, flops:40.63, batch-reuse:1
@ 193 train 6.0970 , allloss: 6.0970, norm:0.4409, dt: 1362.18ms, tok/sec: 96222.54, flops:40.63, batch-reuse:1
@ 194 train 6.2111 , allloss: 6.2111, norm:0.3970, dt: 1362.07ms, tok/sec: 96230.32, flops:40.63, batch-reuse:1
@ 195 train 6.1754 , allloss: 6.1754, norm:0.3079, dt: 1362.20ms, tok/sec: 96221.07, flops:40.63, batch-reuse:1
@ 196 train 6.0865 , allloss: 6.0865, norm:0.4143, dt: 1362.27ms, tok/sec: 96215.97, flops:40.63, batch-reuse:1
@ 197 train 6.1148 , allloss: 6.1148, norm:0.3362, dt: 1361.72ms, tok/sec: 96254.76, flops:40.64, batch-reuse:1
@ 198 train 6.0656 , allloss: 6.0656, norm:0.3633, dt: 1362.11ms, tok/sec: 96227.20, flops:40.63, batch-reuse:1
@ 199 train 6.1494 , allloss: 6.1494, norm:0.3897, dt: 1362.28ms, tok/sec: 96215.36, flops:40.63, batch-reuse:1
@ 200 train 6.1426 , allloss: 6.1426, norm:0.3161, dt: 1362.22ms, tok/sec: 96219.07, flops:40.63, batch-reuse:1
@ 201 train 6.0787 , allloss: 6.0787, norm:0.3135, dt: 1362.41ms, tok/sec: 96206.32, flops:40.62, batch-reuse:1
@ 202 train 6.1943 , allloss: 6.1943, norm:0.3208, dt: 1362.32ms, tok/sec: 96212.50, flops:40.63, batch-reuse:1
@ 203 train 6.1115 , allloss: 6.1115, norm:0.3685, dt: 1361.72ms, tok/sec: 96254.48, flops:40.64, batch-reuse:1
@ 204 train 6.0859 , allloss: 6.0859, norm:0.3081, dt: 1361.95ms, tok/sec: 96238.52, flops:40.64, batch-reuse:1
@ 205 train 6.0873 , allloss: 6.0873, norm:0.3466, dt: 1362.05ms, tok/sec: 96231.41, flops:40.63, batch-reuse:1
@ 206 train 6.1383 , allloss: 6.1383, norm:0.3249, dt: 1362.48ms, tok/sec: 96200.80, flops:40.62, batch-reuse:1
@ 207 train 6.0434 , allloss: 6.0434, norm:0.3367, dt: 1362.21ms, tok/sec: 96220.46, flops:40.63, batch-reuse:1
@ 208 train 6.0758 , allloss: 6.0758, norm:0.3532, dt: 1361.87ms, tok/sec: 96244.37, flops:40.64, batch-reuse:1
@ 209 train 6.1323 , allloss: 6.1323, norm:0.2978, dt: 1362.37ms, tok/sec: 96209.08, flops:40.62, batch-reuse:1
@ 210 train 6.0709 , allloss: 6.0709, norm:0.3351, dt: 1361.83ms, tok/sec: 96247.08, flops:40.64, batch-reuse:1
@ 211 train 6.0207 , allloss: 6.0207, norm:0.3681, dt: 1362.15ms, tok/sec: 96224.29, flops:40.63, batch-reuse:1
@ 212 train 6.0716 , allloss: 6.0716, norm:0.3353, dt: 1362.27ms, tok/sec: 96216.00, flops:40.63, batch-reuse:1
@ 213 train 5.9977 , allloss: 5.9977, norm:0.3597, dt: 1362.35ms, tok/sec: 96210.16, flops:40.62, batch-reuse:1
@ 214 train 6.0127 , allloss: 6.0127, norm:0.4023, dt: 1362.61ms, tok/sec: 96191.54, flops:40.62, batch-reuse:1
@ 215 train 5.9337 , allloss: 5.9337, norm:0.5143, dt: 1361.95ms, tok/sec: 96238.34, flops:40.64, batch-reuse:1
@ 216 train 5.9380 , allloss: 5.9380, norm:0.4347, dt: 1361.86ms, tok/sec: 96244.52, flops:40.64, batch-reuse:1
@ 217 train 6.0183 , allloss: 6.0183, norm:0.3152, dt: 1362.44ms, tok/sec: 96203.86, flops:40.62, batch-reuse:1
@ 218 train 5.9207 , allloss: 5.9207, norm:0.4513, dt: 1362.53ms, tok/sec: 96197.30, flops:40.62, batch-reuse:1
@ 219 train 6.1173 , allloss: 6.1173, norm:0.4443, dt: 1362.45ms, tok/sec: 96203.07, flops:40.62, batch-reuse:1
@ 220 train 6.0325 , allloss: 6.0325, norm:0.3419, dt: 1362.03ms, tok/sec: 96232.89, flops:40.63, batch-reuse:1
@ 221 train 6.0036 , allloss: 6.0036, norm:0.5042, dt: 1362.57ms, tok/sec: 96195.01, flops:40.62, batch-reuse:1
@ 222 train 5.9817 , allloss: 5.9817, norm:0.4180, dt: 1362.06ms, tok/sec: 96230.65, flops:40.63, batch-reuse:1
@ 223 train 5.9856 , allloss: 5.9856, norm:0.3138, dt: 1361.90ms, tok/sec: 96241.71, flops:40.64, batch-reuse:1
@ 224 train 5.9002 , allloss: 5.9002, norm:0.3607, dt: 1362.97ms, tok/sec: 96166.25, flops:40.61, batch-reuse:1
@ 225 train 6.0060 , allloss: 6.0060, norm:0.3352, dt: 1361.94ms, tok/sec: 96239.51, flops:40.64, batch-reuse:1
@ 226 train 6.0063 , allloss: 6.0063, norm:0.3661, dt: 1361.32ms, tok/sec: 96283.30, flops:40.65, batch-reuse:1
@ 227 train 5.9948 , allloss: 5.9948, norm:0.3087, dt: 1361.47ms, tok/sec: 96272.40, flops:40.65, batch-reuse:1
@ 228 train 5.9886 , allloss: 5.9886, norm:0.3349, dt: 1361.22ms, tok/sec: 96290.10, flops:40.66, batch-reuse:1
@ 229 train 5.9879 , allloss: 5.9879, norm:0.2998, dt: 1362.01ms, tok/sec: 96234.33, flops:40.63, batch-reuse:1
@ 230 train 5.9717 , allloss: 5.9717, norm:0.2880, dt: 1362.46ms, tok/sec: 96202.55, flops:40.62, batch-reuse:1
@ 231 train 5.9238 , allloss: 5.9238, norm:0.3188, dt: 1361.91ms, tok/sec: 96241.12, flops:40.64, batch-reuse:1
@ 232 train 5.8714 , allloss: 5.8714, norm:0.3321, dt: 1361.90ms, tok/sec: 96242.14, flops:40.64, batch-reuse:1
@ 233 train 5.9544 , allloss: 5.9544, norm:0.3239, dt: 1362.34ms, tok/sec: 96211.08, flops:40.62, batch-reuse:1
@ 234 train 5.8362 , allloss: 5.8362, norm:0.3518, dt: 1362.27ms, tok/sec: 96215.95, flops:40.63, batch-reuse:1
@ 235 train 5.9304 , allloss: 5.9304, norm:0.3770, dt: 1362.43ms, tok/sec: 96204.28, flops:40.62, batch-reuse:1
@ 236 train 5.8271 , allloss: 5.8271, norm:0.3122, dt: 1362.21ms, tok/sec: 96219.98, flops:40.63, batch-reuse:1
@ 237 train 5.9123 , allloss: 5.9123, norm:0.2939, dt: 1364.05ms, tok/sec: 96090.51, flops:40.57, batch-reuse:1
@ 238 train 5.8693 , allloss: 5.8693, norm:0.3323, dt: 1363.35ms, tok/sec: 96140.00, flops:40.59, batch-reuse:1
@ 239 train 5.8688 , allloss: 5.8688, norm:0.3114, dt: 1362.27ms, tok/sec: 96216.05, flops:40.63, batch-reuse:1
@ 240 train 5.9255 , allloss: 5.9255, norm:0.3149, dt: 1362.23ms, tok/sec: 96218.66, flops:40.63, batch-reuse:1
@ 241 train 5.9162 , allloss: 5.9162, norm:0.2972, dt: 1361.24ms, tok/sec: 96288.92, flops:40.66, batch-reuse:1
@ 242 train 5.8737 , allloss: 5.8737, norm:0.3752, dt: 1362.23ms, tok/sec: 96218.39, flops:40.63, batch-reuse:1
@ 243 train 5.8020 , allloss: 5.8020, norm:0.3407, dt: 1361.90ms, tok/sec: 96242.08, flops:40.64, batch-reuse:1
@ 244 train 5.9162 , allloss: 5.9162, norm:0.3070, dt: 1361.85ms, tok/sec: 96245.33, flops:40.64, batch-reuse:1
@ 245 train 5.6688 , allloss: 5.6688, norm:0.3658, dt: 1362.15ms, tok/sec: 96224.10, flops:40.63, batch-reuse:1
@ 246 train 5.9666 , allloss: 5.9666, norm:0.4995, dt: 1362.21ms, tok/sec: 96220.28, flops:40.63, batch-reuse:1
@ 247 train 5.8751 , allloss: 5.8751, norm:0.6198, dt: 1362.39ms, tok/sec: 96207.46, flops:40.62, batch-reuse:1
@ 248 train 5.8249 , allloss: 5.8249, norm:0.4426, dt: 1361.47ms, tok/sec: 96272.28, flops:40.65, batch-reuse:1
@ 249 train 5.7875 , allloss: 5.7875, norm:0.3814, dt: 1361.86ms, tok/sec: 96244.81, flops:40.64, batch-reuse:1
@ 250 train 5.8609 , allloss: 5.8609, norm:0.5006, dt: 1361.71ms, tok/sec: 96255.59, flops:40.64, batch-reuse:1
@ 251 train 5.8085 , allloss: 5.8085, norm:0.3452, dt: 1361.38ms, tok/sec: 96278.95, flops:40.65, batch-reuse:1
@ 252 train 5.9205 , allloss: 5.9205, norm:0.4974, dt: 1361.96ms, tok/sec: 96237.44, flops:40.64, batch-reuse:1
@ 253 train 5.8204 , allloss: 5.8204, norm:0.5286, dt: 1362.17ms, tok/sec: 96222.96, flops:40.63, batch-reuse:1
@ 254 train 5.7445 , allloss: 5.7445, norm:0.3583, dt: 1362.16ms, tok/sec: 96223.29, flops:40.63, batch-reuse:1
@ 255 train 5.7586 , allloss: 5.7586, norm:0.5023, dt: 1361.81ms, tok/sec: 96248.31, flops:40.64, batch-reuse:1
@ 256 train 5.8132 , allloss: 5.8132, norm:0.4112, dt: 1362.02ms, tok/sec: 96233.35, flops:40.63, batch-reuse:1
@ 257 train 5.8088 , allloss: 5.8088, norm:0.4775, dt: 1362.30ms, tok/sec: 96213.98, flops:40.63, batch-reuse:1
@ 258 train 5.7558 , allloss: 5.7558, norm:0.4846, dt: 1361.91ms, tok/sec: 96241.22, flops:40.64, batch-reuse:1
@ 259 train 5.7817 , allloss: 5.7817, norm:0.3223, dt: 1363.54ms, tok/sec: 96126.25, flops:40.59, batch-reuse:1
@ 260 train 5.8350 , allloss: 5.8350, norm:0.3839, dt: 1362.33ms, tok/sec: 96211.77, flops:40.62, batch-reuse:1
@ 261 train 5.8160 , allloss: 5.8160, norm:0.3346, dt: 1362.07ms, tok/sec: 96230.17, flops:40.63, batch-reuse:1
@ 262 train 5.9016 , allloss: 5.9016, norm:0.3792, dt: 1361.86ms, tok/sec: 96244.62, flops:40.64, batch-reuse:1
@ 263 train 5.8425 , allloss: 5.8425, norm:0.4705, dt: 1362.84ms, tok/sec: 96175.69, flops:40.61, batch-reuse:1
@ 264 train 5.7577 , allloss: 5.7577, norm:0.3959, dt: 1362.38ms, tok/sec: 96208.27, flops:40.62, batch-reuse:1
@ 265 train 5.7691 , allloss: 5.7691, norm:0.3625, dt: 1361.72ms, tok/sec: 96254.44, flops:40.64, batch-reuse:1
@ 266 train 5.7330 , allloss: 5.7330, norm:0.3515, dt: 1362.55ms, tok/sec: 96196.30, flops:40.62, batch-reuse:1
@ 267 train 5.7599 , allloss: 5.7599, norm:0.4005, dt: 1362.49ms, tok/sec: 96200.18, flops:40.62, batch-reuse:1
@ 268 train 5.8381 , allloss: 5.8381, norm:0.3704, dt: 1362.26ms, tok/sec: 96216.61, flops:40.63, batch-reuse:1
@ 269 train 5.7894 , allloss: 5.7894, norm:0.4090, dt: 1362.35ms, tok/sec: 96210.14, flops:40.62, batch-reuse:1
@ 270 train 5.8749 , allloss: 5.8749, norm:0.3509, dt: 1362.46ms, tok/sec: 96202.16, flops:40.62, batch-reuse:1
@ 271 train 5.7481 , allloss: 5.7481, norm:0.3899, dt: 1362.12ms, tok/sec: 96226.31, flops:40.63, batch-reuse:1
@ 272 train 5.7625 , allloss: 5.7625, norm:0.3740, dt: 1361.72ms, tok/sec: 96254.66, flops:40.64, batch-reuse:1
@ 273 train 5.7990 , allloss: 5.7990, norm:0.5330, dt: 1362.05ms, tok/sec: 96231.26, flops:40.63, batch-reuse:1
@ 274 train 5.7589 , allloss: 5.7589, norm:0.3961, dt: 1362.17ms, tok/sec: 96222.70, flops:40.63, batch-reuse:1
@ 275 train 5.7433 , allloss: 5.7433, norm:0.4588, dt: 1362.19ms, tok/sec: 96221.24, flops:40.63, batch-reuse:1
@ 276 train 5.7223 , allloss: 5.7223, norm:0.4505, dt: 1362.38ms, tok/sec: 96208.42, flops:40.62, batch-reuse:1
@ 277 train 5.5711 , allloss: 5.5711, norm:0.3803, dt: 1362.06ms, tok/sec: 96230.55, flops:40.63, batch-reuse:1
@ 278 train 5.7057 , allloss: 5.7057, norm:0.4338, dt: 1361.12ms, tok/sec: 96297.29, flops:40.66, batch-reuse:1
@ 279 train 5.6343 , allloss: 5.6343, norm:0.4056, dt: 1362.00ms, tok/sec: 96234.83, flops:40.63, batch-reuse:1
@ 280 train 5.7628 , allloss: 5.7628, norm:0.4199, dt: 1362.22ms, tok/sec: 96219.29, flops:40.63, batch-reuse:1
@ 281 train 5.6764 , allloss: 5.6764, norm:0.3395, dt: 1362.33ms, tok/sec: 96211.51, flops:40.62, batch-reuse:1
@ 282 train 5.6424 , allloss: 5.6424, norm:0.3757, dt: 1362.29ms, tok/sec: 96214.60, flops:40.63, batch-reuse:1
@ 283 train 5.6026 , allloss: 5.6026, norm:0.3518, dt: 1362.11ms, tok/sec: 96227.08, flops:40.63, batch-reuse:1
@ 284 train 5.6879 , allloss: 5.6879, norm:0.3810, dt: 1362.41ms, tok/sec: 96206.34, flops:40.62, batch-reuse:1
@ 285 train 5.6919 , allloss: 5.6919, norm:0.3645, dt: 1361.86ms, tok/sec: 96244.89, flops:40.64, batch-reuse:1
@ 286 train 5.6655 , allloss: 5.6655, norm:0.4138, dt: 1361.71ms, tok/sec: 96255.12, flops:40.64, batch-reuse:1
@ 287 train 5.6938 , allloss: 5.6938, norm:0.3571, dt: 1361.80ms, tok/sec: 96249.00, flops:40.64, batch-reuse:1
@ 288 train 5.6198 , allloss: 5.6198, norm:0.4605, dt: 1362.58ms, tok/sec: 96193.95, flops:40.62, batch-reuse:1
@ 289 train 5.6570 , allloss: 5.6570, norm:0.3721, dt: 1362.57ms, tok/sec: 96194.89, flops:40.62, batch-reuse:1
@ 290 train 5.6795 , allloss: 5.6795, norm:0.4062, dt: 1362.42ms, tok/sec: 96205.17, flops:40.62, batch-reuse:1
@ 291 train 5.5881 , allloss: 5.5881, norm:0.3905, dt: 1363.01ms, tok/sec: 96163.69, flops:40.60, batch-reuse:1
@ 292 train 5.5885 , allloss: 5.5885, norm:0.3995, dt: 1362.02ms, tok/sec: 96233.58, flops:40.63, batch-reuse:1
@ 293 train 5.6683 , allloss: 5.6683, norm:0.3956, dt: 1361.54ms, tok/sec: 96267.30, flops:40.65, batch-reuse:1
@ 294 train 5.6671 , allloss: 5.6671, norm:0.4069, dt: 1362.33ms, tok/sec: 96211.52, flops:40.62, batch-reuse:1
@ 295 train 5.6718 , allloss: 5.6718, norm:0.4996, dt: 1361.84ms, tok/sec: 96246.05, flops:40.64, batch-reuse:1
@ 296 train 5.5762 , allloss: 5.5762, norm:0.5965, dt: 1361.80ms, tok/sec: 96248.88, flops:40.64, batch-reuse:1
@ 297 train 5.5459 , allloss: 5.5459, norm:0.4803, dt: 1362.57ms, tok/sec: 96194.67, flops:40.62, batch-reuse:1
@ 298 train 5.6095 , allloss: 5.6095, norm:0.4137, dt: 1362.07ms, tok/sec: 96230.28, flops:40.63, batch-reuse:1
@ 299 train 5.5909 , allloss: 5.5909, norm:0.4646, dt: 1362.86ms, tok/sec: 96174.19, flops:40.61, batch-reuse:1
@ 300 train 5.6218 , allloss: 5.6218, norm:0.4233, dt: 1362.14ms, tok/sec: 96225.13, flops:40.63, batch-reuse:1
@ 301 train 5.7004 , allloss: 5.7004, norm:0.4296, dt: 1361.62ms, tok/sec: 96261.66, flops:40.65, batch-reuse:1
@ 302 train 5.5675 , allloss: 5.5675, norm:0.3604, dt: 1362.02ms, tok/sec: 96233.87, flops:40.63, batch-reuse:1
@ 303 train 5.6004 , allloss: 5.6004, norm:0.4485, dt: 1363.97ms, tok/sec: 96096.05, flops:40.58, batch-reuse:1
@ 304 train 5.5217 , allloss: 5.5217, norm:0.4198, dt: 1362.39ms, tok/sec: 96207.48, flops:40.62, batch-reuse:1
@ 305 train 5.5403 , allloss: 5.5403, norm:0.4388, dt: 1361.25ms, tok/sec: 96288.26, flops:40.66, batch-reuse:1
@ 306 train 5.5624 , allloss: 5.5624, norm:0.4294, dt: 1362.04ms, tok/sec: 96232.04, flops:40.63, batch-reuse:1
@ 307 train 5.6238 , allloss: 5.6238, norm:0.5683, dt: 1362.15ms, tok/sec: 96224.19, flops:40.63, batch-reuse:1
@ 308 train 5.5199 , allloss: 5.5199, norm:0.3760, dt: 1361.88ms, tok/sec: 96243.19, flops:40.64, batch-reuse:1
@ 309 train 5.5786 , allloss: 5.5786, norm:0.4621, dt: 1362.32ms, tok/sec: 96212.50, flops:40.63, batch-reuse:1
@ 310 train 5.6002 , allloss: 5.6002, norm:0.4522, dt: 1361.61ms, tok/sec: 96262.33, flops:40.65, batch-reuse:1
@ 311 train 5.6648 , allloss: 5.6648, norm:0.4121, dt: 1361.90ms, tok/sec: 96241.81, flops:40.64, batch-reuse:1
@ 312 train 5.6718 , allloss: 5.6718, norm:0.4181, dt: 1361.88ms, tok/sec: 96243.52, flops:40.64, batch-reuse:1
@ 313 train 5.5528 , allloss: 5.5528, norm:0.4676, dt: 1363.16ms, tok/sec: 96152.90, flops:40.60, batch-reuse:1
@ 314 train 5.5408 , allloss: 5.5408, norm:0.3913, dt: 1361.27ms, tok/sec: 96286.59, flops:40.66, batch-reuse:1
@ 315 train 5.5293 , allloss: 5.5293, norm:0.4621, dt: 1361.98ms, tok/sec: 96236.35, flops:40.64, batch-reuse:1
