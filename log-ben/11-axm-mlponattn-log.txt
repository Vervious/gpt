Threshold: 0.1
Enable layer loss: False
MAX LEARNING RATE: 0.0006
Experiment name: 11-axm-mlponattn
Experiment description:  Reusing blocks, max LR 6e-4, alllayerloss=False, 
Setting:
========
attn = self.attn(x, x)
mlp = self.mlp(attn)
y = attn*mlp
x = res + y
newres = x
x = RMSNorm(x, ELEMENTWISEAFFINE=False), 
======== 
VALUEMATRIX=False
total desired batch size: 131072
Mini-batch size: 8*1024
=> calculated gradient accumulation steps: 16
=> calculated gradient accumulation steps: 16
Training max steps: 300001Num GPUs: 1num decayed parameter tensors: 10, with 52,396,032 parameters
num non-decayed parameter tensors: 8, with 12,288 parameters
@ 0 train 10.8808 , allloss: 10.8808, norm:10.6103, dt: 2975.89ms, tok/sec: 44044.71, flops:19.08, batch-reuse:1
INFO nextres 3.072664499282837 attn*mlp 3.072024345397949 layernormed 0.9992706179618835
			attn_hist -40.875<tensor([  0.,   0.,   1.,  44., 362., 330.,  31.,   0.,   0.,   0.])>38.4375
			mlp_hist -3.125<tensor([  0.,   0.,   0.,   0., 348., 420.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.453125
			x_hist -4.362297058105469<tensor([  0.,   0.,   0.,   0., 356., 412.,   0.,   0.,   0.,   0.])>5.300344467163086
INFO nextres 10.308874130249023 attn*mlp 9.514823913574219 layernormed 0.9998584389686584
			attn_hist -52.5<tensor([  0.,   0.,   1.,  26., 329., 372.,  33.,   5.,   2.,   0.])>63.75
			mlp_hist -3.359375<tensor([  0.,   0.,   0.,   0., 340., 428.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>2.984375
			x_hist -5.660877227783203<tensor([  0.,   0.,   0.,   0., 357., 411.,   0.,   0.,   0.,   0.])>5.608794212341309
INFO nextres 21.9526424407959 attn*mlp 15.833478927612305 layernormed 1.0001722574234009
			attn_hist -67.875<tensor([  0.,   2.,   8.,  20., 327., 377.,  27.,   5.,   2.,   0.])>67.125
			mlp_hist -3.125<tensor([  0.,   0.,   0.,   0., 332., 436.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.1875
			x_hist -9.218981742858887<tensor([  0.,   0.,   0.,   0., 362., 406.,   0.,   0.,   0.,   0.])>7.415503025054932
INFO nextres 43.481101989746094 attn*mlp 23.473215103149414 layernormed 1.0001146793365479
			attn_hist -111.0<tensor([  0.,   2.,   6.,  12., 341., 377.,  21.,   5.,   1.,   2.])>88.875
			mlp_hist -3.28125<tensor([  0.,   0.,   0.,   0., 348., 420.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.359375
			x_hist -11.115561485290527<tensor([  0.,   0.,   0.,   1., 363., 404.,   0.,   0.,   0.,   0.])>9.582350730895996
INFO nextres 69.65025329589844 attn*mlp 27.286823272705078 layernormed 1.0001647472381592
			attn_hist -133.5<tensor([  0.,   2.,   4.,  11., 346., 378.,  18.,   5.,   1.,   0.])>114.75
			mlp_hist -3.359375<tensor([  0.,   0.,   0.,   0., 360., 408.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.328125
			x_hist -12.347838401794434<tensor([  0.,   0.,   0.,   1., 363., 403.,   1.,   0.,   0.,   0.])>11.046921730041504
INFO nextres 96.78858184814453 attn*mlp 27.977081298828125 layernormed 1.0002018213272095
			attn_hist -148.5<tensor([  0.,   1.,   5.,  10., 347., 384.,  13.,   3.,   2.,   0.])>132.75
			mlp_hist -3.3125<tensor([  0.,   0.,   0.,   0., 356., 412.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.3125
			x_hist -13.22569751739502<tensor([  0.,   0.,   0.,   1., 361., 404.,   2.,   0.,   0.,   0.])>11.905191421508789
INFO nextres 124.12464904785156 attn*mlp 27.996417999267578 layernormed 1.000230312347412
			attn_hist -159.0<tensor([  0.,   1.,   5.,   8., 349., 388.,  10.,   2.,   2.,   0.])>142.5
			mlp_hist -3.21875<tensor([  0.,   0.,   0.,   0., 352., 416.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.296875
			x_hist -13.875301361083984<tensor([  0.,   0.,   0.,   1., 360., 405.,   2.,   0.,   0.,   0.])>12.395962715148926
INFO nextres 151.42071533203125 attn*mlp 27.844844818115234 layernormed 1.000252604484558
			attn_hist -166.5<tensor([  0.,   0.,   4.,  10., 349., 389.,   9.,   2.,   2.,   0.])>148.5
			mlp_hist -3.203125<tensor([  0.,   0.,   0.,   0., 346., 422.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.25
			x_hist -14.365925788879395<tensor([  0.,   0.,   0.,   1., 360., 405.,   2.,   0.,   0.,   0.])>12.676458358764648
INFO nextres 178.60177612304688 attn*mlp 27.65813446044922 layernormed 1.000270128250122
			attn_hist -172.5<tensor([  0.,   0.,   4.,   8., 351., 390.,   9.,   1.,   2.,   0.])>152.25
			mlp_hist -3.1875<tensor([  0.,   0.,   0.,   0., 344., 424.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.21875
			x_hist -14.75924015045166<tensor([  0.,   0.,   0.,   1., 359., 406.,   2.,   0.,   0.,   0.])>12.840910911560059
INFO nextres 205.54855346679688 attn*mlp 27.374988555908203 layernormed 1.0002849102020264
			attn_hist -177.0<tensor([  0.,   0.,   3.,   8., 352., 392.,   7.,   1.,   2.,   0.])>153.75
			mlp_hist -3.15625<tensor([  0.,   0.,   0.,   0., 342., 426.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.234375
			x_hist -15.081544876098633<tensor([  0.,   0.,   0.,   1., 358., 407.,   2.,   0.,   0.,   0.])>12.935409545898438
INFO nextres 232.36395263671875 attn*mlp 27.20994758605957 layernormed 1.0002973079681396
			attn_hist -180.75<tensor([  0.,   0.,   3.,   8., 352., 392.,   6.,   2.,   2.,   0.])>155.25
			mlp_hist -3.140625<tensor([  0.,   0.,   0.,   0., 340., 428.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.265625
			x_hist -15.353646278381348<tensor([  0.,   0.,   0.,   1., 358., 407.,   2.,   0.,   0.,   0.])>12.988122940063477
INFO nextres 259.0087585449219 attn*mlp 27.013874053955078 layernormed 1.0003077983856201
			attn_hist -184.5<tensor([  0.,   0.,   3.,   8., 351., 395.,   4.,   3.,   1.,   0.])>156.0
			mlp_hist -3.109375<tensor([  0.,   0.,   0.,   0., 338., 430.,   0.,   0.,   0.,   0.],
       dtype=torch.bfloat16)>3.28125
			x_hist -15.592596054077148<tensor([  0.,   0.,   0.,   1., 358., 407.,   2.,   0.,   0.,   0.])>13.015462875366211
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( 245:0.0002 245:0.0002 245:0.0002 245:0.0002 245:0.0002IF:0.0002IF:0.0002 commercial:0.0002)
		(A:0.0002A:0.0002A:0.0002A:0.0002A:0.0002A:0.0002A:0.0002 push:0.0002)
		( Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002 fingertips:0.0002Super:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002)
		( fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002)
		( fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002)
 fingertips
------
		( 245:0.0002 245:0.0002 245:0.0002 245:0.0002IF:0.0002IF:0.0002 commercial:0.0002 Sparks:0.0002)
		(A:0.0002A:0.0002A:0.0002A:0.0002A:0.0002A:0.0002 push:0.0002 push:0.0001)
		( Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002 Po:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002Super:0.0002)
		(Super:0.0002Super:0.0002 fingertips:0.0002Super:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002Super:0.0002)
		( fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002)
		( fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002 fingertips:0.0002)
 fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips fingertips
@ 1 train 10.6844 , allloss: 10.6844, norm:7.8303, dt: 2097.47ms, tok/sec: 62490.44, flops:27.07, batch-reuse:1
@ 2 train 10.5562 , allloss: 10.5562, norm:8.0431, dt: 1469.21ms, tok/sec: 89212.39, flops:38.64, batch-reuse:1
@ 3 train 10.4372 , allloss: 10.4372, norm:9.0759, dt: 1351.26ms, tok/sec: 96999.89, flops:42.01, batch-reuse:1
@ 4 train 10.2885 , allloss: 10.2885, norm:6.0424, dt: 1351.76ms, tok/sec: 96964.22, flops:42.00, batch-reuse:1
@ 5 train 10.1504 , allloss: 10.1504, norm:5.7118, dt: 1355.47ms, tok/sec: 96698.29, flops:41.88, batch-reuse:1
@ 6 train 9.9209 , allloss: 9.9209, norm:4.6296, dt: 1356.78ms, tok/sec: 96605.30, flops:41.84, batch-reuse:1
@ 7 train 9.7544 , allloss: 9.7544, norm:4.1324, dt: 1355.40ms, tok/sec: 96703.74, flops:41.89, batch-reuse:1
@ 8 train 9.5454 , allloss: 9.5454, norm:3.9570, dt: 1356.39ms, tok/sec: 96632.82, flops:41.85, batch-reuse:1
@ 9 train 9.2872 , allloss: 9.2872, norm:4.3192, dt: 1356.38ms, tok/sec: 96633.40, flops:41.85, batch-reuse:1
@ 10 train 9.0593 , allloss: 9.0593, norm:3.5698, dt: 1356.96ms, tok/sec: 96592.33, flops:41.84, batch-reuse:1
@ 11 train 8.8436 , allloss: 8.8436, norm:3.0008, dt: 1356.22ms, tok/sec: 96645.17, flops:41.86, batch-reuse:1
@ 12 train 8.5722 , allloss: 8.5722, norm:2.6190, dt: 1355.13ms, tok/sec: 96723.15, flops:41.89, batch-reuse:1
@ 13 train 8.4053 , allloss: 8.4053, norm:2.2453, dt: 1357.87ms, tok/sec: 96527.93, flops:41.81, batch-reuse:1
@ 14 train 8.2197 , allloss: 8.2197, norm:1.9056, dt: 1355.46ms, tok/sec: 96699.28, flops:41.88, batch-reuse:1
@ 15 train 8.0714 , allloss: 8.0714, norm:1.8647, dt: 1355.29ms, tok/sec: 96711.60, flops:41.89, batch-reuse:1
@ 16 train 7.9422 , allloss: 7.9422, norm:1.4141, dt: 1355.63ms, tok/sec: 96687.14, flops:41.88, batch-reuse:1
@ 17 train 7.8639 , allloss: 7.8639, norm:1.2701, dt: 1356.61ms, tok/sec: 96617.37, flops:41.85, batch-reuse:1
@ 18 train 7.8231 , allloss: 7.8231, norm:1.3050, dt: 1356.45ms, tok/sec: 96628.95, flops:41.85, batch-reuse:1
@ 19 train 7.6576 , allloss: 7.6576, norm:0.7141, dt: 1355.64ms, tok/sec: 96686.42, flops:41.88, batch-reuse:1
@ 20 train 7.7407 , allloss: 7.7407, norm:0.8561, dt: 1356.08ms, tok/sec: 96655.06, flops:41.86, batch-reuse:1
@ 21 train 7.6862 , allloss: 7.6862, norm:0.5414, dt: 1355.32ms, tok/sec: 96709.11, flops:41.89, batch-reuse:1
@ 22 train 7.6634 , allloss: 7.6634, norm:0.6614, dt: 1355.60ms, tok/sec: 96689.08, flops:41.88, batch-reuse:1
@ 23 train 7.8640 , allloss: 7.8640, norm:0.5127, dt: 1357.40ms, tok/sec: 96561.06, flops:41.82, batch-reuse:1
@ 24 train 7.8106 , allloss: 7.8106, norm:0.5719, dt: 1355.68ms, tok/sec: 96683.28, flops:41.88, batch-reuse:1
@ 25 train 7.5993 , allloss: 7.5993, norm:0.6179, dt: 1356.92ms, tok/sec: 96595.26, flops:41.84, batch-reuse:1
@ 26 train 7.7299 , allloss: 7.7299, norm:0.4267, dt: 1357.19ms, tok/sec: 96576.00, flops:41.83, batch-reuse:1
@ 27 train 7.6990 , allloss: 7.6990, norm:0.3994, dt: 1357.09ms, tok/sec: 96582.96, flops:41.83, batch-reuse:1
@ 28 train 7.7309 , allloss: 7.7309, norm:0.3987, dt: 1358.28ms, tok/sec: 96498.57, flops:41.80, batch-reuse:1
@ 29 train 7.7863 , allloss: 7.7863, norm:0.3346, dt: 1356.71ms, tok/sec: 96610.32, flops:41.84, batch-reuse:1
@ 30 train 7.6859 , allloss: 7.6859, norm:0.3758, dt: 1357.58ms, tok/sec: 96547.92, flops:41.82, batch-reuse:1
@ 31 train 7.7383 , allloss: 7.7383, norm:0.3218, dt: 1357.42ms, tok/sec: 96559.36, flops:41.82, batch-reuse:1
@ 32 train 7.7199 , allloss: 7.7199, norm:0.3759, dt: 1357.88ms, tok/sec: 96526.74, flops:41.81, batch-reuse:1
@ 33 train 7.6664 , allloss: 7.6664, norm:0.3228, dt: 1357.44ms, tok/sec: 96557.96, flops:41.82, batch-reuse:1
@ 34 train 7.6869 , allloss: 7.6869, norm:0.3280, dt: 1357.40ms, tok/sec: 96560.92, flops:41.82, batch-reuse:1
@ 35 train 7.6428 , allloss: 7.6428, norm:0.4299, dt: 1360.45ms, tok/sec: 96344.25, flops:41.73, batch-reuse:1
@ 36 train 7.6425 , allloss: 7.6425, norm:0.3760, dt: 1360.12ms, tok/sec: 96367.73, flops:41.74, batch-reuse:1
@ 37 train 7.6850 , allloss: 7.6850, norm:0.2862, dt: 1359.64ms, tok/sec: 96401.74, flops:41.75, batch-reuse:1
@ 38 train 7.6019 , allloss: 7.6019, norm:0.5353, dt: 1360.40ms, tok/sec: 96348.15, flops:41.73, batch-reuse:1
@ 39 train 7.6721 , allloss: 7.6721, norm:0.3111, dt: 1359.79ms, tok/sec: 96391.57, flops:41.75, batch-reuse:1
@ 40 train 7.6917 , allloss: 7.6917, norm:0.2743, dt: 1359.03ms, tok/sec: 96445.16, flops:41.77, batch-reuse:1
@ 41 train 7.6665 , allloss: 7.6665, norm:0.3628, dt: 1359.86ms, tok/sec: 96386.31, flops:41.75, batch-reuse:1
@ 42 train 7.6497 , allloss: 7.6497, norm:0.4599, dt: 1360.12ms, tok/sec: 96367.73, flops:41.74, batch-reuse:1
@ 43 train 7.6591 , allloss: 7.6591, norm:0.3042, dt: 1358.93ms, tok/sec: 96452.65, flops:41.78, batch-reuse:1
@ 44 train 7.6711 , allloss: 7.6711, norm:0.3706, dt: 1360.28ms, tok/sec: 96356.80, flops:41.73, batch-reuse:1
@ 45 train 7.5860 , allloss: 7.5860, norm:0.3445, dt: 1360.14ms, tok/sec: 96366.90, flops:41.74, batch-reuse:1
@ 46 train 7.6409 , allloss: 7.6409, norm:0.2938, dt: 1361.29ms, tok/sec: 96285.11, flops:41.70, batch-reuse:1
@ 47 train 7.5893 , allloss: 7.5893, norm:0.4470, dt: 1360.99ms, tok/sec: 96306.65, flops:41.71, batch-reuse:1
@ 48 train 7.5853 , allloss: 7.5853, norm:0.3491, dt: 1361.00ms, tok/sec: 96305.70, flops:41.71, batch-reuse:1
@ 49 train 7.5987 , allloss: 7.5987, norm:0.2628, dt: 1361.36ms, tok/sec: 96280.49, flops:41.70, batch-reuse:1
INFO nextres 291.52850341796875 attn*mlp 291.5202331542969 layernormed 0.999596357345581
			attn_hist -42.5625<tensor([  0.,   0.,   1.,  35., 358., 340.,  34.,   0.,   0.,   0.])>38.625
			mlp_hist -112.0<tensor([21., 33., 85., 81., 92., 95., 83., 70., 69., 76.],
       dtype=torch.bfloat16)>64.0
			x_hist -4.583205699920654<tensor([  0.,   0.,   0.,   0., 374., 394.,   0.,   0.,   0.,   0.])>3.906935453414917
INFO nextres 613.5607299804688 attn*mlp 430.7860107421875 layernormed 1.0002033710479736
			attn_hist -55.125<tensor([  0.,   0.,   8.,  31., 335., 355.,  36.,   3.,   0.,   0.])>46.875
			mlp_hist -129.0<tensor([ 22.,  32.,  69.,  89.,  62.,  81.,  66.,  79.,  80., 111.],
       dtype=torch.bfloat16)>60.0
			x_hist -4.938709735870361<tensor([  0.,   0.,   0.,   0., 376., 392.,   0.,   0.,   0.,   0.])>4.879400730133057
INFO nextres 1088.85400390625 attn*mlp 571.9660034179688 layernormed 1.000348687171936
			attn_hist -59.25<tensor([  0.,   0.,   7.,  29., 340., 358.,  23.,  11.,   0.,   0.])>58.5
			mlp_hist -125.5<tensor([ 21.,  32.,  68.,  96.,  59.,  91.,  73.,  69.,  96., 100.],
       dtype=torch.bfloat16)>60.5
			x_hist -5.147071361541748<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>5.276958465576172
INFO nextres 1636.2796630859375 attn*mlp 589.2619018554688 layernormed 1.000427484512329
			attn_hist -61.875<tensor([  0.,   2.,   5.,  26., 338., 362.,  23.,  11.,   1.,   0.])>63.375
			mlp_hist -131.0<tensor([ 22.,  32.,  70.,  91.,  63.,  83.,  65.,  74.,  83., 110.],
       dtype=torch.bfloat16)>61.25
			x_hist -5.436702728271484<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.488097667694092
INFO nextres 2187.31640625 attn*mlp 566.7158203125 layernormed 1.0004013776779175
			attn_hist -65.25<tensor([  0.,   2.,   6.,  25., 339., 362.,  21.,  11.,   2.,   0.])>66.0
			mlp_hist -127.0<tensor([ 21.,  29.,  71.,  94.,  64.,  83.,  67.,  77.,  87., 104.],
       dtype=torch.bfloat16)>59.5
			x_hist -5.732104301452637<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.632323741912842
INFO nextres 2743.93212890625 attn*mlp 567.67529296875 layernormed 1.0003907680511475
			attn_hist -68.625<tensor([  0.,   2.,   5.,  24., 341., 363.,  20.,  10.,   3.,   0.])>67.5
			mlp_hist -125.5<tensor([ 21.,  29.,  70.,  97.,  64.,  84.,  67.,  78.,  87., 108.],
       dtype=torch.bfloat16)>58.25
			x_hist -5.959486961364746<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.737110137939453
INFO nextres 3299.629150390625 attn*mlp 566.591796875 layernormed 1.0003798007965088
			attn_hist -71.625<tensor([  0.,   2.,   6.,  23., 341., 364.,  19.,  10.,   3.,   0.])>69.0
			mlp_hist -124.0<tensor([ 21.,  30.,  70.,  93.,  66.,  88.,  67.,  78.,  90., 110.],
       dtype=torch.bfloat16)>57.5
			x_hist -6.152460098266602<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.820151329040527
INFO nextres 3853.895751953125 attn*mlp 565.1083984375 layernormed 1.0003684759140015
			attn_hist -73.875<tensor([  0.,   1.,   7.,  22., 342., 364.,  20.,   9.,   3.,   0.])>69.75
			mlp_hist -122.5<tensor([ 21.,  32.,  68.,  93.,  68.,  88.,  67.,  80.,  91., 108.],
       dtype=torch.bfloat16)>57.0
			x_hist -6.314084529876709<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.884824275970459
INFO nextres 4406.345703125 attn*mlp 563.3027954101562 layernormed 1.0003571510314941
			attn_hist -75.75<tensor([  0.,   1.,   7.,  22., 342., 364.,  20.,   9.,   3.,   0.])>70.5
			mlp_hist -121.5<tensor([ 21.,  32.,  67.,  94.,  68.,  88.,  69.,  78.,  96., 104.],
       dtype=torch.bfloat16)>56.75
			x_hist -6.450348377227783<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>5.937305927276611
INFO nextres 4957.3837890625 attn*mlp 561.9053344726562 layernormed 1.0003458261489868
			attn_hist -77.25<tensor([  0.,   1.,   8.,  21., 342., 365.,  19.,   8.,   4.,   0.])>71.25
			mlp_hist -121.0<tensor([ 21.,  33.,  67.,  93.,  69.,  88.,  73.,  74., 101., 101.],
       dtype=torch.bfloat16)>56.25
			x_hist -6.5685625076293945<tensor([  0.,   0.,   0.,   0., 372., 396.,   0.,   0.,   0.,   0.])>6.009251594543457
INFO nextres 5506.98779296875 attn*mlp 560.4708251953125 layernormed 1.0003352165222168
			attn_hist -78.75<tensor([  0.,   1.,   8.,  21., 342., 365.,  19.,   8.,   4.,   0.])>72.0
			mlp_hist -120.0<tensor([ 20.,  34.,  65.,  95.,  69.,  88.,  74.,  76., 101.,  99.],
       dtype=torch.bfloat16)>56.0
			x_hist -6.673660755157471<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>6.080731391906738
INFO nextres 6055.2744140625 attn*mlp 559.195556640625 layernormed 1.0003246068954468
			attn_hist -80.25<tensor([  1.,   0.,   8.,  20., 343., 365.,  19.,   8.,   4.,   0.])>73.125
			mlp_hist -119.5<tensor([ 19.,  35.,  65.,  94.,  71.,  88.,  74.,  76., 102., 103.],
       dtype=torch.bfloat16)>55.75
			x_hist -6.768619537353516<tensor([  0.,   0.,   0.,   0., 371., 397.,   0.,   0.,   0.,   0.])>6.147604465484619
rank 0 sample 0: A Poem for you! Roses are red, Potatoes are 
------
		( Classification:0.0002 are:0.0427 are:0.0471 are:0.0342,:0.0479.:0.0374 are:0.0723 are:0.0723)
		( you:0.0540 are:0.0488 you:0.0645 are:0.0649 are:0.0659 you:0.0500 are:0.1289 are:0.1328)
		( you:0.0801 you:0.0508 you:0.0623,:0.0625,:0.0544 are:0.0464 are:0.0918 are:0.1025)
		( you:0.0605 you:0.0474 you:0.0515,:0.0515,:0.0447,:0.0396 are:0.0630 are:0.0693)
		( you:0.0437 you:0.0381 you:0.0405,:0.0481,:0.0425,:0.0376 are:0.0498 are:0.0554)
		( you:0.0344 you:0.0315 you:0.0337,:0.0430,:0.0378,:0.0354 are:0.0405 are:0.0452)
		(.:0.0298 the:0.0294.:0.0315,:0.0393,:0.0356,:0.0322,:0.0359 are:0.0378)
		(.:0.0302.:0.0291.:0.0311,:0.0369,:0.0332.:0.0320,:0.0337,:0.0344)
		(.:0.0304.:0.0295.:0.0315,:0.0344,:0.0320.:0.0315,:0.0322,:0.0330)
		(.:0.0306.:0.0300.:0.0311,:0.0327.:0.0315.:0.0310,:0.0309,:0.0317)
		(.:0.0300.:0.0295.:0.0305,:0.0312.:0.0309.:0.0304,:0.0295,:0.0303)
		(.:0.0302.:0.0298.:0.0309,:0.0299.:0.0304.:0.0298.:0.0291,:0.0288)
		(.:0.0302.:0.0298.:0.0309,:0.0299.:0.0304.:0.0298.:0.0291,:0.0288)
,
------
		( are:0.0427 are:0.0471 are:0.0342,:0.0479.:0.0374 are:0.0723 are:0.0723 are:0.0500)
		( are:0.0488 you:0.0645 are:0.0649 are:0.0659 you:0.0500 are:0.1289 are:0.1328 are:0.1338)
		( you:0.0508 you:0.0623,:0.0625,:0.0544 are:0.0464 are:0.0918 are:0.1025 are:0.1011)
		( you:0.0474 you:0.0515,:0.0515,:0.0447,:0.0396 are:0.0630 are:0.0693 are:0.0776)
		( you:0.0381 you:0.0405,:0.0481,:0.0425,:0.0376 are:0.0498 are:0.0554,:0.0649)
		( you:0.0315 you:0.0337,:0.0430,:0.0378,:0.0354 are:0.0405 are:0.0452,:0.0569)
		( the:0.0294.:0.0315,:0.0393,:0.0356,:0.0322,:0.0359 are:0.0378,:0.0510)
		(.:0.0291.:0.0311,:0.0369,:0.0332.:0.0320,:0.0337,:0.0344,:0.0466)
		(.:0.0295.:0.0315,:0.0344,:0.0320.:0.0315,:0.0322,:0.0330,:0.0422)
		(.:0.0300.:0.0311,:0.0327.:0.0315.:0.0310,:0.0309,:0.0317,:0.0393)
		(.:0.0295.:0.0305,:0.0312.:0.0309.:0.0304,:0.0295,:0.0303,:0.0378)
		(.:0.0298.:0.0309,:0.0299.:0.0304.:0.0298.:0.0291,:0.0288,:0.0361)
		(.:0.0298.:0.0309,:0.0299.:0.0304.:0.0298.:0.0291,:0.0288,:0.0361)
,,,,,,,,,,,,,,,
@ 50 train 7.6883 , allloss: 7.6883, norm:0.6615, dt: 1944.72ms, tok/sec: 67399.00, flops:29.19, batch-reuse:1
@ 51 train 7.7413 , allloss: 7.7413, norm:0.6694, dt: 1361.56ms, tok/sec: 96266.19, flops:41.70, batch-reuse:1
@ 52 train 7.5805 , allloss: 7.5805, norm:0.2921, dt: 1362.28ms, tok/sec: 96215.04, flops:41.67, batch-reuse:1
@ 53 train 7.5639 , allloss: 7.5639, norm:0.6165, dt: 1361.98ms, tok/sec: 96236.52, flops:41.68, batch-reuse:1
@ 54 train 7.6538 , allloss: 7.6538, norm:0.3470, dt: 1361.20ms, tok/sec: 96291.74, flops:41.71, batch-reuse:1
@ 55 train 7.6710 , allloss: 7.6710, norm:0.4880, dt: 1362.34ms, tok/sec: 96211.24, flops:41.67, batch-reuse:1
@ 56 train 7.5270 , allloss: 7.5270, norm:0.4821, dt: 1361.83ms, tok/sec: 96246.63, flops:41.69, batch-reuse:1
@ 57 train 7.4552 , allloss: 7.4552, norm:0.3471, dt: 1362.43ms, tok/sec: 96204.23, flops:41.67, batch-reuse:1
@ 58 train 7.5217 , allloss: 7.5217, norm:0.5580, dt: 1362.78ms, tok/sec: 96180.06, flops:41.66, batch-reuse:1
@ 59 train 7.3108 , allloss: 7.3108, norm:0.4172, dt: 1361.73ms, tok/sec: 96254.19, flops:41.69, batch-reuse:1
@ 60 train 7.5036 , allloss: 7.5036, norm:0.3669, dt: 1361.95ms, tok/sec: 96238.67, flops:41.68, batch-reuse:1
@ 61 train 7.4470 , allloss: 7.4470, norm:0.4468, dt: 1362.93ms, tok/sec: 96169.03, flops:41.65, batch-reuse:1
@ 62 train 7.3987 , allloss: 7.3987, norm:0.8308, dt: 1362.04ms, tok/sec: 96232.02, flops:41.68, batch-reuse:1
@ 63 train 7.4357 , allloss: 7.4357, norm:0.4342, dt: 1361.61ms, tok/sec: 96262.37, flops:41.69, batch-reuse:1
@ 64 train 7.4229 , allloss: 7.4229, norm:0.7583, dt: 1362.32ms, tok/sec: 96212.62, flops:41.67, batch-reuse:1
@ 65 train 7.4810 , allloss: 7.4810, norm:0.5675, dt: 1361.88ms, tok/sec: 96243.42, flops:41.69, batch-reuse:1
@ 66 train 7.5226 , allloss: 7.5226, norm:0.5535, dt: 1361.71ms, tok/sec: 96255.37, flops:41.69, batch-reuse:1
@ 67 train 7.4027 , allloss: 7.4027, norm:0.5364, dt: 1361.85ms, tok/sec: 96245.80, flops:41.69, batch-reuse:1
@ 68 train 7.4634 , allloss: 7.4634, norm:0.5540, dt: 1362.32ms, tok/sec: 96212.55, flops:41.67, batch-reuse:1
@ 69 train 7.3518 , allloss: 7.3518, norm:0.4633, dt: 1362.46ms, tok/sec: 96202.78, flops:41.67, batch-reuse:1
@ 70 train 7.3786 , allloss: 7.3786, norm:0.6491, dt: 1361.77ms, tok/sec: 96251.12, flops:41.69, batch-reuse:1
@ 71 train 7.3293 , allloss: 7.3293, norm:0.5196, dt: 1361.96ms, tok/sec: 96237.90, flops:41.68, batch-reuse:1
@ 72 train 7.3425 , allloss: 7.3425, norm:0.4139, dt: 1361.76ms, tok/sec: 96251.85, flops:41.69, batch-reuse:1
@ 73 train 7.4023 , allloss: 7.4023, norm:0.6542, dt: 1362.86ms, tok/sec: 96174.51, flops:41.66, batch-reuse:1
@ 74 train 7.3570 , allloss: 7.3570, norm:0.4088, dt: 1362.09ms, tok/sec: 96228.31, flops:41.68, batch-reuse:1
@ 75 train 7.8403 , allloss: 7.8403, norm:0.7149, dt: 1361.48ms, tok/sec: 96271.42, flops:41.70, batch-reuse:1
@ 76 train 7.2853 , allloss: 7.2853, norm:0.5708, dt: 1361.75ms, tok/sec: 96252.57, flops:41.69, batch-reuse:1
@ 77 train 7.8350 , allloss: 7.8350, norm:1.5246, dt: 1360.95ms, tok/sec: 96309.03, flops:41.71, batch-reuse:1
@ 78 train 7.4649 , allloss: 7.4649, norm:1.1544, dt: 1361.00ms, tok/sec: 96305.94, flops:41.71, batch-reuse:1
@ 79 train 7.2461 , allloss: 7.2461, norm:0.4427, dt: 1362.30ms, tok/sec: 96213.75, flops:41.67, batch-reuse:1
@ 80 train 7.2907 , allloss: 7.2907, norm:0.6023, dt: 1362.35ms, tok/sec: 96210.01, flops:41.67, batch-reuse:1
@ 81 train 7.2977 , allloss: 7.2977, norm:0.4667, dt: 1361.78ms, tok/sec: 96250.75, flops:41.69, batch-reuse:1
@ 82 train 7.2516 , allloss: 7.2516, norm:0.7152, dt: 1361.91ms, tok/sec: 96241.62, flops:41.69, batch-reuse:1
@ 83 train 7.2412 , allloss: 7.2412, norm:0.5252, dt: 1361.88ms, tok/sec: 96243.56, flops:41.69, batch-reuse:1
@ 84 train 7.2378 , allloss: 7.2378, norm:0.6380, dt: 1362.11ms, tok/sec: 96227.25, flops:41.68, batch-reuse:1
@ 85 train 7.1889 , allloss: 7.1889, norm:0.3652, dt: 1361.48ms, tok/sec: 96271.76, flops:41.70, batch-reuse:1
@ 86 train 7.1577 , allloss: 7.1577, norm:0.4285, dt: 1365.15ms, tok/sec: 96012.76, flops:41.59, batch-reuse:1
@ 87 train 7.1797 , allloss: 7.1797, norm:0.6183, dt: 1365.82ms, tok/sec: 95965.46, flops:41.57, batch-reuse:1
@ 88 train 7.2064 , allloss: 7.2064, norm:0.5017, dt: 1361.74ms, tok/sec: 96253.10, flops:41.69, batch-reuse:1
@ 89 train 7.1315 , allloss: 7.1315, norm:0.4783, dt: 1361.59ms, tok/sec: 96264.02, flops:41.69, batch-reuse:1
@ 90 train 7.0608 , allloss: 7.0608, norm:0.3796, dt: 1363.43ms, tok/sec: 96133.85, flops:41.64, batch-reuse:1
@ 91 train 7.1176 , allloss: 7.1176, norm:0.4483, dt: 1361.91ms, tok/sec: 96241.57, flops:41.69, batch-reuse:1
@ 92 train 7.0850 , allloss: 7.0850, norm:0.4166, dt: 1364.29ms, tok/sec: 96073.50, flops:41.61, batch-reuse:1
@ 93 train 7.0568 , allloss: 7.0568, norm:0.5385, dt: 1361.92ms, tok/sec: 96240.34, flops:41.68, batch-reuse:1
@ 94 train 7.2681 , allloss: 7.2681, norm:0.8228, dt: 1361.86ms, tok/sec: 96244.86, flops:41.69, batch-reuse:1
@ 95 train 7.0603 , allloss: 7.0603, norm:0.3711, dt: 1361.44ms, tok/sec: 96274.55, flops:41.70, batch-reuse:1
@ 96 train 6.9788 , allloss: 6.9788, norm:0.6758, dt: 1362.15ms, tok/sec: 96224.29, flops:41.68, batch-reuse:1
@ 97 train 6.9996 , allloss: 6.9996, norm:0.4947, dt: 1361.17ms, tok/sec: 96293.37, flops:41.71, batch-reuse:1
@ 98 train 7.0223 , allloss: 7.0223, norm:0.5961, dt: 1361.42ms, tok/sec: 96275.84, flops:41.70, batch-reuse:1
@ 99 train 6.9735 , allloss: 6.9735, norm:0.6325, dt: 1361.75ms, tok/sec: 96252.29, flops:41.69, batch-reuse:1
@ 100 train 6.9464 , allloss: 6.9464, norm:0.5120, dt: 1361.75ms, tok/sec: 96252.42, flops:41.69, batch-reuse:1
@ 101 train 6.8313 , allloss: 6.8313, norm:0.5337, dt: 1361.17ms, tok/sec: 96293.54, flops:41.71, batch-reuse:1
@ 102 train 6.9053 , allloss: 6.9053, norm:0.5341, dt: 1361.76ms, tok/sec: 96252.10, flops:41.69, batch-reuse:1
@ 103 train 6.8721 , allloss: 6.8721, norm:0.4179, dt: 1363.63ms, tok/sec: 96120.03, flops:41.63, batch-reuse:1
@ 104 train 6.9509 , allloss: 6.9509, norm:0.6345, dt: 1361.25ms, tok/sec: 96287.62, flops:41.70, batch-reuse:1
@ 105 train 6.9172 , allloss: 6.9172, norm:0.6287, dt: 1363.82ms, tok/sec: 96106.49, flops:41.63, batch-reuse:1
@ 106 train 6.8522 , allloss: 6.8522, norm:0.5658, dt: 1361.17ms, tok/sec: 96293.68, flops:41.71, batch-reuse:1
@ 107 train 6.8221 , allloss: 6.8221, norm:0.4988, dt: 1361.49ms, tok/sec: 96271.32, flops:41.70, batch-reuse:1
@ 108 train 6.7993 , allloss: 6.7993, norm:0.5308, dt: 1361.80ms, tok/sec: 96248.73, flops:41.69, batch-reuse:1
@ 109 train 6.7585 , allloss: 6.7585, norm:0.5505, dt: 1361.87ms, tok/sec: 96244.37, flops:41.69, batch-reuse:1
@ 110 train 6.7973 , allloss: 6.7973, norm:0.3567, dt: 1361.60ms, tok/sec: 96263.53, flops:41.69, batch-reuse:1
@ 111 train 6.8037 , allloss: 6.8037, norm:0.4912, dt: 1361.49ms, tok/sec: 96271.03, flops:41.70, batch-reuse:1
@ 112 train 6.7999 , allloss: 6.7999, norm:0.4389, dt: 1361.35ms, tok/sec: 96280.81, flops:41.70, batch-reuse:1
@ 113 train 6.7571 , allloss: 6.7571, norm:0.5396, dt: 1361.64ms, tok/sec: 96260.46, flops:41.69, batch-reuse:1
@ 114 train 6.7519 , allloss: 6.7519, norm:0.4140, dt: 1362.56ms, tok/sec: 96195.58, flops:41.67, batch-reuse:1
@ 115 train 6.6798 , allloss: 6.6798, norm:0.5323, dt: 1362.57ms, tok/sec: 96194.97, flops:41.66, batch-reuse:1
@ 116 train 6.6483 , allloss: 6.6483, norm:0.4080, dt: 1361.94ms, tok/sec: 96239.50, flops:41.68, batch-reuse:1
@ 117 train 6.6625 , allloss: 6.6625, norm:0.3621, dt: 1362.06ms, tok/sec: 96230.49, flops:41.68, batch-reuse:1
@ 118 train 6.6651 , allloss: 6.6651, norm:0.3568, dt: 1361.99ms, tok/sec: 96235.88, flops:41.68, batch-reuse:1
@ 119 train 6.5763 , allloss: 6.5763, norm:0.4054, dt: 1362.40ms, tok/sec: 96206.54, flops:41.67, batch-reuse:1
@ 120 train 6.7592 , allloss: 6.7592, norm:0.3992, dt: 1361.81ms, tok/sec: 96248.28, flops:41.69, batch-reuse:1
@ 121 train 6.8014 , allloss: 6.8014, norm:0.3976, dt: 1363.18ms, tok/sec: 96151.58, flops:41.65, batch-reuse:1
@ 122 train 6.7523 , allloss: 6.7523, norm:0.4421, dt: 1364.02ms, tok/sec: 96092.78, flops:41.62, batch-reuse:1
@ 123 train 6.6277 , allloss: 6.6277, norm:0.3583, dt: 1365.32ms, tok/sec: 96000.65, flops:41.58, batch-reuse:1
@ 124 train 6.6614 , allloss: 6.6614, norm:0.3732, dt: 1363.17ms, tok/sec: 96152.12, flops:41.65, batch-reuse:1
@ 125 train 6.6742 , allloss: 6.6742, norm:0.4246, dt: 1363.30ms, tok/sec: 96143.19, flops:41.64, batch-reuse:1
@ 126 train 6.6305 , allloss: 6.6305, norm:0.3623, dt: 1361.59ms, tok/sec: 96264.17, flops:41.69, batch-reuse:1
@ 127 train 6.5897 , allloss: 6.5897, norm:0.4943, dt: 1362.34ms, tok/sec: 96210.95, flops:41.67, batch-reuse:1
@ 128 train 6.5468 , allloss: 6.5468, norm:0.3883, dt: 1362.44ms, tok/sec: 96204.00, flops:41.67, batch-reuse:1
@ 129 train 6.6125 , allloss: 6.6125, norm:0.5929, dt: 1362.35ms, tok/sec: 96209.92, flops:41.67, batch-reuse:1
@ 130 train 6.6420 , allloss: 6.6420, norm:0.4534, dt: 1362.47ms, tok/sec: 96201.74, flops:41.67, batch-reuse:1
@ 131 train 6.6240 , allloss: 6.6240, norm:0.4363, dt: 1363.55ms, tok/sec: 96125.21, flops:41.63, batch-reuse:1
@ 132 train 6.6554 , allloss: 6.6554, norm:0.4688, dt: 1363.31ms, tok/sec: 96142.77, flops:41.64, batch-reuse:1
@ 133 train 6.5587 , allloss: 6.5587, norm:0.4298, dt: 1363.62ms, tok/sec: 96120.58, flops:41.63, batch-reuse:1
@ 134 train 6.6109 , allloss: 6.6109, norm:0.3564, dt: 1363.55ms, tok/sec: 96125.34, flops:41.63, batch-reuse:1
@ 135 train 6.5025 , allloss: 6.5025, norm:0.3855, dt: 1363.63ms, tok/sec: 96119.81, flops:41.63, batch-reuse:1
@ 136 train 6.5208 , allloss: 6.5208, norm:0.4027, dt: 1363.55ms, tok/sec: 96125.86, flops:41.63, batch-reuse:1
@ 137 train 6.4297 , allloss: 6.4297, norm:0.3289, dt: 1362.98ms, tok/sec: 96165.56, flops:41.65, batch-reuse:1
@ 138 train 6.4346 , allloss: 6.4346, norm:0.4305, dt: 1363.22ms, tok/sec: 96148.89, flops:41.64, batch-reuse:1
@ 139 train 6.4131 , allloss: 6.4131, norm:0.4384, dt: 1363.21ms, tok/sec: 96149.25, flops:41.65, batch-reuse:1
@ 140 train 6.4104 , allloss: 6.4104, norm:0.5036, dt: 1363.11ms, tok/sec: 96156.38, flops:41.65, batch-reuse:1
@ 141 train 6.3942 , allloss: 6.3942, norm:0.4768, dt: 1363.33ms, tok/sec: 96141.31, flops:41.64, batch-reuse:1
@ 142 train 6.4915 , allloss: 6.4915, norm:0.3856, dt: 1363.86ms, tok/sec: 96103.44, flops:41.63, batch-reuse:1
@ 143 train 6.4413 , allloss: 6.4413, norm:0.4142, dt: 1363.00ms, tok/sec: 96164.20, flops:41.65, batch-reuse:1
@ 144 train 6.4104 , allloss: 6.4104, norm:0.3338, dt: 1363.68ms, tok/sec: 96116.35, flops:41.63, batch-reuse:1
@ 145 train 6.4100 , allloss: 6.4100, norm:0.3538, dt: 1363.92ms, tok/sec: 96099.19, flops:41.62, batch-reuse:1
@ 146 train 6.5639 , allloss: 6.5639, norm:0.3942, dt: 1364.89ms, tok/sec: 96031.38, flops:41.59, batch-reuse:1
@ 147 train 6.4087 , allloss: 6.4087, norm:0.3888, dt: 1364.64ms, tok/sec: 96049.09, flops:41.60, batch-reuse:1
@ 148 train 6.3815 , allloss: 6.3815, norm:0.3683, dt: 1363.36ms, tok/sec: 96139.21, flops:41.64, batch-reuse:1
@ 149 train 6.2379 , allloss: 6.2379, norm:0.3503, dt: 1363.94ms, tok/sec: 96098.07, flops:41.62, batch-reuse:1
@ 150 train 6.4757 , allloss: 6.4757, norm:0.4618, dt: 1364.37ms, tok/sec: 96067.74, flops:41.61, batch-reuse:1
@ 151 train 6.2441 , allloss: 6.2441, norm:0.3089, dt: 1365.19ms, tok/sec: 96010.11, flops:41.58, batch-reuse:1
@ 152 train 6.2825 , allloss: 6.2825, norm:0.5862, dt: 1364.63ms, tok/sec: 96049.68, flops:41.60, batch-reuse:1
@ 153 train 6.2116 , allloss: 6.2116, norm:0.4702, dt: 1364.01ms, tok/sec: 96093.25, flops:41.62, batch-reuse:1
@ 154 train 6.3356 , allloss: 6.3356, norm:0.3676, dt: 1364.37ms, tok/sec: 96067.98, flops:41.61, batch-reuse:1
@ 155 train 6.4711 , allloss: 6.4711, norm:0.4032, dt: 1364.50ms, tok/sec: 96058.73, flops:41.61, batch-reuse:1
@ 156 train 6.3136 , allloss: 6.3136, norm:0.3160, dt: 1365.05ms, tok/sec: 96019.80, flops:41.59, batch-reuse:1
@ 157 train 6.3897 , allloss: 6.3897, norm:0.4192, dt: 1365.00ms, tok/sec: 96023.74, flops:41.59, batch-reuse:1
@ 158 train 6.3722 , allloss: 6.3722, norm:0.4652, dt: 1364.18ms, tok/sec: 96081.24, flops:41.62, batch-reuse:1
@ 159 train 6.3883 , allloss: 6.3883, norm:0.4570, dt: 1364.50ms, tok/sec: 96058.41, flops:41.61, batch-reuse:1
@ 160 train 6.2992 , allloss: 6.2992, norm:0.3503, dt: 1364.80ms, tok/sec: 96037.60, flops:41.60, batch-reuse:1
@ 161 train 6.2840 , allloss: 6.2840, norm:0.5169, dt: 1363.86ms, tok/sec: 96103.75, flops:41.63, batch-reuse:1
@ 162 train 6.3980 , allloss: 6.3980, norm:0.5325, dt: 1364.43ms, tok/sec: 96063.80, flops:41.61, batch-reuse:1
@ 163 train 6.2937 , allloss: 6.2937, norm:0.4554, dt: 1364.38ms, tok/sec: 96067.20, flops:41.61, batch-reuse:1
@ 164 train 6.2354 , allloss: 6.2354, norm:0.5152, dt: 1364.97ms, tok/sec: 96025.42, flops:41.59, batch-reuse:1
@ 165 train 6.3416 , allloss: 6.3416, norm:0.5653, dt: 1364.56ms, tok/sec: 96054.51, flops:41.60, batch-reuse:1
@ 166 train 6.2989 , allloss: 6.2989, norm:0.5151, dt: 1364.84ms, tok/sec: 96034.38, flops:41.60, batch-reuse:1
@ 167 train 6.3116 , allloss: 6.3116, norm:0.4377, dt: 1365.83ms, tok/sec: 95964.76, flops:41.57, batch-reuse:1
@ 168 train 6.1918 , allloss: 6.1918, norm:0.3320, dt: 1364.85ms, tok/sec: 96034.01, flops:41.60, batch-reuse:1
@ 169 train 6.2578 , allloss: 6.2578, norm:0.4139, dt: 1365.58ms, tok/sec: 95982.85, flops:41.57, batch-reuse:1
@ 170 train 6.2259 , allloss: 6.2259, norm:0.3394, dt: 1364.59ms, tok/sec: 96052.16, flops:41.60, batch-reuse:1
@ 171 train 6.2028 , allloss: 6.2028, norm:0.3822, dt: 1364.68ms, tok/sec: 96045.82, flops:41.60, batch-reuse:1
@ 172 train 6.2372 , allloss: 6.2372, norm:0.3502, dt: 1364.64ms, tok/sec: 96048.47, flops:41.60, batch-reuse:1
@ 173 train 6.2215 , allloss: 6.2215, norm:0.4052, dt: 1363.61ms, tok/sec: 96121.00, flops:41.63, batch-reuse:1
@ 174 train 6.3556 , allloss: 6.3556, norm:0.3580, dt: 1362.82ms, tok/sec: 96177.32, flops:41.66, batch-reuse:1
@ 175 train 6.1430 , allloss: 6.1430, norm:1.2982, dt: 1362.39ms, tok/sec: 96207.46, flops:41.67, batch-reuse:1
@ 176 train 6.2663 , allloss: 6.2663, norm:0.3956, dt: 1362.52ms, tok/sec: 96198.05, flops:41.67, batch-reuse:1
@ 177 train 6.2208 , allloss: 6.2208, norm:0.5272, dt: 1362.78ms, tok/sec: 96180.03, flops:41.66, batch-reuse:1
@ 178 train 6.2667 , allloss: 6.2667, norm:0.4547, dt: 1363.81ms, tok/sec: 96107.07, flops:41.63, batch-reuse:1
@ 179 train 6.1497 , allloss: 6.1497, norm:0.3611, dt: 1362.90ms, tok/sec: 96171.20, flops:41.65, batch-reuse:1
@ 180 train 6.2518 , allloss: 6.2518, norm:0.4679, dt: 1363.57ms, tok/sec: 96123.81, flops:41.63, batch-reuse:1
@ 181 train 6.1249 , allloss: 6.1249, norm:0.4250, dt: 1363.02ms, tok/sec: 96162.65, flops:41.65, batch-reuse:1
@ 182 train 6.2482 , allloss: 6.2482, norm:0.3684, dt: 1363.06ms, tok/sec: 96159.98, flops:41.65, batch-reuse:1
@ 183 train 6.1935 , allloss: 6.1935, norm:0.3612, dt: 1363.04ms, tok/sec: 96161.37, flops:41.65, batch-reuse:1
@ 184 train 6.0906 , allloss: 6.0906, norm:0.4231, dt: 1363.85ms, tok/sec: 96104.70, flops:41.63, batch-reuse:1
@ 185 train 6.1294 , allloss: 6.1294, norm:0.4262, dt: 1362.42ms, tok/sec: 96204.96, flops:41.67, batch-reuse:1
@ 186 train 6.2973 , allloss: 6.2973, norm:0.4354, dt: 1362.32ms, tok/sec: 96212.16, flops:41.67, batch-reuse:1
@ 187 train 6.4094 , allloss: 6.4094, norm:0.5032, dt: 1362.06ms, tok/sec: 96230.70, flops:41.68, batch-reuse:1
@ 188 train 6.2587 , allloss: 6.2587, norm:0.4950, dt: 1362.83ms, tok/sec: 96176.61, flops:41.66, batch-reuse:1
@ 189 train 6.3769 , allloss: 6.3769, norm:0.3437, dt: 1364.45ms, tok/sec: 96062.24, flops:41.61, batch-reuse:1
@ 190 train 6.3442 , allloss: 6.3442, norm:0.5082, dt: 1363.60ms, tok/sec: 96121.81, flops:41.63, batch-reuse:1
@ 191 train 6.3378 , allloss: 6.3378, norm:0.5620, dt: 1363.22ms, tok/sec: 96149.11, flops:41.64, batch-reuse:1
@ 192 train 6.3713 , allloss: 6.3713, norm:0.3741, dt: 1363.18ms, tok/sec: 96151.79, flops:41.65, batch-reuse:1
@ 193 train 6.2550 , allloss: 6.2550, norm:0.6011, dt: 1362.92ms, tok/sec: 96169.98, flops:41.65, batch-reuse:1
@ 194 train 6.3529 , allloss: 6.3529, norm:0.6705, dt: 1362.84ms, tok/sec: 96175.54, flops:41.66, batch-reuse:1
@ 195 train 6.3262 , allloss: 6.3262, norm:0.3280, dt: 1362.88ms, tok/sec: 96172.78, flops:41.66, batch-reuse:1
@ 196 train 6.2422 , allloss: 6.2422, norm:0.5590, dt: 1363.61ms, tok/sec: 96121.52, flops:41.63, batch-reuse:1
@ 197 train 6.2733 , allloss: 6.2733, norm:0.4132, dt: 1363.16ms, tok/sec: 96152.86, flops:41.65, batch-reuse:1
@ 198 train 6.2188 , allloss: 6.2188, norm:0.5057, dt: 1362.83ms, tok/sec: 96176.24, flops:41.66, batch-reuse:1
@ 199 train 6.2869 , allloss: 6.2869, norm:0.4343, dt: 1362.47ms, tok/sec: 96202.01, flops:41.67, batch-reuse:1
@ 200 train 6.3061 , allloss: 6.3061, norm:0.4679, dt: 1363.50ms, tok/sec: 96129.19, flops:41.64, batch-reuse:1
@ 201 train 6.2479 , allloss: 6.2479, norm:0.4254, dt: 1363.63ms, tok/sec: 96119.98, flops:41.63, batch-reuse:1
@ 202 train 6.3606 , allloss: 6.3606, norm:0.4553, dt: 1363.00ms, tok/sec: 96164.25, flops:41.65, batch-reuse:1
@ 203 train 6.2679 , allloss: 6.2679, norm:0.4704, dt: 1362.48ms, tok/sec: 96200.92, flops:41.67, batch-reuse:1
@ 204 train 6.2389 , allloss: 6.2389, norm:0.3566, dt: 1363.05ms, tok/sec: 96160.53, flops:41.65, batch-reuse:1
@ 205 train 6.2459 , allloss: 6.2459, norm:0.3966, dt: 1362.50ms, tok/sec: 96199.47, flops:41.67, batch-reuse:1
@ 206 train 6.3100 , allloss: 6.3100, norm:0.3629, dt: 1363.97ms, tok/sec: 96096.07, flops:41.62, batch-reuse:1
@ 207 train 6.2033 , allloss: 6.2033, norm:0.4720, dt: 1363.14ms, tok/sec: 96154.48, flops:41.65, batch-reuse:1
@ 208 train 6.2494 , allloss: 6.2494, norm:0.3563, dt: 1361.93ms, tok/sec: 96239.60, flops:41.68, batch-reuse:1
@ 209 train 6.3010 , allloss: 6.3010, norm:0.4165, dt: 1363.68ms, tok/sec: 96116.06, flops:41.63, batch-reuse:1
@ 210 train 6.2299 , allloss: 6.2299, norm:0.5212, dt: 1363.75ms, tok/sec: 96111.44, flops:41.63, batch-reuse:1
@ 211 train 6.1925 , allloss: 6.1925, norm:0.3518, dt: 1363.11ms, tok/sec: 96156.91, flops:41.65, batch-reuse:1
@ 212 train 6.2504 , allloss: 6.2504, norm:0.6096, dt: 1364.50ms, tok/sec: 96058.59, flops:41.61, batch-reuse:1
@ 213 train 6.1606 , allloss: 6.1606, norm:0.2987, dt: 1363.77ms, tok/sec: 96110.20, flops:41.63, batch-reuse:1
@ 214 train 6.1838 , allloss: 6.1838, norm:0.5230, dt: 1363.79ms, tok/sec: 96108.97, flops:41.63, batch-reuse:1
@ 215 train 6.1048 , allloss: 6.1048, norm:0.4529, dt: 1363.99ms, tok/sec: 96094.64, flops:41.62, batch-reuse:1
@ 216 train 6.1071 , allloss: 6.1071, norm:0.3407, dt: 1362.08ms, tok/sec: 96229.07, flops:41.68, batch-reuse:1
@ 217 train 6.1900 , allloss: 6.1900, norm:0.4364, dt: 1363.47ms, tok/sec: 96131.24, flops:41.64, batch-reuse:1
@ 218 train 6.1009 , allloss: 6.1009, norm:0.3821, dt: 1363.45ms, tok/sec: 96132.53, flops:41.64, batch-reuse:1
@ 219 train 6.2851 , allloss: 6.2851, norm:0.4114, dt: 1364.49ms, tok/sec: 96059.35, flops:41.61, batch-reuse:1
@ 220 train 6.2104 , allloss: 6.2104, norm:0.3874, dt: 1363.10ms, tok/sec: 96157.54, flops:41.65, batch-reuse:1
@ 221 train 6.1779 , allloss: 6.1779, norm:0.5008, dt: 1363.46ms, tok/sec: 96131.64, flops:41.64, batch-reuse:1
@ 222 train 6.1640 , allloss: 6.1640, norm:0.3731, dt: 1363.51ms, tok/sec: 96128.33, flops:41.64, batch-reuse:1
@ 223 train 6.1683 , allloss: 6.1683, norm:0.4162, dt: 1363.81ms, tok/sec: 96107.34, flops:41.63, batch-reuse:1
@ 224 train 6.0719 , allloss: 6.0719, norm:0.4095, dt: 1362.69ms, tok/sec: 96186.07, flops:41.66, batch-reuse:1
@ 225 train 6.1786 , allloss: 6.1786, norm:0.3441, dt: 1363.42ms, tok/sec: 96135.01, flops:41.64, batch-reuse:1
@ 226 train 6.1916 , allloss: 6.1916, norm:0.3815, dt: 1363.20ms, tok/sec: 96150.29, flops:41.65, batch-reuse:1
@ 227 train 6.1758 , allloss: 6.1758, norm:0.3516, dt: 1363.34ms, tok/sec: 96140.28, flops:41.64, batch-reuse:1
@ 228 train 6.1705 , allloss: 6.1705, norm:0.2826, dt: 1363.81ms, tok/sec: 96107.29, flops:41.63, batch-reuse:1
@ 229 train 6.1690 , allloss: 6.1690, norm:0.3492, dt: 1362.91ms, tok/sec: 96170.78, flops:41.65, batch-reuse:1
@ 230 train 6.1598 , allloss: 6.1598, norm:0.3568, dt: 1363.28ms, tok/sec: 96144.94, flops:41.64, batch-reuse:1
@ 231 train 6.1059 , allloss: 6.1059, norm:0.3313, dt: 1364.21ms, tok/sec: 96078.70, flops:41.61, batch-reuse:1
@ 232 train 6.0571 , allloss: 6.0571, norm:0.4811, dt: 1363.55ms, tok/sec: 96125.78, flops:41.63, batch-reuse:1
@ 233 train 6.1426 , allloss: 6.1426, norm:0.3665, dt: 1364.23ms, tok/sec: 96077.81, flops:41.61, batch-reuse:1
@ 234 train 6.0175 , allloss: 6.0175, norm:0.3190, dt: 1364.50ms, tok/sec: 96058.74, flops:41.61, batch-reuse:1
@ 235 train 6.1100 , allloss: 6.1100, norm:0.4580, dt: 1363.92ms, tok/sec: 96099.51, flops:41.62, batch-reuse:1
@ 236 train 6.0076 , allloss: 6.0076, norm:0.3367, dt: 1364.93ms, tok/sec: 96028.16, flops:41.59, batch-reuse:1
@ 237 train 6.0968 , allloss: 6.0968, norm:0.3413, dt: 1364.25ms, tok/sec: 96075.92, flops:41.61, batch-reuse:1
@ 238 train 6.0481 , allloss: 6.0481, norm:0.3037, dt: 1363.15ms, tok/sec: 96153.94, flops:41.65, batch-reuse:1
@ 239 train 6.0546 , allloss: 6.0546, norm:0.3417, dt: 1362.84ms, tok/sec: 96175.77, flops:41.66, batch-reuse:1
@ 240 train 6.1138 , allloss: 6.1138, norm:0.4810, dt: 1363.26ms, tok/sec: 96145.82, flops:41.64, batch-reuse:1
@ 241 train 6.1087 , allloss: 6.1087, norm:0.4525, dt: 1364.24ms, tok/sec: 96076.99, flops:41.61, batch-reuse:1
@ 242 train 6.0791 , allloss: 6.0791, norm:0.4111, dt: 1363.48ms, tok/sec: 96130.23, flops:41.64, batch-reuse:1
@ 243 train 5.9922 , allloss: 5.9922, norm:0.4028, dt: 1362.13ms, tok/sec: 96225.69, flops:41.68, batch-reuse:1
@ 244 train 6.1063 , allloss: 6.1063, norm:0.4698, dt: 1363.11ms, tok/sec: 96156.66, flops:41.65, batch-reuse:1
@ 245 train 5.8630 , allloss: 5.8630, norm:0.5640, dt: 1364.08ms, tok/sec: 96088.38, flops:41.62, batch-reuse:1
@ 246 train 6.1530 , allloss: 6.1530, norm:0.3435, dt: 1363.53ms, tok/sec: 96127.26, flops:41.64, batch-reuse:1
@ 247 train 6.0611 , allloss: 6.0611, norm:0.4188, dt: 1363.30ms, tok/sec: 96142.94, flops:41.64, batch-reuse:1
@ 248 train 6.0084 , allloss: 6.0084, norm:0.3916, dt: 1364.04ms, tok/sec: 96090.88, flops:41.62, batch-reuse:1
@ 249 train 5.9785 , allloss: 5.9785, norm:0.3967, dt: 1363.72ms, tok/sec: 96113.53, flops:41.63, batch-reuse:1
@ 250 train 6.0533 , allloss: 6.0533, norm:0.3563, dt: 1364.28ms, tok/sec: 96073.97, flops:41.61, batch-reuse:1
@ 251 train 6.0042 , allloss: 6.0042, norm:0.4060, dt: 1363.87ms, tok/sec: 96103.28, flops:41.63, batch-reuse:1
@ 252 train 6.1194 , allloss: 6.1194, norm:0.5097, dt: 1363.61ms, tok/sec: 96121.02, flops:41.63, batch-reuse:1
@ 253 train 6.0143 , allloss: 6.0143, norm:0.4113, dt: 1363.49ms, tok/sec: 96129.98, flops:41.64, batch-reuse:1
@ 254 train 5.9593 , allloss: 5.9593, norm:0.4985, dt: 1362.92ms, tok/sec: 96169.82, flops:41.65, batch-reuse:1
@ 255 train 5.9504 , allloss: 5.9504, norm:0.3832, dt: 1364.32ms, tok/sec: 96071.07, flops:41.61, batch-reuse:1
@ 256 train 6.0122 , allloss: 6.0122, norm:0.4803, dt: 1363.65ms, tok/sec: 96118.82, flops:41.63, batch-reuse:1
@ 257 train 6.0071 , allloss: 6.0071, norm:0.4681, dt: 1363.93ms, tok/sec: 96098.49, flops:41.62, batch-reuse:1
@ 258 train 5.9563 , allloss: 5.9563, norm:0.5203, dt: 1363.46ms, tok/sec: 96131.88, flops:41.64, batch-reuse:1
@ 259 train 5.9767 , allloss: 5.9767, norm:0.3705, dt: 1363.18ms, tok/sec: 96151.42, flops:41.65, batch-reuse:1
@ 260 train 6.0374 , allloss: 6.0374, norm:0.4607, dt: 1363.77ms, tok/sec: 96109.85, flops:41.63, batch-reuse:1
@ 261 train 6.0174 , allloss: 6.0174, norm:0.3867, dt: 1362.98ms, tok/sec: 96165.91, flops:41.65, batch-reuse:1
@ 262 train 6.1143 , allloss: 6.1143, norm:0.4495, dt: 1364.13ms, tok/sec: 96084.57, flops:41.62, batch-reuse:1
@ 263 train 6.0345 , allloss: 6.0345, norm:0.5144, dt: 1364.55ms, tok/sec: 96054.88, flops:41.60, batch-reuse:1
