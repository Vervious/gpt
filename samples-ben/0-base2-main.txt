1: sample 0: Hello, I'm a language model,
------
		(Hello:0.0003,:0.0008 I:0.0004'm:0.0003 a:0.0002 language:0.0003 model:0.0003,:0.0009)
		( program:0.0002,:0.0005,:0.0002,:0.0002,:0.0002pps:0.0002,:0.0002,:0.0006)
		( program:0.0002,:0.0004,:0.0003,:0.0003,:0.0002,:0.0002,:0.0003,:0.0005)
		(,:0.0002,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0005)
		(,:0.0002,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0002,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004)
		(,:0.0003,:0.0004,:0.0004,:0.0004,:0.0003,:0.0004,:0.0003,:0.0004)
,
------
		(,:0.0008 I:0.0004'm:0.0003 a:0.0002 language:0.0003 model:0.0003,:0.0009,:0.0013)
		(,:0.0005,:0.0002,:0.0002,:0.0002pps:0.0002,:0.0002,:0.0006,:0.0006)
		(,:0.0004,:0.0003,:0.0003,:0.0002,:0.0002,:0.0003,:0.0005,:0.0005)
		(,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0005,:0.0005)
		(,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0004,:0.0003,:0.0003,:0.0003,:0.0004,:0.0004)
		(,:0.0004,:0.0004,:0.0004,:0.0003,:0.0004,:0.0003,:0.0004,:0.0004)
,,,,,,,,,,,
50: sample 0: Hello, I'm a language model,
------
		( the:0.0143 the:0.0144.:0.2422.:0.2217 the:0.0144.:0.2207.:0.2070 the:0.0107)
		( the:0.0214 the:0.0184,:0.2393,:0.2432 the:0.0179,:0.2354,:0.2354 the:0.0179)
		( the:0.0223 the:0.0204,:0.1904,:0.1924 the:0.0206,:0.1934,:0.1982 the:0.0214)
		( the:0.0222 the:0.0208,:0.1543,:0.1504 the:0.0217,:0.1494,:0.1553 the:0.0225)
		( the:0.0227 the:0.0212,:0.1216,:0.1270 the:0.0222,:0.1270,:0.1309 the:0.0229)
		( the:0.0227 the:0.0219,:0.1045,:0.1094 the:0.0227,:0.1089,:0.1138 the:0.0234)
		( the:0.0226 the:0.0217,:0.0962,:0.0952 the:0.0226,:0.0952,:0.0996 the:0.0233)
		(,:0.0226 the:0.0216,:0.0864,:0.0854 the:0.0232,:0.0879,:0.0869 the:0.0233)
		(,:0.0226 the:0.0216,:0.0781,:0.0801 the:0.0232,:0.0801,:0.0815 the:0.0239)
		(,:0.0232 the:0.0222,:0.0723,:0.0737 the:0.0231,:0.0737,:0.0752 the:0.0238)
		(,:0.0232 the:0.0221,:0.0684,:0.0698 the:0.0231,:0.0698,:0.0693 the:0.0238)
		(,:0.0232 the:0.0221,:0.0640,:0.0654 the:0.0231,:0.0654,:0.0649 the:0.0237)
 the
------
		( the:0.0144.:0.2422.:0.2217 the:0.0144.:0.2207.:0.2070 the:0.0107 the:0.0117)
		( the:0.0184,:0.2393,:0.2432 the:0.0179,:0.2354,:0.2354 the:0.0179 the:0.0179)
		( the:0.0204,:0.1904,:0.1924 the:0.0206,:0.1934,:0.1982 the:0.0214 the:0.0208)
		( the:0.0208,:0.1543,:0.1504 the:0.0217,:0.1494,:0.1553 the:0.0225 the:0.0225)
		( the:0.0212,:0.1216,:0.1270 the:0.0222,:0.1270,:0.1309 the:0.0229 the:0.0229)
		( the:0.0219,:0.1045,:0.1094 the:0.0227,:0.1089,:0.1138 the:0.0234 the:0.0228)
		( the:0.0217,:0.0962,:0.0952 the:0.0226,:0.0952,:0.0996 the:0.0233 the:0.0233)
		( the:0.0216,:0.0864,:0.0854 the:0.0232,:0.0879,:0.0869 the:0.0233 the:0.0233)
		( the:0.0216,:0.0781,:0.0801 the:0.0232,:0.0801,:0.0815 the:0.0239 the:0.0232)
		( the:0.0222,:0.0723,:0.0737 the:0.0231,:0.0737,:0.0752 the:0.0238 the:0.0231)
		( the:0.0221,:0.0684,:0.0698 the:0.0231,:0.0698,:0.0693 the:0.0238 the:0.0238)
		( the:0.0221,:0.0640,:0.0654 the:0.0231,:0.0654,:0.0649 the:0.0237 the:0.0238)
 the the the the the the the the the the the
200: sample 0: Hello, I'm a language model,
------
		(-:0.0292The:0.0315 have:0.1562 them:0.1396-:0.0237�:0.5234�:0.6875 including:0.0811)
		(-:0.0312The:0.0287 have:0.2168 to:0.1787 years:0.0210 of:0.8008 of:0.7891 they:0.0483)
		(-:0.0471The:0.0227 have:0.2432 to:0.1416 years:0.0160 of:0.5625 of:0.7500 and:0.0474)
		(-:0.0510The:0.0179 have:0.2598 the:0.1436 years:0.0123 of:0.3613 of:0.5742 and:0.0562)
		(-:0.0481The:0.0148 have:0.2578 the:0.1787 years:0.0103 of:0.2617 of:0.4102 and:0.0613)
		(-:0.0422-:0.0134 have:0.2539 the:0.1992 years:0.0087 of:0.1865 of:0.3047 and:0.0635)
		(-:0.0374-:0.0152 have:0.2539 the:0.2109 years:0.0075,:0.1592 of:0.2324 and:0.0645)
		(-:0.0330
:0.0179 have:0.2412 the:0.2119 year:0.0066,:0.1475 of:0.1836 and:0.0635)
		(-:0.0308
:0.0201 have:0.2363 the:0.2168 year:0.0063,:0.1289.:0.1611 and:0.0620)
		(-:0.0278
:0.0223 have:0.2256 the:0.2061 year:0.0061,:0.1221.:0.1436 and:0.0608)
		( and:0.0286
:0.0243 have:0.2217 the:0.2021 time:0.0061,:0.1118,:0.1289 and:0.0588)
		( and:0.0293
:0.0262 have:0.2139 the:0.1953 time:0.0060,:0.1035,:0.1196 and:0.0576)
 and
------
		(The:0.0315 have:0.1562 them:0.1396-:0.0237�:0.5234�:0.6875 including:0.0811000:0.0649)
		(The:0.0287 have:0.2168 to:0.1787 years:0.0210 of:0.8008 of:0.7891 they:0.0483 they:0.0540)
		(The:0.0227 have:0.2432 to:0.1416 years:0.0160 of:0.5625 of:0.7500 and:0.0474 they:0.0364)
		(The:0.0179 have:0.2598 the:0.1436 years:0.0123 of:0.3613 of:0.5742 and:0.0562 and:0.0337)
		(The:0.0148 have:0.2578 the:0.1787 years:0.0103 of:0.2617 of:0.4102 and:0.0613 the:0.0337)
		(-:0.0134 have:0.2539 the:0.1992 years:0.0087 of:0.1865 of:0.3047 and:0.0635 the:0.0349)
		(-:0.0152 have:0.2539 the:0.2109 years:0.0075,:0.1592 of:0.2324 and:0.0645 the:0.0356)
		(
:0.0179 have:0.2412 the:0.2119 year:0.0066,:0.1475 of:0.1836 and:0.0635 the:0.0376)
		(
:0.0201 have:0.2363 the:0.2168 year:0.0063,:0.1289.:0.1611 and:0.0620 the:0.0383)
		(
:0.0223 have:0.2256 the:0.2061 year:0.0061,:0.1221.:0.1436 and:0.0608 the:0.0398)
		(
:0.0243 have:0.2217 the:0.2021 time:0.0061,:0.1118,:0.1289 and:0.0588 the:0.0400)
		(
:0.0262 have:0.2139 the:0.1953 time:0.0060,:0.1035,:0.1196 and:0.0576 the:0.0415)
 the U.
- States.
- States.
350: sample 0: Hello, I'm a language model,
------
		( as:0.2109 acid:0.0264�:0.3262 the:0.0732 century:0.3613�:0.1855�:0.5742 who:0.5938)
		( as:0.1299�:0.0078�:0.1572 the:0.1299 century:0.0918.:0.1865�:0.1523 who:0.2539)
		(�:0.1216 the:0.0125�:0.0840 the:0.1543 century:0.0242.:0.1914.:0.1807 who:0.1094)
		(�:0.1001 the:0.0188�:0.0859 the:0.1592 way:0.0103.:0.1729.:0.1699 the:0.0583)
		(�:0.0820 the:0.0255�:0.1040 the:0.1592 way:0.0126.:0.1523.:0.1602 the:0.0703)
		(�:0.0669 the:0.0306�:0.1138 the:0.1533 way:0.0144.:0.1348.:0.1426 the:0.0767)
		(�:0.0566 the:0.0361�:0.1289 the:0.1504 way:0.0151,:0.1167,:0.1318 the:0.0830)
		(�:0.0491 the:0.0408�:0.1426 the:0.1377 way:0.0154,:0.1143,:0.1309 the:0.0854)
		(�:0.0422 the:0.0442�:0.1484 the:0.1377 way:0.0153,:0.1084,:0.1309 the:0.0864)
		(,:0.0383 the:0.0479�:0.1523 the:0.1299 way:0.0150,:0.1079,:0.1279 the:0.0889)
		(,:0.0408 the:0.0513�:0.1553 the:0.1206 way:0.0142,:0.1035,:0.1230 the:0.0884)
		(,:0.0420 the:0.0532�:0.1572 the:0.1182 way:0.0135,:0.1006,:0.1230 the:0.0903)
 the
------
		( acid:0.0264�:0.3262 the:0.0732 century:0.3613�:0.1855�:0.5742 who:0.5938 century:0.0574)
		(�:0.0078�:0.1572 the:0.1299 century:0.0918.:0.1865�:0.1523 who:0.2539 same:0.0391)
		( the:0.0125�:0.0840 the:0.1543 century:0.0242.:0.1914.:0.1807 who:0.1094 same:0.0320)
		( the:0.0188�:0.0859 the:0.1592 way:0.0103.:0.1729.:0.1699 the:0.0583 same:0.0272)
		( the:0.0255�:0.1040 the:0.1592 way:0.0126.:0.1523.:0.1602 the:0.0703 same:0.0232)
		( the:0.0306�:0.1138 the:0.1533 way:0.0144.:0.1348.:0.1426 the:0.0767 same:0.0199)
		( the:0.0361�:0.1289 the:0.1504 way:0.0151,:0.1167,:0.1318 the:0.0830 same:0.0175)
		( the:0.0408�:0.1426 the:0.1377 way:0.0154,:0.1143,:0.1309 the:0.0854 same:0.0152)
		( the:0.0442�:0.1484 the:0.1377 way:0.0153,:0.1084,:0.1309 the:0.0864 most:0.0149)
		( the:0.0479�:0.1523 the:0.1299 way:0.0150,:0.1079,:0.1279 the:0.0889 most:0.0156)
		( the:0.0513�:0.1553 the:0.1206 way:0.0142,:0.1035,:0.1230 the:0.0884 most:0.0161)
		( the:0.0532�:0.1572 the:0.1182 way:0.0135,:0.1006,:0.1230 the:0.0903 most:0.0162)
 most of the most of the most common to the most
500: sample 0: Hello, I'm a language model,
------
		(.:0.0776 who:0.0245 don:0.0591 over:0.0244 century:0.0195.:0.1270 between:0.2197 who:0.2139)
		(.:0.0835 the:0.0155�:0.0649 over:0.0231 world:0.0128.:0.1621 for:0.1396 who:0.1069)
		(.:0.0894 the:0.0248�:0.1006 used:0.0236 world:0.0112.:0.1758 to:0.1572 who:0.0608)
		(.:0.0859 the:0.0332�:0.1299 a:0.0238 world:0.0091.:0.1699 to:0.1895 who:0.0410)
		(.:0.0830 the:0.0405�:0.1465 a:0.0282 way:0.0092.:0.1650 to:0.2070 and:0.0430)
		(.:0.0801 the:0.0466�:0.1582 a:0.0305 �:0.0109.:0.1631 to:0.2148 and:0.0457)
		(,:0.0752 the:0.0500�:0.1660 a:0.0317 �:0.0140.:0.1631 to:0.2178 and:0.0461)
		(,:0.0732 the:0.0549�:0.1641 a:0.0332 �:0.0170.:0.1670 to:0.2236 and:0.0464)
		(,:0.0703 the:0.0566�:0.1582 a:0.0339 �:0.0193.:0.1670 to:0.2236 and:0.0454)
		(,:0.0684 the:0.0598�:0.1602 a:0.0342 �:0.0212.:0.1582 to:0.2207 and:0.0442)
		(�:0.0684 the:0.0613�:0.1543 a:0.0339 �:0.0222.:0.1562 to:0.2139 and:0.0432)
		(�:0.0674 the:0.0625�:0.1475 a:0.0337 �:0.0239.:0.1543 to:0.2041 and:0.0422)
 and
------
		( who:0.0245 don:0.0591 over:0.0244 century:0.0195.:0.1270 between:0.2197 who:0.2139 said:0.0547)
		( the:0.0155�:0.0649 over:0.0231 world:0.0128.:0.1621 for:0.1396 who:0.1069 said:0.0315)
		( the:0.0248�:0.1006 used:0.0236 world:0.0112.:0.1758 to:0.1572 who:0.0608 even:0.0220)
		( the:0.0332�:0.1299 a:0.0238 world:0.0091.:0.1699 to:0.1895 who:0.0410 even:0.0216)
		( the:0.0405�:0.1465 a:0.0282 way:0.0092.:0.1650 to:0.2070 and:0.0430 even:0.0208)
		( the:0.0466�:0.1582 a:0.0305 �:0.0109.:0.1631 to:0.2148 and:0.0457 the:0.0220)
		( the:0.0500�:0.1660 a:0.0317 �:0.0140.:0.1631 to:0.2178 and:0.0461 the:0.0229)
		( the:0.0549�:0.1641 a:0.0332 �:0.0170.:0.1670 to:0.2236 and:0.0464 the:0.0237)
		( the:0.0566�:0.1582 a:0.0339 �:0.0193.:0.1670 to:0.2236 and:0.0454 the:0.0237)
		( the:0.0598�:0.1602 a:0.0342 �:0.0212.:0.1582 to:0.2207 and:0.0442 the:0.0237)
		( the:0.0613�:0.1543 a:0.0339 �:0.0222.:0.1562 to:0.2139 and:0.0432 the:0.0238)
		( the:0.0625�:0.1475 a:0.0337 �:0.0239.:0.1543 to:0.2041 and:0.0422 the:0.0231)
 the same time.
The “The “
650: sample 0: Hello, I'm a language model,
------
		(,:0.1230 the:0.0128 we:0.0615 all:0.0099 course:0.0195,:0.2852,:0.0811�:0.0645)
		(,:0.1128 the:0.0255�:0.0535 a:0.0205 matter:0.0162,:0.2891,:0.1133 and:0.0742)
		(,:0.1011 the:0.0378�:0.0610 the:0.0322 matter:0.0145,:0.2793,:0.1270 and:0.0898)
		(,:0.0908 the:0.0479�:0.0645 the:0.0408 matter:0.0121,:0.2676,:0.1270 and:0.0981)
		(,:0.0830 the:0.0576�:0.0669 the:0.0461 matter:0.0099,:0.2559,:0.1279 and:0.1045)
		(,:0.0737 the:0.0645�:0.0669 the:0.0503 matter:0.0081,:0.2500,:0.1235 and:0.1099)
		(,:0.0679 the:0.0713�:0.0635 the:0.0520 good:0.0084,:0.2383,:0.1245 and:0.1108)
		(,:0.0618 the:0.0767�:0.0615 the:0.0513 good:0.0102,:0.2354,:0.1172 and:0.1084)
		(,:0.0554 the:0.0815�:0.0588 the:0.0510 good:0.0121,:0.2197,:0.1162 and:0.1060)
		(,:0.0503 the:0.0859�:0.0557 the:0.0491 good:0.0136,:0.2139,:0.1143 and:0.1035)
		(,:0.0454 the:0.0908�:0.0535 the:0.0479 good:0.0159,:0.2070,:0.1055 and:0.0981)
		(,:0.0417 the:0.0928�:0.0510 the:0.0449 good:0.0173,:0.1982,:0.1025 and:0.0957)
 and
------
		( the:0.0128 we:0.0615 all:0.0099 course:0.0195,:0.2852,:0.0811�:0.0645 the:0.0288)
		( the:0.0255�:0.0535 a:0.0205 matter:0.0162,:0.2891,:0.1133 and:0.0742 the:0.0532)
		( the:0.0378�:0.0610 the:0.0322 matter:0.0145,:0.2793,:0.1270 and:0.0898 the:0.0679)
		( the:0.0479�:0.0645 the:0.0408 matter:0.0121,:0.2676,:0.1270 and:0.0981 the:0.0747)
		( the:0.0576�:0.0669 the:0.0461 matter:0.0099,:0.2559,:0.1279 and:0.1045 the:0.0786)
		( the:0.0645�:0.0669 the:0.0503 matter:0.0081,:0.2500,:0.1235 and:0.1099 the:0.0791)
		( the:0.0713�:0.0635 the:0.0520 good:0.0084,:0.2383,:0.1245 and:0.1108 the:0.0796)
		( the:0.0767�:0.0615 the:0.0513 good:0.0102,:0.2354,:0.1172 and:0.1084 the:0.0771)
		( the:0.0815�:0.0588 the:0.0510 good:0.0121,:0.2197,:0.1162 and:0.1060 the:0.0752)
		( the:0.0859�:0.0557 the:0.0491 good:0.0136,:0.2139,:0.1143 and:0.1035 the:0.0708)
		( the:0.0908�:0.0535 the:0.0479 good:0.0159,:0.2070,:0.1055 and:0.0981 the:0.0669)
		( the:0.0928�:0.0510 the:0.0449 good:0.0173,:0.1982,:0.1025 and:0.0957 the:0.0654)
 the same, and the same, and the same,
800: sample 0: Hello, I'm a language model,
------
		( to:0.0566 and:0.0166 they:0.0442as:0.0063 matter:0.0208�:0.1250�:0.1416�:0.0454)
		( to:0.0566 the:0.0244�:0.0396 not:0.0112 matter:0.0178,:0.1216 of:0.1377 and:0.0520)
		(.:0.0503 the:0.0325�:0.0449 not:0.0173 matter:0.0147,:0.1270 of:0.1680 and:0.0620)
		(.:0.0564 the:0.0388�:0.0515 not:0.0215 matter:0.0116,:0.1348 of:0.1836 and:0.0703)
		(.:0.0620 the:0.0447�:0.0566 the:0.0251 way:0.0090,:0.1328 of:0.1865 and:0.0747)
		(.:0.0649 the:0.0500�:0.0605 the:0.0289 way:0.0098,:0.1357 of:0.1875 and:0.0767)
		(.:0.0679 the:0.0530�:0.0615 the:0.0330 way:0.0103,:0.1367 of:0.1777 and:0.0781)
		(.:0.0708 the:0.0557�:0.0640 the:0.0374 way:0.0108,:0.1357 of:0.1650 and:0.0752)
		(.:0.0728 the:0.0586�:0.0654 the:0.0393 way:0.0111,:0.1348 of:0.1533 and:0.0737)
		(.:0.0723 the:0.0613�:0.0669 the:0.0422 way:0.0110,:0.1309 of:0.1406 and:0.0708)
		(.:0.0742 the:0.0618�:0.0679 the:0.0422 way:0.0110,:0.1338 of:0.1279 I:0.0698)
		(.:0.0747 and:0.0659�:0.0703 the:0.0432 good:0.0110,:0.1348.:0.1221 I:0.0908)
 I
------
		( and:0.0166 they:0.0442as:0.0063 matter:0.0208�:0.1250�:0.1416�:0.0454 they:0.0432)
		( the:0.0244�:0.0396 not:0.0112 matter:0.0178,:0.1216 of:0.1377 and:0.0520�:0.0366)
		( the:0.0325�:0.0449 not:0.0173 matter:0.0147,:0.1270 of:0.1680 and:0.0620�:0.0391)
		( the:0.0388�:0.0515 not:0.0215 matter:0.0116,:0.1348 of:0.1836 and:0.0703�:0.0415)
		( the:0.0447�:0.0566 the:0.0251 way:0.0090,:0.1328 of:0.1865 and:0.0747 have:0.0435)
		( the:0.0500�:0.0605 the:0.0289 way:0.0098,:0.1357 of:0.1875 and:0.0767 have:0.0469)
		( the:0.0530�:0.0615 the:0.0330 way:0.0103,:0.1367 of:0.1777 and:0.0781 have:0.0513)
		( the:0.0557�:0.0640 the:0.0374 way:0.0108,:0.1357 of:0.1650 and:0.0752 have:0.0549)
		( the:0.0586�:0.0654 the:0.0393 way:0.0111,:0.1348 of:0.1533 and:0.0737 have:0.0588)
		( the:0.0613�:0.0669 the:0.0422 way:0.0110,:0.1309 of:0.1406 and:0.0708 have:0.0605)
		( the:0.0618�:0.0679 the:0.0422 way:0.0110,:0.1338 of:0.1279 I:0.0698 have:0.0640)
		( and:0.0659�:0.0703 the:0.0432 good:0.0110,:0.1348.:0.1221 I:0.0908 have:0.0649)
 have a way to the same time.
The first
950: sample 0: Hello, I'm a language model,
------
		(
:0.0393 about:0.0107�:0.0245 a:0.0151 matter:0.0113�:0.1328 of:0.1357�:0.0688)
		( to:0.0447 the:0.0153 have:0.0262 a:0.0177 matter:0.0103�:0.0549 of:0.2168 and:0.0515)
		( to:0.0486 the:0.0229 have:0.0327 not:0.0234 matter:0.0076,:0.0703 of:0.2373 and:0.0486)
		( to:0.0466 the:0.0302 have:0.0386 not:0.0282 new:0.0055,:0.0796 of:0.2295 and:0.0459)
		( to:0.0422 the:0.0376 have:0.0422 the:0.0332 new:0.0065.:0.0845 of:0.2295 and:0.0435)
		(,:0.0393 the:0.0452 have:0.0466 the:0.0396 new:0.0079.:0.0903 of:0.2363 and:0.0420)
		(,:0.0427 the:0.0540 have:0.0500 the:0.0437 new:0.0095.:0.0928 of:0.2393 and:0.0437)
		(,:0.0464 the:0.0640 have:0.0547 the:0.0493 new:0.0110.:0.0942 of:0.2383 and:0.0488)
		(,:0.0486 the:0.0728 have:0.0591 the:0.0515 new:0.0131.:0.0967 of:0.2354 and:0.0525)
		(,:0.0508 the:0.0796 have:0.0640 the:0.0530 new:0.0150.:0.0986 of:0.2295 and:0.0596)
		(,:0.0530 the:0.0884 have:0.0684 the:0.0554 new:0.0166.:0.0986 of:0.2295 and:0.0664)
		(,:0.0535 the:0.0913 have:0.0781 the:0.0540 new:0.0183.:0.1060 of:0.2236 and:0.0757)
 and
------
		( about:0.0107�:0.0245 a:0.0151 matter:0.0113�:0.1328 of:0.1357�:0.0688 �:0.0121)
		( the:0.0153 have:0.0262 a:0.0177 matter:0.0103�:0.0549 of:0.2168 and:0.0515 the:0.0155)
		( the:0.0229 have:0.0327 not:0.0234 matter:0.0076,:0.0703 of:0.2373 and:0.0486 the:0.0203)
		( the:0.0302 have:0.0386 not:0.0282 new:0.0055,:0.0796 of:0.2295 and:0.0459 the:0.0234)
		( the:0.0376 have:0.0422 the:0.0332 new:0.0065.:0.0845 of:0.2295 and:0.0435 the:0.0258)
		( the:0.0452 have:0.0466 the:0.0396 new:0.0079.:0.0903 of:0.2363 and:0.0420 the:0.0270)
		( the:0.0540 have:0.0500 the:0.0437 new:0.0095.:0.0928 of:0.2393 and:0.0437 the:0.0281)
		( the:0.0640 have:0.0547 the:0.0493 new:0.0110.:0.0942 of:0.2383 and:0.0488 the:0.0292)
		( the:0.0728 have:0.0591 the:0.0515 new:0.0131.:0.0967 of:0.2354 and:0.0525 the:0.0289)
		( the:0.0796 have:0.0640 the:0.0530 new:0.0150.:0.0986 of:0.2295 and:0.0596 the:0.0292)
		( the:0.0884 have:0.0684 the:0.0554 new:0.0166.:0.0986 of:0.2295 and:0.0664 the:0.0299)
		( the:0.0913 have:0.0781 the:0.0540 new:0.0183.:0.1060 of:0.2236 and:0.0757 the:0.0303)
 the book, and the book, and the book,
1100: sample 0: Hello, I'm a language model,
------
		(�:0.0187�:0.0183�:0.0461 ap:0.0072 matter:0.0106�:0.1177�:0.0879�:0.1602)
		( to:0.0204 the:0.0078�:0.0236 not:0.0068 matter:0.0096�:0.0304 of:0.0742�:0.0566)
		( to:0.0239 the:0.0103 have:0.0278 used:0.0104 matter:0.0069smanship:0.0284 of:0.0918�:0.0243)
		( of:0.0277 the:0.0119 have:0.0303 used:0.0160 matter:0.0046,:0.0229 of:0.0933 which:0.0315)
		( of:0.0310 the:0.0154 have:0.0347 used:0.0211 lot:0.0051.:0.0236 of:0.1104 which:0.0396)
		( of:0.0330 the:0.0209 have:0.0417 used:0.0273 lot:0.0084.:0.0289 of:0.1504 which:0.0442)
		( of:0.0337 the:0.0289 have:0.0488 used:0.0322 lot:0.0129.:0.0396 of:0.1816 and:0.0496)
		(.:0.0366 the:0.0405 have:0.0557 used:0.0342 lot:0.0171.:0.0542 of:0.1943 and:0.0703)
		(.:0.0486 the:0.0505 have:0.0613 not:0.0332 lot:0.0216.:0.0732 of:0.1943 and:0.0972)
		(.:0.0610 the:0.0598 have:0.0620 not:0.0361 lot:0.0254.:0.0952 of:0.1826 and:0.1250)
		(,:0.0771 and:0.0776 have:0.0593 not:0.0378 lot:0.0293.:0.1187 of:0.1689 and:0.1523)
		(,:0.0947 and:0.1021 have:0.0544 not:0.0361 lot:0.0315.:0.1387.:0.1934 and:0.1729)
 and
------
		(�:0.0183�:0.0461 ap:0.0072 matter:0.0106�:0.1177�:0.0879�:0.1602 �:0.0060)
		( the:0.0078�:0.0236 not:0.0068 matter:0.0096�:0.0304 of:0.0742�:0.0566 other:0.0106)
		( the:0.0103 have:0.0278 used:0.0104 matter:0.0069smanship:0.0284 of:0.0918�:0.0243 other:0.0135)
		( the:0.0119 have:0.0303 used:0.0160 matter:0.0046,:0.0229 of:0.0933 which:0.0315 other:0.0134)
		( the:0.0154 have:0.0347 used:0.0211 lot:0.0051.:0.0236 of:0.1104 which:0.0396 other:0.0152)
		( the:0.0209 have:0.0417 used:0.0273 lot:0.0084.:0.0289 of:0.1504 which:0.0442 the:0.0186)
		( the:0.0289 have:0.0488 used:0.0322 lot:0.0129.:0.0396 of:0.1816 and:0.0496 the:0.0253)
		( the:0.0405 have:0.0557 used:0.0342 lot:0.0171.:0.0542 of:0.1943 and:0.0703 the:0.0337)
		( the:0.0505 have:0.0613 not:0.0332 lot:0.0216.:0.0732 of:0.1943 and:0.0972 the:0.0410)
		( the:0.0598 have:0.0620 not:0.0361 lot:0.0254.:0.0952 of:0.1826 and:0.1250 the:0.0461)
		( and:0.0776 have:0.0593 not:0.0378 lot:0.0293.:0.1187 of:0.1689 and:0.1523 the:0.0486)
		( and:0.1021 have:0.0544 not:0.0361 lot:0.0315.:0.1387.:0.1934 and:0.1729 the:0.0459)
 the first step to the project.
The first step
1250: sample 0: Hello, I'm a language model,
------
		( area:0.0046�:0.0085�:0.0304 in:0.0118 matter:0.0052�:0.0908�:0.0688�:0.0942)
		( to:0.0067 the:0.0034 use:0.0117 in:0.0204 matter:0.0081,:0.0208 of:0.0248�:0.0138)
		( to:0.0114 the:0.0062 have:0.0131 in:0.0203 matter:0.0059.:0.0277 of:0.0297 and:0.0072)
		( to:0.0127 the:0.0080 have:0.0143 in:0.0184 matter:0.0036.:0.0273 of:0.0337 and:0.0089)
		( of:0.0156 the:0.0107 have:0.0190 in:0.0170 few:0.0053.:0.0286 of:0.0491 and:0.0137)
		( of:0.0199 the:0.0165 was:0.0315 in:0.0165 lot:0.0098.:0.0359 of:0.0713 and:0.0220)
		( of:0.0248 the:0.0266 was:0.0510 sure:0.0217 lot:0.0142.:0.0522 of:0.0928 and:0.0344)
		( of:0.0308 the:0.0430 was:0.0732 sure:0.0281 lot:0.0170.:0.0786 of:0.1123 and:0.0503)
		( of:0.0352 the:0.0645 was:0.0913 able:0.0359 lot:0.0171.:0.1128 of:0.1162 and:0.0654)
		( of:0.0388 the:0.0864 was:0.1001 able:0.0466 lot:0.0159.:0.1543.:0.1426 I:0.0889)
		(,:0.0408 the:0.1060 was:0.0957 able:0.0530 lot:0.0131.:0.1836.:0.1758 I:0.1230)
		(.:0.0513 the:0.1133 was:0.0918 able:0.0588 great:0.0130.:0.2041.:0.2070 I:0.1631)
 I
------
		(�:0.0085�:0.0304 in:0.0118 matter:0.0052�:0.0908�:0.0688�:0.0942�:0.0364)
		( the:0.0034 use:0.0117 in:0.0204 matter:0.0081,:0.0208 of:0.0248�:0.0138 have:0.0145)
		( the:0.0062 have:0.0131 in:0.0203 matter:0.0059.:0.0277 of:0.0297 and:0.0072 have:0.0166)
		( the:0.0080 have:0.0143 in:0.0184 matter:0.0036.:0.0273 of:0.0337 and:0.0089 have:0.0183)
		( the:0.0107 have:0.0190 in:0.0170 few:0.0053.:0.0286 of:0.0491 and:0.0137 have:0.0228)
		( the:0.0165 was:0.0315 in:0.0165 lot:0.0098.:0.0359 of:0.0713 and:0.0220 have:0.0294)
		( the:0.0266 was:0.0510 sure:0.0217 lot:0.0142.:0.0522 of:0.0928 and:0.0344 have:0.0386)
		( the:0.0430 was:0.0732 sure:0.0281 lot:0.0170.:0.0786 of:0.1123 and:0.0503 have:0.0513)
		( the:0.0645 was:0.0913 able:0.0359 lot:0.0171.:0.1128 of:0.1162 and:0.0654 have:0.0618)
		( the:0.0864 was:0.1001 able:0.0466 lot:0.0159.:0.1543.:0.1426 I:0.0889 have:0.0684)
		( the:0.1060 was:0.0957 able:0.0530 lot:0.0131.:0.1836.:0.1758 I:0.1230 have:0.0728)
		( the:0.1133 was:0.0918 able:0.0588 great:0.0130.:0.2041.:0.2070 I:0.1631 have:0.0752)
 have a new language.
The first step is a
1400: sample 0: Hello, I'm a language model,
------
		( period:0.0028�:0.0042�:0.0215ter:0.0135 matter:0.0032�:0.0781�:0.0723�:0.0437)
		( and:0.0033 and:0.0015 have:0.0087 the:0.0082 matter:0.0041�:0.0115.:0.0132�:0.0070)
		( of:0.0045 the:0.0018 have:0.0085ations:0.0084 new:0.0027,:0.0101.:0.0153 and:0.0034)
		( of:0.0056 the:0.0023 have:0.0080ations:0.0076 few:0.0040,:0.0115 of:0.0160 and:0.0056)
		( of:0.0069 the:0.0036 have:0.0092ations:0.0055 few:0.0082,:0.0175 of:0.0228 and:0.0101)
		( of:0.0099 the:0.0066 have:0.0123 sure:0.0079 few:0.0138,:0.0325,:0.0352 and:0.0179)
		(,:0.0159 the:0.0137 have:0.0183 sure:0.0116 few:0.0195,:0.0640,:0.0625 and:0.0342)
		(,:0.0284 the:0.0253,:0.0284 sure:0.0154 few:0.0240,:0.1094,:0.1011 and:0.0615)
		(,:0.0464 the:0.0415,:0.0486 I:0.0208 few:0.0245,:0.1582,:0.1465 and:0.1001)
		(,:0.0645 and:0.0588,:0.0669 I:0.0354 few:0.0216,:0.1982,:0.1875 and:0.1436)
		(,:0.0796 and:0.0752,:0.0791 I:0.0537 new:0.0188,:0.2119,:0.2070 and:0.1787)
		(,:0.0874 and:0.0840,:0.0830 I:0.0679 new:0.0162,:0.2031,:0.2139 and:0.1973)
 and
------
		(�:0.0042�:0.0215ter:0.0135 matter:0.0032�:0.0781�:0.0723�:0.0437 �:0.0032)
		( and:0.0015 have:0.0087 the:0.0082 matter:0.0041�:0.0115.:0.0132�:0.0070 other:0.0060)
		( the:0.0018 have:0.0085ations:0.0084 new:0.0027,:0.0101.:0.0153 and:0.0034 other:0.0121)
		( the:0.0023 have:0.0080ations:0.0076 few:0.0040,:0.0115 of:0.0160 and:0.0056 other:0.0193)
		( the:0.0036 have:0.0092ations:0.0055 few:0.0082,:0.0175 of:0.0228 and:0.0101 other:0.0271)
		( the:0.0066 have:0.0123 sure:0.0079 few:0.0138,:0.0325,:0.0352 and:0.0179 other:0.0359)
		( the:0.0137 have:0.0183 sure:0.0116 few:0.0195,:0.0640,:0.0625 and:0.0342 other:0.0413)
		( the:0.0253,:0.0284 sure:0.0154 few:0.0240,:0.1094,:0.1011 and:0.0615 the:0.0564)
		( the:0.0415,:0.0486 I:0.0208 few:0.0245,:0.1582,:0.1465 and:0.1001 the:0.0791)
		( and:0.0588,:0.0669 I:0.0354 few:0.0216,:0.1982,:0.1875 and:0.1436 the:0.0947)
		( and:0.0752,:0.0791 I:0.0537 new:0.0188,:0.2119,:0.2070 and:0.1787 the:0.0972)
		( and:0.0840,:0.0830 I:0.0679 new:0.0162,:0.2031,:0.2139 and:0.1973 I:0.0894)
 I have a lot of the world.
The first
1550: sample 0: Hello, I'm a language model,
------
		( fore:0.0024�:0.0020 use:0.0113 over:0.0036 matter:0.0033�:0.0620�:0.0245�:0.0243)
		( of:0.0033 then:0.0013 use:0.0083 the:0.0041 matter:0.0026,:0.0101 used:0.0093�:0.0024)
		( of:0.0051 the:0.0019 have:0.0056 the:0.0045 matter:0.0013,:0.0095.:0.0089 not:0.0019)
		( of:0.0057 the:0.0026gu:0.0055 the:0.0039 few:0.0020,:0.0093 of:0.0120 and:0.0024)
		( of:0.0065 the:0.0038gu:0.0065 sure:0.0078 few:0.0035,:0.0126 of:0.0195 and:0.0051)
		( of:0.0086 the:0.0071gu:0.0077 sure:0.0146 few:0.0063,:0.0211 of:0.0308 and:0.0120)
		( of:0.0124 the:0.0136 have:0.0101 sure:0.0278 few:0.0109,:0.0398 of:0.0498 and:0.0283)
		(.:0.0195 the:0.0251,:0.0165 sure:0.0505 few:0.0176,:0.0718,:0.0747 and:0.0588)
		(.:0.0298 the:0.0405,:0.0278 sure:0.0820 few:0.0236,:0.1060,:0.1079 and:0.1016)
		(,:0.0420 the:0.0557,:0.0383 sure:0.1118 few:0.0266,:0.1270,:0.1270 and:0.1328)
		(,:0.0532 the:0.0679,:0.0444 sure:0.1230 few:0.0261.:0.1426.:0.1357 and:0.1455)
		(,:0.0610 the:0.0728,:0.0481 sure:0.1167 few:0.0240.:0.1504 that:0.1514 and:0.1416)
 and
------
		(�:0.0020 use:0.0113 over:0.0036 matter:0.0033�:0.0620�:0.0245�:0.0243 activity:0.0018)
		( then:0.0013 use:0.0083 the:0.0041 matter:0.0026,:0.0101 used:0.0093�:0.0024 other:0.0046)
		( the:0.0019 have:0.0056 the:0.0045 matter:0.0013,:0.0095.:0.0089 not:0.0019 other:0.0079)
		( the:0.0026gu:0.0055 the:0.0039 few:0.0020,:0.0093 of:0.0120 and:0.0024 other:0.0106)
		( the:0.0038gu:0.0065 sure:0.0078 few:0.0035,:0.0126 of:0.0195 and:0.0051 other:0.0142)
		( the:0.0071gu:0.0077 sure:0.0146 few:0.0063,:0.0211 of:0.0308 and:0.0120 other:0.0190)
		( the:0.0136 have:0.0101 sure:0.0278 few:0.0109,:0.0398 of:0.0498 and:0.0283 other:0.0244)
		( the:0.0251,:0.0165 sure:0.0505 few:0.0176,:0.0718,:0.0747 and:0.0588 the:0.0344)
		( the:0.0405,:0.0278 sure:0.0820 few:0.0236,:0.1060,:0.1079 and:0.1016 the:0.0498)
		( the:0.0557,:0.0383 sure:0.1118 few:0.0266,:0.1270,:0.1270 and:0.1328 I:0.0732)
		( the:0.0679,:0.0444 sure:0.1230 few:0.0261.:0.1426.:0.1357 and:0.1455 I:0.1270)
		( the:0.0728,:0.0481 sure:0.1167 few:0.0240.:0.1504 that:0.1514 and:0.1416 I:0.1982)
 I am not a good idea. I don't just
1700: sample 0: Hello, I'm a language model,
------
		( number:0.0015�:0.0022�:0.0073-:0.0085 matter:0.0029�:0.0618�:0.0134�:0.0199)
		( of:0.0020 then:0.0011 am:0.0054 the:0.0031 change:0.0017,:0.0092.:0.0051�:0.0018)
		( of:0.0024 the:0.0012 am:0.0042 the:0.0032 change:0.0014,:0.0060.:0.0054 and:0.0014)
		( of:0.0023 the:0.0015 am:0.0035resses:0.0031 few:0.0011,:0.0050.:0.0058 and:0.0022)
		( of:0.0025 the:0.0025 am:0.0038 sure:0.0047 small:0.0017,:0.0065 of:0.0096 and:0.0058)
		( of:0.0035 the:0.0054 am:0.0053 sure:0.0109 small:0.0029,:0.0123 for:0.0179 and:0.0154)
		( of:0.0059 the:0.0120 am:0.0090 sure:0.0256 small:0.0049,:0.0266,:0.0327 and:0.0376)
		( of:0.0106 the:0.0242,:0.0189 sure:0.0540 small:0.0077,:0.0544,:0.0613 and:0.0776)
		( of:0.0189 the:0.0435,:0.0369 sure:0.0962 small:0.0106,:0.0938,:0.0977 and:0.1289)
		(,:0.0306 the:0.0625,:0.0542 sure:0.1289 small:0.0120,:0.1289,:0.1348 and:0.1719)
		(,:0.0464 the:0.0796,:0.0669 sure:0.1406 member:0.0146,:0.1357,:0.1582 and:0.1777)
		(,:0.0603 the:0.0850,:0.0771 sure:0.1260 member:0.0209,:0.1260 that:0.1738 and:0.1787)
 and
------
		(�:0.0022�:0.0073-:0.0085 matter:0.0029�:0.0618�:0.0134�:0.0199 other:0.0020)
		( then:0.0011 am:0.0054 the:0.0031 change:0.0017,:0.0092.:0.0051�:0.0018 other:0.0049)
		( the:0.0012 am:0.0042 the:0.0032 change:0.0014,:0.0060.:0.0054 and:0.0014 other:0.0073)
		( the:0.0015 am:0.0035resses:0.0031 few:0.0011,:0.0050.:0.0058 and:0.0022 other:0.0084)
		( the:0.0025 am:0.0038 sure:0.0047 small:0.0017,:0.0065 of:0.0096 and:0.0058 other:0.0109)
		( the:0.0054 am:0.0053 sure:0.0109 small:0.0029,:0.0123 for:0.0179 and:0.0154 the:0.0162)
		( the:0.0120 am:0.0090 sure:0.0256 small:0.0049,:0.0266,:0.0327 and:0.0376 the:0.0349)
		( the:0.0242,:0.0189 sure:0.0540 small:0.0077,:0.0544,:0.0613 and:0.0776 the:0.0620)
		( the:0.0435,:0.0369 sure:0.0962 small:0.0106,:0.0938,:0.0977 and:0.1289 the:0.0894)
		( the:0.0625,:0.0542 sure:0.1289 small:0.0120,:0.1289,:0.1348 and:0.1719 the:0.0933)
		( the:0.0796,:0.0669 sure:0.1406 member:0.0146,:0.1357,:0.1582 and:0.1777 I:0.0815)
		( the:0.0850,:0.0771 sure:0.1260 member:0.0209,:0.1260 that:0.1738 and:0.1787 I:0.1230)
 I am not a good idea.
The first step
1850: sample 0: Hello, I'm a language model,
------
		( limits:0.0010�:0.0015�:0.0065 in:0.0101 �:0.0021�:0.0228�:0.0271�:0.0145)
		( of:0.0016 then:0.0009 am:0.0052 in:0.0027 matter:0.0012smanship:0.0052�:0.0038�:0.0015)
		( of:0.0013 editors:0.0010 am:0.0044 like:0.0021]:0.0007smanship:0.0040.:0.0019 Christ:0.0010)
		( of:0.0011 editors:0.0012 am:0.0043 like:0.0022 few:0.0010 editors:0.0033 editors:0.0020 editors:0.0012)
		( editors:0.0011 the:0.0016 am:0.0059 sure:0.0047 few:0.0015 editors:0.0031.:0.0032 and:0.0020)
		( of:0.0015 and:0.0034 am:0.0096 sure:0.0096 few:0.0025,:0.0056.:0.0090 and:0.0064)
		( of:0.0027 and:0.0091 am:0.0162 sure:0.0203 few:0.0041,:0.0132.:0.0288 and:0.0183)
		(.:0.0067 and:0.0232 have:0.0266 sure:0.0371 few:0.0072,:0.0309.:0.0762 and:0.0437)
		(.:0.0164 and:0.0513 have:0.0369 like:0.0608 few:0.0115,:0.0581.:0.1377 and:0.0864)
		(,:0.0344 and:0.0854�:0.0659 like:0.0884 few:0.0149,:0.0845.:0.1895 and:0.1201)
		(,:0.0564 and:0.1089�:0.0889 like:0.0952 great:0.0171 that:0.1357.:0.2197 and:0.1240)
		(,:0.0757 and:0.1133�:0.0967 like:0.0864 great:0.0206 that:0.1533.:0.2002 and:0.1147)
 and
------
		(�:0.0015�:0.0065 in:0.0101 �:0.0021�:0.0228�:0.0271�:0.0145 I:0.0018)
		( then:0.0009 am:0.0052 in:0.0027 matter:0.0012smanship:0.0052�:0.0038�:0.0015 other:0.0018)
		( editors:0.0010 am:0.0044 like:0.0021]:0.0007smanship:0.0040.:0.0019 Christ:0.0010 other:0.0023)
		( editors:0.0012 am:0.0043 like:0.0022 few:0.0010 editors:0.0033 editors:0.0020 editors:0.0012 other:0.0027)
		( the:0.0016 am:0.0059 sure:0.0047 few:0.0015 editors:0.0031.:0.0032 and:0.0020 other:0.0039)
		( and:0.0034 am:0.0096 sure:0.0096 few:0.0025,:0.0056.:0.0090 and:0.0064 the:0.0079)
		( and:0.0091 am:0.0162 sure:0.0203 few:0.0041,:0.0132.:0.0288 and:0.0183 the:0.0256)
		( and:0.0232 have:0.0266 sure:0.0371 few:0.0072,:0.0309.:0.0762 and:0.0437 the:0.0664)
		( and:0.0513 have:0.0369 like:0.0608 few:0.0115,:0.0581.:0.1377 and:0.0864 the:0.1147)
		( and:0.0854�:0.0659 like:0.0884 few:0.0149,:0.0845.:0.1895 and:0.1201 the:0.1250)
		( and:0.1089�:0.0889 like:0.0952 great:0.0171 that:0.1357.:0.2197 and:0.1240 I:0.1089)
		( and:0.1133�:0.0967 like:0.0864 great:0.0206 that:0.1533.:0.2002 and:0.1147 I:0.1562)
 I'm a great way to learn about the world.
2000: sample 0: Hello, I'm a language model,
------
		( fore:0.0013 �:0.0011aps:0.0049-:0.0056 �:0.0028�:0.0284�:0.0115�:0.0104)
		( �:0.0018 I:0.0006aps:0.0029adow:0.0012 lot:0.0007smanship:0.0036,:0.0017 no:0.0009)
		( of:0.0010 then:0.0005deals:0.0021adow:0.0010 way:0.0008smanship:0.0020.:0.0015 no:0.0008)
		( of:0.0008 the:0.0007cons:0.0030 myself:0.0013 way:0.0013.:0.0026.:0.0034 and:0.0020)
		( the:0.0009 the:0.0017 have:0.0049 myself:0.0029 way:0.0020.:0.0061.:0.0107 and:0.0069)
		( the:0.0017 the:0.0043 have:0.0099 sure:0.0066 lot:0.0036.:0.0157.:0.0339 and:0.0195)
		( the:0.0038 the:0.0105 have:0.0216 a:0.0186 lot:0.0076.:0.0374.:0.0884 and:0.0483)
		( the:0.0090 the:0.0236 have:0.0405 a:0.0459 lot:0.0156.:0.0732.:0.1807 and:0.0977)
		( the:0.0201 the:0.0432�:0.0757 a:0.0796 lot:0.0245.:0.1045.:0.2617 and:0.1572)
		( the:0.0369 the:0.0598�:0.1299 a:0.0903 great:0.0327.:0.1025.:0.2949 and:0.1865)
		( the:0.0549 the:0.0659�:0.1543 a:0.0830 great:0.0400 that:0.0981.:0.2734 and:0.1885)
		( the:0.0635 the:0.0610�:0.1416 going:0.0942 very:0.0430 that:0.0933.:0.2305 and:0.1875)
 and
------
		( �:0.0011aps:0.0049-:0.0056 �:0.0028�:0.0284�:0.0115�:0.0104 b:0.0019)
		( I:0.0006aps:0.0029adow:0.0012 lot:0.0007smanship:0.0036,:0.0017 no:0.0009 other:0.0011)
		( then:0.0005deals:0.0021adow:0.0010 way:0.0008smanship:0.0020.:0.0015 no:0.0008 other:0.0018)
		( the:0.0007cons:0.0030 myself:0.0013 way:0.0013.:0.0026.:0.0034 and:0.0020 other:0.0029)
		( the:0.0017 have:0.0049 myself:0.0029 way:0.0020.:0.0061.:0.0107 and:0.0069 the:0.0071)
		( the:0.0043 have:0.0099 sure:0.0066 lot:0.0036.:0.0157.:0.0339 and:0.0195 the:0.0190)
		( the:0.0105 have:0.0216 a:0.0186 lot:0.0076.:0.0374.:0.0884 and:0.0483 the:0.0405)
		( the:0.0236 have:0.0405 a:0.0459 lot:0.0156.:0.0732.:0.1807 and:0.0977 the:0.0640)
		( the:0.0432�:0.0757 a:0.0796 lot:0.0245.:0.1045.:0.2617 and:0.1572 the:0.0718)
		( the:0.0598�:0.1299 a:0.0903 great:0.0327.:0.1025.:0.2949 and:0.1865 it:0.0894)
		( the:0.0659�:0.1543 a:0.0830 great:0.0400 that:0.0981.:0.2734 and:0.1885 I:0.1113)
		( the:0.0610�:0.1416 going:0.0942 very:0.0430 that:0.0933.:0.2305 and:0.1875 I:0.1338)
 I'm going to the world.
I'm going
2150: sample 0: Hello, I'm a language model,
------
		( Island:0.0009 demand:0.0011 needed:0.0048-:0.0082 �:0.0023�:0.0173�:0.0117�:0.0100)
		(,:0.0015 place:0.0005 have:0.0029 doing:0.0012 new:0.0007]:0.0032-:0.0012 I:0.0007)
		( in:0.0009 shadowy:0.0006 have:0.0033akers:0.0016 coach:0.0007]:0.0022ists:0.0009 the:0.0008)
		( the:0.0008 the:0.0009 have:0.0051akers:0.0018 coach:0.0011 teacher:0.0029 that:0.0013 alternatively:0.0011)
		( the:0.0011 the:0.0022 have:0.0101 aware:0.0036 coach:0.0015 teacher:0.0064 that:0.0035 and:0.0028)
		( the:0.0019 the:0.0052 have:0.0188 aware:0.0092 few:0.0022.:0.0123.:0.0137 and:0.0096)
		( the:0.0041 the:0.0121 have:0.0356 aware:0.0210 few:0.0036.:0.0304.:0.0503 and:0.0273)
		( the:0.0089 the:0.0262 have:0.0598 aware:0.0413 good:0.0063.:0.0630.:0.1357 and:0.0669)
		( the:0.0182 the:0.0471 have:0.0801 sure:0.0679 good:0.0121.:0.0967.:0.2295 and:0.1226)
		(.:0.0327 the:0.0703 have:0.0869 sure:0.1128 good:0.0228.:0.1157.:0.3066 and:0.1709)
		(,:0.0510 the:0.0811 have:0.0830 sure:0.1338 good:0.0339 that:0.1494.:0.3379 and:0.1875)
		(,:0.0684 the:0.0767 have:0.0762 sure:0.1245 good:0.0383 that:0.1533.:0.3125 and:0.1836)
 and
------
		( demand:0.0011 needed:0.0048-:0.0082 �:0.0023�:0.0173�:0.0117�:0.0100 �:0.0022)
		( place:0.0005 have:0.0029 doing:0.0012 new:0.0007]:0.0032-:0.0012 I:0.0007 other:0.0010)
		( shadowy:0.0006 have:0.0033akers:0.0016 coach:0.0007]:0.0022ists:0.0009 the:0.0008 possibly:0.0012)
		( the:0.0009 have:0.0051akers:0.0018 coach:0.0011 teacher:0.0029 that:0.0013 alternatively:0.0011 possibly:0.0018)
		( the:0.0022 have:0.0101 aware:0.0036 coach:0.0015 teacher:0.0064 that:0.0035 and:0.0028 possibly:0.0035)
		( the:0.0052 have:0.0188 aware:0.0092 few:0.0022.:0.0123.:0.0137 and:0.0096 the:0.0069)
		( the:0.0121 have:0.0356 aware:0.0210 few:0.0036.:0.0304.:0.0503 and:0.0273 the:0.0197)
		( the:0.0262 have:0.0598 aware:0.0413 good:0.0063.:0.0630.:0.1357 and:0.0669 the:0.0442)
		( the:0.0471 have:0.0801 sure:0.0679 good:0.0121.:0.0967.:0.2295 and:0.1226 the:0.0664)
		( the:0.0703 have:0.0869 sure:0.1128 good:0.0228.:0.1157.:0.3066 and:0.1709 I:0.1060)
		( the:0.0811 have:0.0830 sure:0.1338 good:0.0339 that:0.1494.:0.3379 and:0.1875 I:0.1709)
		( the:0.0767 have:0.0762 sure:0.1245 good:0.0383 that:0.1533.:0.3125 and:0.1836 I:0.2090)
 I have a different approach to the world. I have
2300: sample 0: Hello, I'm a language model,
------
		( request:0.0007�:0.0021 used:0.0037 in:0.0078 �:0.0038�:0.0205�:0.0061�:0.0122)
		( �:0.0013 bullets:0.0005deals:0.0076ings:0.0011 path:0.0007,:0.0028 results:0.0016 sir:0.0007)
		( �:0.0008 bullets:0.0008deals:0.0157ethyst:0.0010 few:0.0007,:0.0016 −:0.0011 sir:0.0008)
		( the:0.0006 bullets:0.0008deals:0.0135 the:0.0013 few:0.0010 therapist:0.0025,:0.0013 then:0.0009)
		( the:0.0008 the:0.0010deals:0.0108 the:0.0029 few:0.0017 therapist:0.0038.:0.0039 and:0.0031)
		( the:0.0012 and:0.0026 have:0.0148 the:0.0074 few:0.0031,:0.0069.:0.0167 and:0.0112)
		( the:0.0024 and:0.0067 have:0.0306 a:0.0200 few:0.0054.:0.0161.:0.0618 and:0.0280)
		( the:0.0050 and:0.0194 have:0.0540 a:0.0496 few:0.0090,:0.0304.:0.1475 and:0.0645)
		( the:0.0102 and:0.0491 have:0.0752 a:0.0771 good:0.0166 of:0.0471.:0.2305 and:0.1177)
		( the:0.0190 and:0.0957 have:0.0903 a:0.0874 good:0.0325 of:0.0718.:0.2354 and:0.1729)
		( the:0.0309 and:0.1318 have:0.0913 not:0.0908 good:0.0432 of:0.0854.:0.1865 and:0.1836)
		(,:0.0442 and:0.1357 have:0.0898 not:0.0923 very:0.0469 of:0.0811 for:0.1641 and:0.1787)
 and
------
		(�:0.0021 used:0.0037 in:0.0078 �:0.0038�:0.0205�:0.0061�:0.0122 �:0.0027)
		( bullets:0.0005deals:0.0076ings:0.0011 path:0.0007,:0.0028 results:0.0016 sir:0.0007 then:0.0007)
		( bullets:0.0008deals:0.0157ethyst:0.0010 few:0.0007,:0.0016 −:0.0011 sir:0.0008 then:0.0009)
		( bullets:0.0008deals:0.0135 the:0.0013 few:0.0010 therapist:0.0025,:0.0013 then:0.0009 other:0.0014)
		( the:0.0010deals:0.0108 the:0.0029 few:0.0017 therapist:0.0038.:0.0039 and:0.0031 the:0.0035)
		( and:0.0026 have:0.0148 the:0.0074 few:0.0031,:0.0069.:0.0167 and:0.0112 the:0.0100)
		( and:0.0067 have:0.0306 a:0.0200 few:0.0054.:0.0161.:0.0618 and:0.0280 the:0.0240)
		( and:0.0194 have:0.0540 a:0.0496 few:0.0090,:0.0304.:0.1475 and:0.0645 the:0.0442)
		( and:0.0491 have:0.0752 a:0.0771 good:0.0166 of:0.0471.:0.2305 and:0.1177 the:0.0623)
		( and:0.0957 have:0.0903 a:0.0874 good:0.0325 of:0.0718.:0.2354 and:0.1729 I:0.1221)
		( and:0.1318 have:0.0913 not:0.0908 good:0.0432 of:0.0854.:0.1865 and:0.1836 I:0.2178)
		( and:0.1357 have:0.0898 not:0.0923 very:0.0469 of:0.0811 for:0.1641 and:0.1787 I:0.2402)
 I'm a very good example.
I'm a
2450: sample 0: Hello, I'm a language model,
------
		( fore:0.0006 �:0.0018 needed:0.0032 and:0.0061 �:0.0053�:0.0117�:0.0065�:0.0156)
		( ":0.0011 then:0.0007deals:0.0040ings:0.0020 new:0.0008 arts:0.0027 sets:0.0010 then:0.0009)
		( in:0.0007 then:0.0008deals:0.0095ings:0.0017 detachment:0.0007 arts:0.0026 name:0.0009 then:0.0013)
		( in:0.0005 then:0.0011deals:0.0092ize:0.0011que:0.0009 spoken:0.0029 name:0.0016 then:0.0018)
		( in:0.0006 the:0.0017deals:0.0072 the:0.0021 new:0.0013 spoken:0.0052,:0.0035 and:0.0029)
		( in:0.0008 the:0.0031 wonder:0.0074 the:0.0052 new:0.0020 spoken:0.0089,:0.0109 and:0.0092)
		(,:0.0014 the:0.0059 have:0.0113 a:0.0109 new:0.0031,:0.0152,:0.0334 and:0.0259)
		(,:0.0031 and:0.0133 have:0.0201 not:0.0259 new:0.0049,:0.0317,:0.0786 and:0.0664)
		(,:0.0074 and:0.0308 have:0.0297 not:0.0530 good:0.0085,:0.0518,:0.1196 and:0.1318)
		(,:0.0177 and:0.0603,:0.0403 not:0.0864 member:0.0172,:0.0571 that:0.1582 and:0.1904)
		(,:0.0366 and:0.0894,:0.0542 not:0.1108 member:0.0327 of:0.0603 that:0.1816 and:0.2041)
		(,:0.0630 and:0.1035,:0.0654 not:0.1113 member:0.0469 of:0.0481 that:0.1768 and:0.1914)
 and
------
		( �:0.0018 needed:0.0032 and:0.0061 �:0.0053�:0.0117�:0.0065�:0.0156�:0.0027)
		( then:0.0007deals:0.0040ings:0.0020 new:0.0008 arts:0.0027 sets:0.0010 then:0.0009 then:0.0010)
		( then:0.0008deals:0.0095ings:0.0017 detachment:0.0007 arts:0.0026 name:0.0009 then:0.0013 then:0.0013)
		( then:0.0011deals:0.0092ize:0.0011que:0.0009 spoken:0.0029 name:0.0016 then:0.0018 then:0.0020)
		( the:0.0017deals:0.0072 the:0.0021 new:0.0013 spoken:0.0052,:0.0035 and:0.0029 then:0.0035)
		( the:0.0031 wonder:0.0074 the:0.0052 new:0.0020 spoken:0.0089,:0.0109 and:0.0092 then:0.0072)
		( the:0.0059 have:0.0113 a:0.0109 new:0.0031,:0.0152,:0.0334 and:0.0259 the:0.0198)
		( and:0.0133 have:0.0201 not:0.0259 new:0.0049,:0.0317,:0.0786 and:0.0664 the:0.0420)
		( and:0.0308 have:0.0297 not:0.0530 good:0.0085,:0.0518,:0.1196 and:0.1318 the:0.0645)
		( and:0.0603,:0.0403 not:0.0864 member:0.0172,:0.0571 that:0.1582 and:0.1904 I:0.1050)
		( and:0.0894,:0.0542 not:0.1108 member:0.0327 of:0.0603 that:0.1816 and:0.2041 I:0.1504)
		( and:0.1035,:0.0654 not:0.1113 member:0.0469 of:0.0481 that:0.1768 and:0.1914 I:0.1777)
 I'm a member of the ICTS.

2600: sample 0: Hello, I'm a language model,
------
		( strike:0.0006 and:0.0022 needed:0.0042 in:0.0026 �:0.0055�:0.0086 �:0.0039�:0.0142)
		( �:0.0012 then:0.0005deals:0.0066ings:0.0005 new:0.0008,:0.0021isans:0.0010�:0.0007)
		(,:0.0006 and:0.0005deals:0.0115 passively:0.0006 new:0.0007 problems:0.0018 analyses:0.0010 and:0.0007)
		(,:0.0004 and:0.0010deals:0.0099 passively:0.0007 series:0.0008.:0.0023 failure:0.0013 and:0.0014)
		(,:0.0005 and:0.0021deals:0.0082 guilty:0.0008 bunch:0.0011 teacher:0.0034.:0.0032 and:0.0037)
		(,:0.0008 and:0.0041deals:0.0070 not:0.0018 lot:0.0019.:0.0068.:0.0124 and:0.0102)
		(,:0.0017 and:0.0090 have:0.0097 not:0.0053 lot:0.0038.:0.0165.:0.0449 and:0.0269)
		(.:0.0044 and:0.0215 have:0.0181 not:0.0168 lot:0.0076.:0.0378.:0.1172 and:0.0669)
		(.:0.0126 and:0.0481 have:0.0289 a:0.0457 bit:0.0155.:0.0635.:0.2012 and:0.1270)
		(.:0.0334 and:0.0845�:0.0481 a:0.0767 bit:0.0374 that:0.1128.:0.2275 and:0.1797)
		(.:0.0713 and:0.1113�:0.0723 a:0.0767 bit:0.0635 that:0.1709.:0.2051 and:0.1875)
		(.:0.1172 and:0.1133�:0.0825 not:0.0713 bit:0.0713 that:0.2012 that:0.2412 and:0.1631)
 and
------
		( and:0.0022 needed:0.0042 in:0.0026 �:0.0055�:0.0086 �:0.0039�:0.0142 �:0.0030)
		( then:0.0005deals:0.0066ings:0.0005 new:0.0008,:0.0021isans:0.0010�:0.0007 other:0.0011)
		( and:0.0005deals:0.0115 passively:0.0006 new:0.0007 problems:0.0018 analyses:0.0010 and:0.0007 other:0.0018)
		( and:0.0010deals:0.0099 passively:0.0007 series:0.0008.:0.0023 failure:0.0013 and:0.0014 other:0.0029)
		( and:0.0021deals:0.0082 guilty:0.0008 bunch:0.0011 teacher:0.0034.:0.0032 and:0.0037 other:0.0053)
		( and:0.0041deals:0.0070 not:0.0018 lot:0.0019.:0.0068.:0.0124 and:0.0102 other:0.0090)
		( and:0.0090 have:0.0097 not:0.0053 lot:0.0038.:0.0165.:0.0449 and:0.0269 the:0.0203)
		( and:0.0215 have:0.0181 not:0.0168 lot:0.0076.:0.0378.:0.1172 and:0.0669 the:0.0430)
		( and:0.0481 have:0.0289 a:0.0457 bit:0.0155.:0.0635.:0.2012 and:0.1270 the:0.0679)
		( and:0.0845�:0.0481 a:0.0767 bit:0.0374 that:0.1128.:0.2275 and:0.1797 I:0.0796)
		( and:0.1113�:0.0723 a:0.0767 bit:0.0635 that:0.1709.:0.2051 and:0.1875 I:0.1201)
		( and:0.1133�:0.0825 not:0.0713 bit:0.0713 that:0.2012 that:0.2412 and:0.1631 I:0.1475)
 I think it is a language model.
I think
