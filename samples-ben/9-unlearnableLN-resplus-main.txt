1: sample 0: Hello, I'm a language model,
------
		(Hello:0.0003,:0.0014 I:0.0005'm:0.0007 a:0.0005 language:0.0012 model:0.0004,:0.0017)
		( outcome:0.0002,:0.0006 schema:0.0002 McGu:0.0002 wives:0.0002FH:0.0002 merits:0.0002,:0.0009)
		( outcome:0.0002,:0.0003 determin:0.0002 McGu:0.0002Extreme:0.0002English:0.0002audio:0.0002,:0.0006)
		( fa:0.0002,:0.0003 determin:0.0002 McGu:0.0002Extreme:0.0002English:0.0002audio:0.0002,:0.0005)
		( fa:0.0002,:0.0002 determin:0.0002 McGu:0.0001ises:0.0002English:0.0002audio:0.0002,:0.0004)
		(English:0.0002,:0.0002 determin:0.0001 the:0.0001ises:0.0002English:0.0002audio:0.0002,:0.0003)
		(English:0.0002,:0.0002 2016:0.0001 the:0.0002ises:0.0002English:0.0002audio:0.0002,:0.0003)
		(English:0.0002 derailed:0.0002 the:0.0001 the:0.0002ises:0.0002English:0.0002audio:0.0002,:0.0003)
		(English:0.0002 derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002audio:0.0002,:0.0002)
		(English:0.0002 derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002audio:0.0002,:0.0002)
		(English:0.0002 derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002)
		(English:0.0002 derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002)
		(English:0.0002 derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002)
,
------
		(,:0.0014 I:0.0005'm:0.0007 a:0.0005 language:0.0012 model:0.0004,:0.0017,:0.0016)
		(,:0.0006 schema:0.0002 McGu:0.0002 wives:0.0002FH:0.0002 merits:0.0002,:0.0009,:0.0008)
		(,:0.0003 determin:0.0002 McGu:0.0002Extreme:0.0002English:0.0002audio:0.0002,:0.0006,:0.0005)
		(,:0.0003 determin:0.0002 McGu:0.0002Extreme:0.0002English:0.0002audio:0.0002,:0.0005,:0.0004)
		(,:0.0002 determin:0.0002 McGu:0.0001ises:0.0002English:0.0002audio:0.0002,:0.0004,:0.0004)
		(,:0.0002 determin:0.0001 the:0.0001ises:0.0002English:0.0002audio:0.0002,:0.0003,:0.0003)
		(,:0.0002 2016:0.0001 the:0.0002ises:0.0002English:0.0002audio:0.0002,:0.0003,:0.0003)
		( derailed:0.0002 the:0.0001 the:0.0002ises:0.0002English:0.0002audio:0.0002,:0.0003,:0.0003)
		( derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002audio:0.0002,:0.0002,:0.0002)
		( derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002audio:0.0002,:0.0002,:0.0002)
		( derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002,:0.0002)
		( derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002,:0.0002)
		( derailed:0.0002 the:0.0002 the:0.0002 the:0.0002English:0.0002 the:0.0002,:0.0002,:0.0002)
,,,,,,,,,,,
50: sample 0: Hello, I'm a language model,
------
		(-:0.0327,:0.0615.:0.0557.:0.0649-:0.0255.:0.0693.:0.0703 and:0.0811)
		(-:0.0320.:0.0708.:0.0645.:0.0752-:0.0255.:0.0776.:0.0708 and:0.0664)
		(-:0.0306.:0.0771.:0.0703.:0.0796-:0.0250.:0.0815.:0.0703 and:0.0593)
		(-:0.0286.:0.0806.:0.0718.:0.0835-:0.0254.:0.0830.:0.0674 and:0.0566)
		(-:0.0283.:0.0820.:0.0732.:0.0845-:0.0259.:0.0820.:0.0664.:0.0537)
		(-:0.0273.:0.0835.:0.0742.:0.0864-:0.0258.:0.0835.:0.0630.:0.0544)
		(-:0.0271.:0.0845.:0.0752.:0.0845-:0.0256.:0.0820.:0.0615.:0.0566)
		(-:0.0262.:0.0854.:0.0737.:0.0859-:0.0264.:0.0806.:0.0603.:0.0571)
		(-:0.0261.:0.0864.:0.0747.:0.0869-:0.0262.:0.0791.:0.0586.:0.0593)
		(-:0.0260.:0.0850.:0.0732.:0.0850-:0.0262.:0.0796.:0.0574.:0.0598)
		(-:0.0260.:0.0859.:0.0737.:0.0859-:0.0270.:0.0781.:0.0559.:0.0603)
		(-:0.0251.:0.0840.:0.0723.:0.0840-:0.0269.:0.0762.:0.0544.:0.0608)
		(-:0.0251.:0.0840.:0.0723.:0.0840-:0.0269.:0.0762.:0.0544.:0.0608)
.
------
		(,:0.0615.:0.0557.:0.0649-:0.0255.:0.0693.:0.0703 and:0.0811
:0.2773)
		(.:0.0708.:0.0645.:0.0752-:0.0255.:0.0776.:0.0708 and:0.0664
:0.2383)
		(.:0.0771.:0.0703.:0.0796-:0.0250.:0.0815.:0.0703 and:0.0593
:0.2178)
		(.:0.0806.:0.0718.:0.0835-:0.0254.:0.0830.:0.0674 and:0.0566
:0.2090)
		(.:0.0820.:0.0732.:0.0845-:0.0259.:0.0820.:0.0664.:0.0537
:0.1992)
		(.:0.0835.:0.0742.:0.0864-:0.0258.:0.0835.:0.0630.:0.0544
:0.1904)
		(.:0.0845.:0.0752.:0.0845-:0.0256.:0.0820.:0.0615.:0.0566
:0.1816)
		(.:0.0854.:0.0737.:0.0859-:0.0264.:0.0806.:0.0603.:0.0571
:0.1729)
		(.:0.0864.:0.0747.:0.0869-:0.0262.:0.0791.:0.0586.:0.0593
:0.1650)
		(.:0.0850.:0.0732.:0.0850-:0.0262.:0.0796.:0.0574.:0.0598
:0.1650)
		(.:0.0859.:0.0737.:0.0859-:0.0270.:0.0781.:0.0559.:0.0603
:0.1572)
		(.:0.0840.:0.0723.:0.0840-:0.0269.:0.0762.:0.0544.:0.0608
:0.1494)
		(.:0.0840.:0.0723.:0.0840-:0.0269.:0.0762.:0.0544.:0.0608
:0.1494)


The-The-The-The-The
200: sample 0: Hello, I'm a language model,
------
		(,:0.0767 the:0.0830,:0.0417 the:0.0723-:0.0222.:0.1113,:0.0962 and:0.1260)
		(,:0.0596 the:0.0879,:0.0491 the:0.0732-:0.0077.:0.1182.:0.0972 and:0.1338)
		(,:0.0513 the:0.0898,:0.0552 the:0.0674 very:0.0062.:0.1147.:0.0996 and:0.1299)
		(,:0.0476 the:0.0884,:0.0593 the:0.0625 number:0.0065.:0.1177.:0.1021 and:0.1206)
		(,:0.0439 the:0.0869,:0.0645,:0.0549 number:0.0069.:0.1152.:0.1011 and:0.1187)
		(,:0.0430 the:0.0854,:0.0664,:0.0581 way:0.0073.:0.1201.:0.1050 and:0.1113)
		(,:0.0408 the:0.0859,:0.0645,:0.0579 way:0.0076.:0.1191.:0.1045 and:0.1040)
		(,:0.0398 the:0.0845,:0.0674,:0.0613 way:0.0080.:0.1182.:0.1035 and:0.0977)
		(,:0.0388 the:0.0850,:0.0664,:0.0610 way:0.0082.:0.1167.:0.1089 and:0.0889)
		(,:0.0376 the:0.0854,:0.0654,:0.0645 way:0.0084.:0.1162.:0.1084 and:0.0840)
		(,:0.0366 the:0.0830,:0.0649,:0.0645 way:0.0082.:0.1157.:0.1079 and:0.0786)
		(,:0.0366 the:0.0835 are:0.0684,:0.0684 way:0.0085.:0.1157.:0.1074 and:0.0737)
		(,:0.0366 the:0.0835 are:0.0684,:0.0684 way:0.0085.:0.1157.:0.1074 and:0.0737)
 and
------
		( the:0.0830,:0.0417 the:0.0723-:0.0222.:0.1113,:0.0962 and:0.1260 the:0.0322)
		( the:0.0879,:0.0491 the:0.0732-:0.0077.:0.1182.:0.0972 and:0.1338 the:0.0322)
		( the:0.0898,:0.0552 the:0.0674 very:0.0062.:0.1147.:0.0996 and:0.1299 the:0.0309)
		( the:0.0884,:0.0593 the:0.0625 number:0.0065.:0.1177.:0.1021 and:0.1206 the:0.0300)
		( the:0.0869,:0.0645,:0.0549 number:0.0069.:0.1152.:0.1011 and:0.1187 the:0.0293)
		( the:0.0854,:0.0664,:0.0581 way:0.0073.:0.1201.:0.1050 and:0.1113 the:0.0297)
		( the:0.0859,:0.0645,:0.0579 way:0.0076.:0.1191.:0.1045 and:0.1040 the:0.0293)
		( the:0.0845,:0.0674,:0.0613 way:0.0080.:0.1182.:0.1035 and:0.0977 the:0.0289)
		( the:0.0850,:0.0664,:0.0610 way:0.0082.:0.1167.:0.1089 and:0.0889 the:0.0295)
		( the:0.0854,:0.0654,:0.0645 way:0.0084.:0.1162.:0.1084 and:0.0840 the:0.0293)
		( the:0.0830,:0.0649,:0.0645 way:0.0082.:0.1157.:0.1079 and:0.0786 the:0.0299)
		( the:0.0835 are:0.0684,:0.0684 way:0.0085.:0.1157.:0.1074 and:0.0737 the:0.0298)
		( the:0.0835 are:0.0684,:0.0684 way:0.0085.:0.1157.:0.1074 and:0.0737 the:0.0298)
 the own, the way of the child is the child
350: sample 0: Hello, I'm a language model,
------
		(,:0.0488 the:0.0605 have:0.0859 the:0.0347-:0.1152,:0.1250,:0.1177 and:0.1689)
		(,:0.0674 the:0.0684 have:0.1011 a:0.0500-:0.0253,:0.1177,:0.1221 and:0.1846)
		(,:0.0618 the:0.0737 have:0.1030,:0.0557-:0.0148,:0.1133,:0.1235 and:0.1807)
		(,:0.0574 the:0.0781 have:0.1016,:0.0586 good:0.0125,:0.1162,:0.1279 and:0.1738)
		(,:0.0520 the:0.0796 have:0.1006,:0.0581 good:0.0135,:0.1147,:0.1328 and:0.1689)
		(,:0.0500 the:0.0806 have:0.0947,:0.0581 good:0.0139,:0.1196,:0.1318 and:0.1660)
		(.:0.0491 the:0.0840 have:0.0942,:0.0579 good:0.0145,:0.1187,:0.1309 and:0.1543)
		(.:0.0498 the:0.0845 have:0.0942,:0.0576 good:0.0151,:0.1177,:0.1309 and:0.1523)
		(.:0.0491 the:0.0854 have:0.0889,:0.0576 good:0.0153,:0.1172,:0.1367 and:0.1504)
		(.:0.0496 the:0.0864 have:0.0889,:0.0574 good:0.0156,:0.1162,:0.1367 and:0.1494)
		(.:0.0488 the:0.0874 have:0.0889,:0.0574 good:0.0159,:0.1099,:0.1299 and:0.1475)
		(.:0.0481 the:0.0884 have:0.0894,:0.0571 good:0.0162,:0.1099,:0.1299 and:0.1465)
		(.:0.0481 the:0.0884 have:0.0894,:0.0571 good:0.0162,:0.1099,:0.1299 and:0.1465)
 and
------
		( the:0.0605 have:0.0859 the:0.0347-:0.1152,:0.1250,:0.1177 and:0.1689 the:0.0649)
		( the:0.0684 have:0.1011 a:0.0500-:0.0253,:0.1177,:0.1221 and:0.1846 the:0.0588)
		( the:0.0737 have:0.1030,:0.0557-:0.0148,:0.1133,:0.1235 and:0.1807 the:0.0488)
		( the:0.0781 have:0.1016,:0.0586 good:0.0125,:0.1162,:0.1279 and:0.1738 the:0.0437)
		( the:0.0796 have:0.1006,:0.0581 good:0.0135,:0.1147,:0.1328 and:0.1689 the:0.0396)
		( the:0.0806 have:0.0947,:0.0581 good:0.0139,:0.1196,:0.1318 and:0.1660 the:0.0371)
		( the:0.0840 have:0.0942,:0.0579 good:0.0145,:0.1187,:0.1309 and:0.1543 the:0.0359)
		( the:0.0845 have:0.0942,:0.0576 good:0.0151,:0.1177,:0.1309 and:0.1523 the:0.0349)
		( the:0.0854 have:0.0889,:0.0576 good:0.0153,:0.1172,:0.1367 and:0.1504 the:0.0352)
		( the:0.0864 have:0.0889,:0.0574 good:0.0156,:0.1162,:0.1367 and:0.1494 the:0.0342)
		( the:0.0874 have:0.0889,:0.0574 good:0.0159,:0.1099,:0.1299 and:0.1475 the:0.0334)
		( the:0.0884 have:0.0894,:0.0571 good:0.0162,:0.1099,:0.1299 and:0.1465 the:0.0337)
		( the:0.0884 have:0.0894,:0.0571 good:0.0162,:0.1099,:0.1299 and:0.1465 the:0.0337)
 the time, and the time, and the time,
500: sample 0: Hello, I'm a language model,
------
		(
:0.0432 and:0.0371 have:0.0231 the:0.0339 one:0.0179,:0.0835,:0.0835 and:0.0679)
		(
:0.0566 the:0.0530 have:0.0479 the:0.0520 few:0.0157,:0.0859,:0.0825 and:0.0776)
		(
:0.0481 and:0.0540 have:0.0630 a:0.0581 few:0.0172,:0.0908,:0.0898 and:0.0806)
		(
:0.0430 and:0.0544 have:0.0703 a:0.0679 good:0.0195,:0.0952,:0.0977 and:0.0771)
		(,:0.0398 and:0.0554 have:0.0723 a:0.0752 good:0.0219,:0.0981,:0.1016 and:0.0742)
		(,:0.0417 and:0.0557 have:0.0723 a:0.0771 good:0.0237,:0.1030,:0.1128 and:0.0723)
		(,:0.0425 and:0.0544 have:0.0737 a:0.0825 good:0.0255,:0.1099,:0.1152 and:0.0688)
		(,:0.0422 and:0.0552 have:0.0718 a:0.0845 good:0.0270,:0.1118,:0.1245 and:0.0664)
		(,:0.0430 and:0.0544 have:0.0708 a:0.0825 good:0.0278,:0.1152,:0.1289 and:0.0645)
		(,:0.0427 and:0.0537 have:0.0698 a:0.0854 good:0.0289,:0.1191,:0.1338 and:0.0630)
		(,:0.0427 the:0.0518 have:0.0698 a:0.0840 good:0.0293,:0.1240,:0.1396 I:0.0635)
		(,:0.0425 the:0.0515 have:0.0698 a:0.0874 good:0.0298,:0.1299,:0.1387 I:0.0688)
		(,:0.0425 the:0.0515 have:0.0698 a:0.0874 good:0.0298,:0.1299,:0.1387 I:0.0688)
 I
------
		( and:0.0371 have:0.0231 the:0.0339 one:0.0179,:0.0835,:0.0835 and:0.0679 have:0.0243)
		( the:0.0530 have:0.0479 the:0.0520 few:0.0157,:0.0859,:0.0825 and:0.0776 have:0.0332)
		( and:0.0540 have:0.0630 a:0.0581 few:0.0172,:0.0908,:0.0898 and:0.0806 am:0.0364)
		( and:0.0544 have:0.0703 a:0.0679 good:0.0195,:0.0952,:0.0977 and:0.0771 am:0.0420)
		( and:0.0554 have:0.0723 a:0.0752 good:0.0219,:0.0981,:0.1016 and:0.0742 am:0.0454)
		( and:0.0557 have:0.0723 a:0.0771 good:0.0237,:0.1030,:0.1128 and:0.0723 am:0.0481)
		( and:0.0544 have:0.0737 a:0.0825 good:0.0255,:0.1099,:0.1152 and:0.0688 am:0.0500)
		( and:0.0552 have:0.0718 a:0.0845 good:0.0270,:0.1118,:0.1245 and:0.0664 am:0.0508)
		( and:0.0544 have:0.0708 a:0.0825 good:0.0278,:0.1152,:0.1289 and:0.0645 am:0.0518)
		( and:0.0537 have:0.0698 a:0.0854 good:0.0289,:0.1191,:0.1338 and:0.0630 am:0.0530)
		( the:0.0518 have:0.0698 a:0.0840 good:0.0293,:0.1240,:0.1396 I:0.0635 am:0.0525)
		( the:0.0515 have:0.0698 a:0.0874 good:0.0298,:0.1299,:0.1387 I:0.0688 am:0.0557)
		( the:0.0515 have:0.0698 a:0.0874 good:0.0298,:0.1299,:0.1387 I:0.0688 am:0.0557)
 am a good, I am a good, I am
