1: sample 0: Hello, I'm a language model,
------
		( Choose:0.0001 Fle:0.0001 Gloria:0.0001 86:0.0001 ordinary:0.0001anova:0.0001Charge:0.0001Stats:0.0001)
		( subsidized:0.0001eties:0.0001 contribut:0.0001 oppressed:0.0001 contribut:0.0001 contribut:0.0001 contribut:0.0001 Fancy:0.0001)
		( TLS:0.0001 bathroom:0.0001 bathroom:0.0001 bathroom:0.0001 bathroom:0.0001Lua:0.0001Lua:0.0001Lua:0.0001)
		( Ev:0.0001 Ev:0.0001 Sometimes:0.0001 Sometimes:0.0001adder:0.0001adder:0.0001adder:0.0001adder:0.0001)
		( decade:0.0001 decade:0.0001 decade:0.0001modified:0.0001modified:0.0001modified:0.0001modified:0.0001forestation:0.0001)
		(heim:0.0001heim:0.0001heim:0.0001heim:0.0001 dreams:0.0001 online:0.0001 online:0.0001 online:0.0001)
		(bows:0.0001bows:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001)
		( hopefully:0.0001 diarrhea:0.0001 pet:0.0001 pet:0.0001 pet:0.0001 pet:0.0001 McH:0.0001 NBC:0.0001)
		(orc:0.0001orc:0.0001orc:0.0001hip:0.0001hip:0.0001hip:0.0001hip:0.0001hip:0.0001)
		(bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001)
		( Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001)
		( Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001)
		( Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001)
 Trouble
------
		( Fle:0.0001 Gloria:0.0001 86:0.0001 ordinary:0.0001anova:0.0001Charge:0.0001Stats:0.0001 Orig:0.0001)
		(eties:0.0001 contribut:0.0001 oppressed:0.0001 contribut:0.0001 contribut:0.0001 contribut:0.0001 Fancy:0.0001 fant:0.0001)
		( bathroom:0.0001 bathroom:0.0001 bathroom:0.0001 bathroom:0.0001Lua:0.0001Lua:0.0001Lua:0.0001Lua:0.0001)
		( Ev:0.0001 Sometimes:0.0001 Sometimes:0.0001adder:0.0001adder:0.0001adder:0.0001adder:0.0001adder:0.0001)
		( decade:0.0001 decade:0.0001modified:0.0001modified:0.0001modified:0.0001modified:0.0001forestation:0.0001forestation:0.0001)
		(heim:0.0001heim:0.0001heim:0.0001 dreams:0.0001 online:0.0001 online:0.0001 online:0.0001 online:0.0001)
		(bows:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001姫:0.0001)
		( diarrhea:0.0001 pet:0.0001 pet:0.0001 pet:0.0001 pet:0.0001 McH:0.0001 NBC:0.0001 NBC:0.0001)
		(orc:0.0001orc:0.0001hip:0.0001hip:0.0001hip:0.0001hip:0.0001hip:0.0001hip:0.0001)
		(bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001bet:0.0001)
		( Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 Sarah:0.0001 terminals:0.0001)
		( Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001)
		( Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001 Trouble:0.0001)
 Trouble Trouble Trouble Trouble Tripoli Tripoli Tripoli Tripoli Tripoli Tripoli Tripoli
50: sample 0: Hello, I'm a language model,
------
		( to:0.0009.:0.0396.:0.0393.:0.0369.:0.0361.:0.0369.:0.0381.:0.0369)
		(.:0.0383.:0.0366.:0.0391.:0.0410.:0.0410.:0.0410.:0.0408.:0.0391)
		(.:0.0398.:0.0388.:0.0378.:0.0369.:0.0359.:0.0359.:0.0352.:0.0369)
		(.:0.0334.:0.0354.:0.0361.:0.0371.:0.0371.:0.0371.:0.0381.:0.0361)
		(.:0.0381.:0.0364.:0.0364.:0.0354.:0.0354.:0.0354.:0.0354.:0.0364)
		(.:0.0354.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364)
		(.:0.0364.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0366.:0.0366.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
.
------
		(.:0.0396.:0.0393.:0.0369.:0.0361.:0.0369.:0.0381.:0.0369.:0.0359)
		(.:0.0366.:0.0391.:0.0410.:0.0410.:0.0410.:0.0408.:0.0391.:0.0403)
		(.:0.0388.:0.0378.:0.0369.:0.0359.:0.0359.:0.0352.:0.0369.:0.0369)
		(.:0.0354.:0.0361.:0.0371.:0.0371.:0.0371.:0.0381.:0.0361.:0.0374)
		(.:0.0364.:0.0364.:0.0354.:0.0354.:0.0354.:0.0354.:0.0364.:0.0364)
		(.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364.:0.0364)
		(.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0366.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0356.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
		(.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354.:0.0354)
...........
200: sample 0: Hello, I'm a language model,
------
		( the:0.0527
:0.0698 be:0.0376 the:0.0564-:0.0089 the:0.0549 the:0.0535 and:0.0942)
		( the:0.0801
:0.1738 the:0.0471 the:0.0747,:0.0122,:0.0767,:0.0791 and:0.0718)
		(,:0.0581
:0.1279 to:0.0403 the:0.0525 and:0.0149,:0.0688,:0.0708 and:0.0713)
		(,:0.0771
:0.1709 to:0.0376 the:0.0447 and:0.0153,:0.0820,:0.0840 and:0.0723)
		(,:0.0698
:0.1719 to:0.0459 the:0.0486-:0.0156,:0.0850,:0.0874 and:0.0708)
		(,:0.0938
:0.1855 to:0.0432 the:0.0459-:0.0123,:0.0967,:0.0967 and:0.0718)
		(,:0.0796
:0.2031 to:0.0537 the:0.0574-:0.0137,:0.0991,:0.0996 and:0.0708)
		(,:0.0996
:0.2012 to:0.0481 the:0.0520-:0.0114,:0.1055,:0.1030 and:0.0708)
		(,:0.0811
:0.2109 to:0.0564 the:0.0618-:0.0117,:0.1045,:0.1045 and:0.0713)
		(,:0.0947
:0.1982 to:0.0500 the:0.0571-:0.0100,:0.1099,:0.1055 and:0.0688)
		(,:0.0757
:0.2031 to:0.0571 the:0.0640-:0.0107 of:0.1113,:0.1030 and:0.0708)
		(,:0.0840
:0.1885,:0.0493 the:0.0586-:0.0093,:0.1138,:0.1045 and:0.0684)
		(,:0.0840
:0.1885,:0.0493 the:0.0586-:0.0093,:0.1138,:0.1045 and:0.0684)
 and
------
		(
:0.0698 be:0.0376 the:0.0564-:0.0089 the:0.0549 the:0.0535 and:0.0942 the:0.0315)
		(
:0.1738 the:0.0471 the:0.0747,:0.0122,:0.0767,:0.0791 and:0.0718 the:0.0364)
		(
:0.1279 to:0.0403 the:0.0525 and:0.0149,:0.0688,:0.0708 and:0.0713 the:0.0410)
		(
:0.1709 to:0.0376 the:0.0447 and:0.0153,:0.0820,:0.0840 and:0.0723 the:0.0437)
		(
:0.1719 to:0.0459 the:0.0486-:0.0156,:0.0850,:0.0874 and:0.0708 the:0.0557)
		(
:0.1855 to:0.0432 the:0.0459-:0.0123,:0.0967,:0.0967 and:0.0718 the:0.0525)
		(
:0.2031 to:0.0537 the:0.0574-:0.0137,:0.0991,:0.0996 and:0.0708 the:0.0576)
		(
:0.2012 to:0.0481 the:0.0520-:0.0114,:0.1055,:0.1030 and:0.0708 the:0.0559)
		(
:0.2109 to:0.0564 the:0.0618-:0.0117,:0.1045,:0.1045 and:0.0713 the:0.0557)
		(
:0.1982 to:0.0500 the:0.0571-:0.0100,:0.1099,:0.1055 and:0.0688 the:0.0554)
		(
:0.2031 to:0.0571 the:0.0640-:0.0107 of:0.1113,:0.1030 and:0.0708 the:0.0515)
		(
:0.1885,:0.0493 the:0.0586-:0.0093,:0.1138,:0.1045 and:0.0684 the:0.0510)
		(
:0.1885,:0.0493 the:0.0586-:0.0093,:0.1138,:0.1045 and:0.0684 the:0.0510)
 the first, and the first, and the other,
350: sample 0: Hello, I'm a language model,
------
		(,:0.0496 the:0.0781�:0.1455 a:0.0245 way:0.0070.:0.1113 of:0.1826 and:0.1138)
		(,:0.0591 the:0.0820�:0.1934 not:0.0366 child:0.0146.:0.1250 of:0.1406 the:0.0742)
		(,:0.0532 the:0.0603�:0.2871 a:0.0303 child:0.0166.:0.0986 of:0.1240 it:0.0640)
		(,:0.0347 the:0.0554�:0.2812 a:0.0396 child:0.0171.:0.1226 of:0.1118 it:0.0674)
		(,:0.0488 the:0.0908�:0.2734 the:0.0425 child:0.0190.:0.1099,:0.1055 it:0.0688)
		(,:0.0608 and:0.0918�:0.2383 the:0.0425 child:0.0177.:0.1270,:0.1118 it:0.0649)
		(,:0.0598 and:0.0996�:0.2051 the:0.0459 child:0.0188.:0.1138,:0.1162 it:0.0664)
		(,:0.0640 and:0.1064�:0.2061 the:0.0430 child:0.0182.:0.1299,:0.1196 and:0.0664)
		(,:0.0588 and:0.1025�:0.1807 the:0.0439 child:0.0193,:0.1191,:0.1240 and:0.0674)
		(,:0.0608 and:0.1069�:0.1807 the:0.0410 child:0.0190.:0.1318,:0.1245 and:0.0669)
		(,:0.0532 and:0.1030�:0.1729 the:0.0413 child:0.0209.:0.1226,:0.1260 and:0.0693)
		(,:0.0518 and:0.1040�:0.1680 the:0.0396 child:0.0211.:0.1299,:0.1270 and:0.0703)
		(,:0.0518 and:0.1040�:0.1680 the:0.0396 child:0.0211.:0.1299,:0.1270 and:0.0703)
 and
------
		( the:0.0781�:0.1455 a:0.0245 way:0.0070.:0.1113 of:0.1826 and:0.1138 the:0.0737)
		( the:0.0820�:0.1934 not:0.0366 child:0.0146.:0.1250 of:0.1406 the:0.0742 the:0.0830)
		( the:0.0603�:0.2871 a:0.0303 child:0.0166.:0.0986 of:0.1240 it:0.0640 the:0.0605)
		( the:0.0554�:0.2812 a:0.0396 child:0.0171.:0.1226 of:0.1118 it:0.0674 the:0.0549)
		( the:0.0908�:0.2734 the:0.0425 child:0.0190.:0.1099,:0.1055 it:0.0688 the:0.0552)
		( and:0.0918�:0.2383 the:0.0425 child:0.0177.:0.1270,:0.1118 it:0.0649 the:0.0510)
		( and:0.0996�:0.2051 the:0.0459 child:0.0188.:0.1138,:0.1162 it:0.0664 the:0.0518)
		( and:0.1064�:0.2061 the:0.0430 child:0.0182.:0.1299,:0.1196 and:0.0664 the:0.0547)
		( and:0.1025�:0.1807 the:0.0439 child:0.0193,:0.1191,:0.1240 and:0.0674 the:0.0559)
		( and:0.1069�:0.1807 the:0.0410 child:0.0190.:0.1318,:0.1245 and:0.0669 the:0.0576)
		( and:0.1030�:0.1729 the:0.0413 child:0.0209.:0.1226,:0.1260 and:0.0693 the:0.0547)
		( and:0.1040�:0.1680 the:0.0396 child:0.0211.:0.1299,:0.1270 and:0.0703 the:0.0586)
		( and:0.1040�:0.1680 the:0.0396 child:0.0211.:0.1299,:0.1270 and:0.0703 the:0.0586)
 the most important to the same, and the same,
500: sample 0: Hello, I'm a language model,
------
		(,:0.0474 the:0.0815�:0.1201 a:0.0933 good:0.0148.:0.0913 of:0.2080 the:0.0713)
		(,:0.0291 the:0.1206�:0.2129 a:0.0532 good:0.0217.:0.1309 of:0.2617 I:0.0742)
		(.:0.0361 the:0.0977�:0.1338 a:0.0549 few:0.0237.:0.1143 of:0.2812 I:0.0732)
		(.:0.0244 the:0.1035�:0.1025 a:0.0559 few:0.0262.:0.1133 of:0.2793 I:0.0928)
		(.:0.0391 the:0.1089�:0.0859 a:0.0574 few:0.0337.:0.1094 of:0.2891 I:0.0913)
		(.:0.0281 the:0.1079�:0.0728 a:0.0549 few:0.0332.:0.1079 of:0.2930 I:0.1001)
		(.:0.0439 the:0.1060�:0.0623 in:0.0542 few:0.0352.:0.1069 of:0.3086 I:0.0981)
		(.:0.0342 the:0.0962�:0.0554 in:0.0532 few:0.0315.:0.1040 of:0.3164 I:0.0986)
		(.:0.0413 the:0.0996 have:0.0549 in:0.0576 few:0.0300.:0.1035 of:0.3242 I:0.0962)
		(.:0.0292 the:0.0894 have:0.0552 in:0.0559 few:0.0271.:0.0996 of:0.3262 I:0.0942)
		(.:0.0273 the:0.0933 have:0.0574 in:0.0601 few:0.0256.:0.0962 of:0.3262 I:0.0889)
		(.:0.0200 the:0.0884 have:0.0569 in:0.0583 few:0.0242.:0.0938 of:0.3223 I:0.0840)
		(.:0.0200 the:0.0884 have:0.0569 in:0.0583 few:0.0242.:0.0938 of:0.3223 I:0.0840)
 I
------
		( the:0.0815�:0.1201 a:0.0933 good:0.0148.:0.0913 of:0.2080 the:0.0713�:0.0771)
		( the:0.1206�:0.2129 a:0.0532 good:0.0217.:0.1309 of:0.2617 I:0.0742�:0.0908)
		( the:0.0977�:0.1338 a:0.0549 few:0.0237.:0.1143 of:0.2812 I:0.0732 have:0.0762)
		( the:0.1035�:0.1025 a:0.0559 few:0.0262.:0.1133 of:0.2793 I:0.0928 have:0.0679)
		( the:0.1089�:0.0859 a:0.0574 few:0.0337.:0.1094 of:0.2891 I:0.0913 have:0.0684)
		( the:0.1079�:0.0728 a:0.0549 few:0.0332.:0.1079 of:0.2930 I:0.1001 have:0.0669)
		( the:0.1060�:0.0623 in:0.0542 few:0.0352.:0.1069 of:0.3086 I:0.0981 have:0.0713)
		( the:0.0962�:0.0554 in:0.0532 few:0.0315.:0.1040 of:0.3164 I:0.0986 have:0.0742)
		( the:0.0996 have:0.0549 in:0.0576 few:0.0300.:0.1035 of:0.3242 I:0.0962 have:0.0776)
		( the:0.0894 have:0.0552 in:0.0559 few:0.0271.:0.0996 of:0.3262 I:0.0942 have:0.0791)
		( the:0.0933 have:0.0574 in:0.0601 few:0.0256.:0.0962 of:0.3262 I:0.0889 have:0.0806)
		( the:0.0884 have:0.0569 in:0.0583 few:0.0242.:0.0938 of:0.3223 I:0.0840 have:0.0815)
		( the:0.0884 have:0.0569 in:0.0583 few:0.0242.:0.0938 of:0.3223 I:0.0840 have:0.0815)
 have a lot of the same time, I have a
650: sample 0: Hello, I'm a language model,
------
		(,:0.0708 the:0.0693�:0.0786 a:0.0417 very:0.0092 of:0.0742 of:0.1328 the:0.0747)
		(,:0.1235 the:0.1270�:0.1143 not:0.0238 good:0.0184 of:0.1396 of:0.1816 and:0.0869)
		(,:0.0781 the:0.1436�:0.1001 in:0.0273 very:0.0262.:0.1455 of:0.1221 and:0.1094)
		(,:0.0640 the:0.1523�:0.0742 going:0.0308 good:0.0231.:0.1641 of:0.1133 and:0.1226)
		(,:0.0708 the:0.1592�:0.0674 going:0.0337 good:0.0242.:0.1650.:0.1074 and:0.1416)
		(,:0.0664 the:0.1523�:0.0583 going:0.0376 few:0.0244.:0.1680.:0.1104 and:0.1514)
		(,:0.0723 the:0.1484�:0.0527 going:0.0371 few:0.0292.:0.1621.:0.1089 and:0.1514)
		(,:0.0776 the:0.1416 was:0.0598 going:0.0383 few:0.0327.:0.1592.:0.1099 and:0.1572)
		(,:0.0815 the:0.1387 was:0.0693 going:0.0381 few:0.0352.:0.1514 for:0.1089 and:0.1543)
		(,:0.0898 the:0.1387 was:0.0791 going:0.0393 few:0.0352.:0.1445 for:0.1108 and:0.1553)
		(,:0.0933 the:0.1387 was:0.0884 going:0.0381 few:0.0361.:0.1309 for:0.1069 and:0.1562)
		(,:0.1025 the:0.1387 was:0.0996 going:0.0383 few:0.0356.:0.1240 for:0.1069 and:0.1523)
		(,:0.1025 the:0.1387 was:0.0996 going:0.0383 few:0.0356.:0.1240 for:0.1069 and:0.1523)
 and
------
		( the:0.0693�:0.0786 a:0.0417 very:0.0092 of:0.0742 of:0.1328 the:0.0747 the:0.1016)
		( the:0.1270�:0.1143 not:0.0238 good:0.0184 of:0.1396 of:0.1816 and:0.0869 the:0.1113)
		( the:0.1436�:0.1001 in:0.0273 very:0.0262.:0.1455 of:0.1221 and:0.1094 the:0.1030)
		( the:0.1523�:0.0742 going:0.0308 good:0.0231.:0.1641 of:0.1133 and:0.1226 the:0.0942)
		( the:0.1592�:0.0674 going:0.0337 good:0.0242.:0.1650.:0.1074 and:0.1416 the:0.0972)
		( the:0.1523�:0.0583 going:0.0376 few:0.0244.:0.1680.:0.1104 and:0.1514 the:0.0850)
		( the:0.1484�:0.0527 going:0.0371 few:0.0292.:0.1621.:0.1089 and:0.1514 the:0.0869)
		( the:0.1416 was:0.0598 going:0.0383 few:0.0327.:0.1592.:0.1099 and:0.1572 the:0.0796)
		( the:0.1387 was:0.0693 going:0.0381 few:0.0352.:0.1514 for:0.1089 and:0.1543 the:0.0796)
		( the:0.1387 was:0.0791 going:0.0393 few:0.0352.:0.1445 for:0.1108 and:0.1553 the:0.0762)
		( the:0.1387 was:0.0884 going:0.0381 few:0.0361.:0.1309 for:0.1069 and:0.1562 the:0.0742)
		( the:0.1387 was:0.0996 going:0.0383 few:0.0356.:0.1240 for:0.1069 and:0.1523 the:0.0723)
		( the:0.1387 was:0.0996 going:0.0383 few:0.0356.:0.1240 for:0.1069 and:0.1523 the:0.0723)
 the same, and the same, and the same,
800: sample 0: Hello, I'm a language model,
------
		(,:0.0845 the:0.0830�:0.0630 a:0.0588 �:0.0101 of:0.1011 of:0.1079 the:0.0596)
		(,:0.1250 the:0.1245�:0.0625 a:0.0280 good:0.0231 of:0.1465.:0.0923 I:0.1357)
		(,:0.0664 the:0.1260�:0.0640 not:0.0420 good:0.0289 of:0.1396.:0.1089 I:0.1455)
		(,:0.0640 the:0.1064 have:0.0574 not:0.0403 good:0.0334 of:0.1406.:0.1147 I:0.1797)
		(,:0.0620 the:0.1118 have:0.0601 not:0.0457 good:0.0349 of:0.1260.:0.1152 I:0.1885)
		(,:0.0562 the:0.1011 have:0.0635 not:0.0439 good:0.0342 of:0.1191.:0.1172 I:0.2041)
		(,:0.0581 the:0.0972 have:0.0659 not:0.0464 good:0.0349.:0.1118.:0.1157 I:0.2051)
		(,:0.0564 the:0.0928 have:0.0664 not:0.0461 good:0.0339.:0.1108.:0.1147 I:0.2148)
		(,:0.0605 the:0.0903 have:0.0693 not:0.0493 good:0.0359.:0.1055.:0.1138 I:0.2178)
		(,:0.0664 the:0.0859 have:0.0708 not:0.0503 good:0.0364.:0.1025.:0.1113 I:0.2285)
		(,:0.0698 the:0.0815 have:0.0737 not:0.0515 good:0.0369.:0.0962.:0.1099 I:0.2314)
		(,:0.0850 the:0.0801 have:0.0767 not:0.0518 good:0.0364 of:0.0938.:0.1025 I:0.2373)
		(,:0.0850 the:0.0801 have:0.0767 not:0.0518 good:0.0364 of:0.0938.:0.1025 I:0.2373)
 I
------
		( the:0.0830�:0.0630 a:0.0588 �:0.0101 of:0.1011 of:0.1079 the:0.0596�:0.0811)
		( the:0.1245�:0.0625 a:0.0280 good:0.0231 of:0.1465.:0.0923 I:0.1357 have:0.0762)
		( the:0.1260�:0.0640 not:0.0420 good:0.0289 of:0.1396.:0.1089 I:0.1455 have:0.0762)
		( the:0.1064 have:0.0574 not:0.0403 good:0.0334 of:0.1406.:0.1147 I:0.1797 am:0.0889)
		( the:0.1118 have:0.0601 not:0.0457 good:0.0349 of:0.1260.:0.1152 I:0.1885 am:0.0923)
		( the:0.1011 have:0.0635 not:0.0439 good:0.0342 of:0.1191.:0.1172 I:0.2041 am:0.0957)
		( the:0.0972 have:0.0659 not:0.0464 good:0.0349.:0.1118.:0.1157 I:0.2051 have:0.0947)
		( the:0.0928 have:0.0664 not:0.0461 good:0.0339.:0.1108.:0.1147 I:0.2148 have:0.1006)
		( the:0.0903 have:0.0693 not:0.0493 good:0.0359.:0.1055.:0.1138 I:0.2178 have:0.1045)
		( the:0.0859 have:0.0708 not:0.0503 good:0.0364.:0.1025.:0.1113 I:0.2285 have:0.1055)
		( the:0.0815 have:0.0737 not:0.0515 good:0.0369.:0.0962.:0.1099 I:0.2314 have:0.1045)
		( the:0.0801 have:0.0767 not:0.0518 good:0.0364 of:0.0938.:0.1025 I:0.2373 have:0.1045)
		( the:0.0801 have:0.0767 not:0.0518 good:0.0364 of:0.0938.:0.1025 I:0.2373 have:0.1045)
 have a lot of the first, I have to do
950: sample 0: Hello, I'm a language model,
------
		(,:0.0610 the:0.0649�:0.0354 a:0.0515 good:0.0126,:0.1089,:0.1104 and:0.0535)
		(,:0.0840 the:0.1177 have:0.0505 in:0.0476 very:0.0231.:0.1123.:0.1240 and:0.0938)
		(,:0.0571 the:0.1104 am:0.0659 in:0.0669 very:0.0259.:0.1240.:0.1465 and:0.1377)
		(,:0.0806 the:0.1079 am:0.0645 in:0.0562 little:0.0325.:0.1172.:0.1445 and:0.1436)
		(,:0.0879 the:0.1079 am:0.0806 not:0.0603 little:0.0364.:0.1147.:0.1455 and:0.1641)
		(,:0.0771 the:0.1050 am:0.0796 not:0.0625 little:0.0396.:0.1147.:0.1514 and:0.1729)
		(,:0.0952 the:0.1079 am:0.0874 not:0.0693 little:0.0403.:0.1152.:0.1523 and:0.1816)
		(,:0.0933 the:0.1074 am:0.0879 not:0.0718 little:0.0410.:0.1123.:0.1553 and:0.1816)
		(,:0.1099 the:0.1104 am:0.0908 not:0.0752 little:0.0398.:0.1104.:0.1572 and:0.1865)
		(,:0.1157 the:0.1099 am:0.0884 not:0.0762 little:0.0386.:0.1089.:0.1533 and:0.1816)
		(,:0.1250 the:0.1094 am:0.0889 not:0.0786 little:0.0378.:0.1055.:0.1523 and:0.1777)
		(,:0.1357 the:0.1064 am:0.0874 not:0.0791 little:0.0359.:0.1050.:0.1475 and:0.1699)
		(,:0.1357 the:0.1064 am:0.0874 not:0.0791 little:0.0359.:0.1050.:0.1475 and:0.1699)
 and
------
		( the:0.0649�:0.0354 a:0.0515 good:0.0126,:0.1089,:0.1104 and:0.0535 the:0.0869)
		( the:0.1177 have:0.0505 in:0.0476 very:0.0231.:0.1123.:0.1240 and:0.0938 I:0.0684)
		( the:0.1104 am:0.0659 in:0.0669 very:0.0259.:0.1240.:0.1465 and:0.1377 the:0.0728)
		( the:0.1079 am:0.0645 in:0.0562 little:0.0325.:0.1172.:0.1445 and:0.1436 the:0.0713)
		( the:0.1079 am:0.0806 not:0.0603 little:0.0364.:0.1147.:0.1455 and:0.1641 the:0.0698)
		( the:0.1050 am:0.0796 not:0.0625 little:0.0396.:0.1147.:0.1514 and:0.1729 the:0.0747)
		( the:0.1079 am:0.0874 not:0.0693 little:0.0403.:0.1152.:0.1523 and:0.1816 the:0.0728)
		( the:0.1074 am:0.0879 not:0.0718 little:0.0410.:0.1123.:0.1553 and:0.1816 the:0.0757)
		( the:0.1104 am:0.0908 not:0.0752 little:0.0398.:0.1104.:0.1572 and:0.1865 the:0.0747)
		( the:0.1099 am:0.0884 not:0.0762 little:0.0386.:0.1089.:0.1533 and:0.1816 the:0.0752)
		( the:0.1094 am:0.0889 not:0.0786 little:0.0378.:0.1055.:0.1523 and:0.1777 the:0.0737)
		( the:0.1064 am:0.0874 not:0.0791 little:0.0359.:0.1050.:0.1475 and:0.1699 the:0.0723)
		( the:0.1064 am:0.0874 not:0.0791 little:0.0359.:0.1050.:0.1475 and:0.1699 the:0.0723)
 the language.
The first language is a language that
1100: sample 0: Hello, I'm a language model,
------
		(,:0.0596 the:0.0649 was:0.0447 a:0.0535 good:0.0188,:0.0879,:0.1035 and:0.0605)
		(,:0.0574 the:0.1016 was:0.0613 a:0.0530 good:0.0344 that:0.0898.:0.1089 I:0.1309)
		(,:0.0376 the:0.0806 would:0.0708 a:0.0713 good:0.0371 that:0.0938.:0.1367 and:0.1250)
		(.:0.0605 the:0.0835 would:0.0669 a:0.0664 good:0.0400 that:0.1016.:0.1416 and:0.1406)
		(,:0.0630 the:0.0859�:0.0674 sure:0.0723 good:0.0410 that:0.1011.:0.1426 and:0.1572)
		(.:0.0591 the:0.0898�:0.0674 sure:0.0752 little:0.0449 that:0.1104.:0.1523 and:0.1709)
		(,:0.0728 the:0.0903�:0.0674 a:0.0781 little:0.0454 that:0.1172.:0.1533 and:0.1797)
		(.:0.0791 the:0.0913�:0.0659 a:0.0835 little:0.0459 that:0.1245.:0.1582 and:0.1816)
		(.:0.0869 the:0.0918�:0.0640 a:0.0898 little:0.0454 that:0.1299.:0.1582 and:0.1836)
		(.:0.0957 the:0.0903 was:0.0640 a:0.0972 good:0.0437 that:0.1328.:0.1582 and:0.1777)
		(.:0.1040 the:0.0894 was:0.0654 a:0.1030 few:0.0464 that:0.1338.:0.1562 and:0.1758)
		(.:0.1118 the:0.0864 was:0.0659 a:0.1069 few:0.0496 that:0.1309.:0.1592 and:0.1699)
		(.:0.1118 the:0.0864 was:0.0659 a:0.1069 few:0.0496 that:0.1309.:0.1592 and:0.1699)
 and
------
		( the:0.0649 was:0.0447 a:0.0535 good:0.0188,:0.0879,:0.1035 and:0.0605 the:0.0615)
		( the:0.1016 was:0.0613 a:0.0530 good:0.0344 that:0.0898.:0.1089 I:0.1309 I:0.0732)
		( the:0.0806 would:0.0708 a:0.0713 good:0.0371 that:0.0938.:0.1367 and:0.1250 I:0.1006)
		( the:0.0835 would:0.0669 a:0.0664 good:0.0400 that:0.1016.:0.1416 and:0.1406 I:0.0845)
		( the:0.0859�:0.0674 sure:0.0723 good:0.0410 that:0.1011.:0.1426 and:0.1572 I:0.0942)
		( the:0.0898�:0.0674 sure:0.0752 little:0.0449 that:0.1104.:0.1523 and:0.1709 I:0.0898)
		( the:0.0903�:0.0674 a:0.0781 little:0.0454 that:0.1172.:0.1533 and:0.1797 I:0.0908)
		( the:0.0913�:0.0659 a:0.0835 little:0.0459 that:0.1245.:0.1582 and:0.1816 I:0.0894)
		( the:0.0918�:0.0640 a:0.0898 little:0.0454 that:0.1299.:0.1582 and:0.1836 I:0.0918)
		( the:0.0903 was:0.0640 a:0.0972 good:0.0437 that:0.1328.:0.1582 and:0.1777 I:0.0889)
		( the:0.0894 was:0.0654 a:0.1030 few:0.0464 that:0.1338.:0.1562 and:0.1758 I:0.0869)
		( the:0.0864 was:0.0659 a:0.1069 few:0.0496 that:0.1309.:0.1592 and:0.1699 I:0.0874)
		( the:0.0864 was:0.0659 a:0.1069 few:0.0496 that:0.1309.:0.1592 and:0.1699 I:0.0874)
 I'm a lot of the most important thing to do
1250: sample 0: Hello, I'm a language model,
------
		(,:0.1035 the:0.0442 was:0.0947 a:0.0330 good:0.0276,:0.0535.:0.1196 and:0.0654)
		(,:0.0679 the:0.0659 was:0.1328 a:0.0420 good:0.0376.:0.0718.:0.2217 and:0.1309)
		(,:0.0569 the:0.0576 was:0.1309 a:0.0510 good:0.0342.:0.0889.:0.2656 and:0.1738)
		(,:0.0547 the:0.0630 was:0.1099 a:0.0435 good:0.0330.:0.1021.:0.2578 and:0.1748)
		(,:0.0894 the:0.0613 was:0.1123 a:0.0457 great:0.0347.:0.1055.:0.2520 and:0.1816)
		(,:0.0728 the:0.0605 was:0.1177 a:0.0466 great:0.0374.:0.1177.:0.2578 and:0.1904)
		(,:0.1162 the:0.0601 was:0.1221 a:0.0498 great:0.0408.:0.1221.:0.2617 and:0.1982)
		(,:0.1035 the:0.0579 was:0.1289 a:0.0527 great:0.0417.:0.1328.:0.2637 and:0.2002)
		(,:0.1309 the:0.0554 was:0.1338 a:0.0557 great:0.0430.:0.1318.:0.2676 and:0.2041)
		(,:0.1235 the:0.0520 was:0.1348 a:0.0559 great:0.0417.:0.1357.:0.2695 and:0.1982)
		(,:0.1279 the:0.0500 was:0.1406 a:0.0564 great:0.0417.:0.1328.:0.2617 and:0.1953)
		(,:0.1230 the:0.0457 was:0.1426 a:0.0566 great:0.0396.:0.1309.:0.2617 and:0.1914)
		(,:0.1230 the:0.0457 was:0.1426 a:0.0566 great:0.0396.:0.1309.:0.2617 and:0.1914)
 and
------
		( the:0.0442 was:0.0947 a:0.0330 good:0.0276,:0.0535.:0.1196 and:0.0654 the:0.0679)
		( the:0.0659 was:0.1328 a:0.0420 good:0.0376.:0.0718.:0.2217 and:0.1309 the:0.0908)
		( the:0.0576 was:0.1309 a:0.0510 good:0.0342.:0.0889.:0.2656 and:0.1738 a:0.1118)
		( the:0.0630 was:0.1099 a:0.0435 good:0.0330.:0.1021.:0.2578 and:0.1748 I:0.1436)
		( the:0.0613 was:0.1123 a:0.0457 great:0.0347.:0.1055.:0.2520 and:0.1816 I:0.1621)
		( the:0.0605 was:0.1177 a:0.0466 great:0.0374.:0.1177.:0.2578 and:0.1904 I:0.1660)
		( the:0.0601 was:0.1221 a:0.0498 great:0.0408.:0.1221.:0.2617 and:0.1982 I:0.1562)
		( the:0.0579 was:0.1289 a:0.0527 great:0.0417.:0.1328.:0.2637 and:0.2002 I:0.1445)
		( the:0.0554 was:0.1338 a:0.0557 great:0.0430.:0.1318.:0.2676 and:0.2041 I:0.1289)
		( the:0.0520 was:0.1348 a:0.0559 great:0.0417.:0.1357.:0.2695 and:0.1982 I:0.1152)
		( the:0.0500 was:0.1406 a:0.0564 great:0.0417.:0.1328.:0.2617 and:0.1953 a:0.1011)
		( the:0.0457 was:0.1426 a:0.0566 great:0.0396.:0.1309.:0.2617 and:0.1914 a:0.0972)
		( the:0.0457 was:0.1426 a:0.0566 great:0.0396.:0.1309.:0.2617 and:0.1914 a:0.0972)
 a language.
- The language is the language that
1400: sample 0: Hello, I'm a language model,
------
		(,:0.0586 the:0.0356 was:0.0635 a:0.0276 good:0.0160.:0.0566.:0.1543 and:0.0786)
		(.:0.0408 the:0.0586 was:0.0703 a:0.0393 good:0.0270.:0.0674.:0.2344 and:0.1650)
		(.:0.0464 we:0.0535 will:0.0771 a:0.0481 great:0.0339.:0.0703.:0.2451 and:0.1992)
		(.:0.0640 the:0.0610 will:0.0786 a:0.0491 great:0.0400.:0.0840.:0.2539 and:0.2051)
		(,:0.0815 we:0.0630 will:0.0820 a:0.0515 great:0.0413.:0.0864.:0.2412 and:0.2109)
		(.:0.0859 the:0.0630 will:0.0806 a:0.0542 great:0.0430.:0.0952.:0.2451 and:0.2080)
		(.:0.1025 the:0.0640 will:0.0771 a:0.0591 great:0.0444.:0.1016.:0.2480 and:0.2012)
		(.:0.1060 the:0.0635 will:0.0737 a:0.0630 great:0.0449.:0.1084.:0.2441 and:0.1992)
		(.:0.1147 the:0.0613 will:0.0688 a:0.0659 great:0.0442.:0.1143.:0.2441 and:0.1875)
		(.:0.1172 the:0.0593 will:0.0625 a:0.0688 great:0.0437.:0.1167.:0.2412 and:0.1826)
		(.:0.1201 the:0.0576�:0.0630 a:0.0723 great:0.0417.:0.1201.:0.2383 and:0.1738)
		(.:0.1201 the:0.0544�:0.0635 a:0.0718 great:0.0393.:0.1216.:0.2383 and:0.1650)
		(.:0.1201 the:0.0544�:0.0635 a:0.0718 great:0.0393.:0.1216.:0.2383 and:0.1650)
 and
------
		( the:0.0356 was:0.0635 a:0.0276 good:0.0160.:0.0566.:0.1543 and:0.0786 the:0.0791)
		( the:0.0586 was:0.0703 a:0.0393 good:0.0270.:0.0674.:0.2344 and:0.1650 I:0.1064)
		( we:0.0535 will:0.0771 a:0.0481 great:0.0339.:0.0703.:0.2451 and:0.1992 I:0.1001)
		( the:0.0610 will:0.0786 a:0.0491 great:0.0400.:0.0840.:0.2539 and:0.2051 I:0.1177)
		( we:0.0630 will:0.0820 a:0.0515 great:0.0413.:0.0864.:0.2412 and:0.2109 I:0.1279)
		( the:0.0630 will:0.0806 a:0.0542 great:0.0430.:0.0952.:0.2451 and:0.2080 I:0.1279)
		( the:0.0640 will:0.0771 a:0.0591 great:0.0444.:0.1016.:0.2480 and:0.2012 I:0.1221)
		( the:0.0635 will:0.0737 a:0.0630 great:0.0449.:0.1084.:0.2441 and:0.1992 I:0.1152)
		( the:0.0613 will:0.0688 a:0.0659 great:0.0442.:0.1143.:0.2441 and:0.1875 I:0.1074)
		( the:0.0593 will:0.0625 a:0.0688 great:0.0437.:0.1167.:0.2412 and:0.1826 I:0.1011)
		( the:0.0576�:0.0630 a:0.0723 great:0.0417.:0.1201.:0.2383 and:0.1738 the:0.0962)
		( the:0.0544�:0.0635 a:0.0718 great:0.0393.:0.1216.:0.2383 and:0.1650 the:0.0972)
		( the:0.0544�:0.0635 a:0.0718 great:0.0393.:0.1216.:0.2383 and:0.1650 the:0.0972)
 the language is the language of the language.
The
1550: sample 0: Hello, I'm a language model,
------
		(.:0.0579 I:0.0552 was:0.0796 a:0.0447 good:0.0162 that:0.0618.:0.1533 and:0.0762)
		(
:0.0388 I:0.0884 was:0.0889 a:0.0635 good:0.0162.:0.0640.:0.2070 and:0.1631)
		(.:0.0669 I:0.0918 was:0.0796 a:0.0610 teacher:0.0226.:0.0718.:0.2051 and:0.1748)
		(.:0.0986 I:0.0869 was:0.0645 a:0.0613 great:0.0275.:0.0918.:0.2256 and:0.1982)
		(.:0.0981 I:0.0913 was:0.0640 a:0.0605 great:0.0299.:0.0898.:0.2158 and:0.2051)
		(.:0.0977 I:0.0903 was:0.0630 a:0.0625 great:0.0325.:0.0938.:0.2158 and:0.2158)
		(.:0.1040 I:0.0913 was:0.0635 a:0.0669 great:0.0337.:0.0962.:0.2080 and:0.2217)
		(.:0.1045 I:0.0903 was:0.0640 a:0.0693 great:0.0347.:0.0981.:0.2051 and:0.2285)
		(.:0.1050 I:0.0874 was:0.0630 a:0.0693 great:0.0347.:0.0986.:0.2002 and:0.2305)
		(.:0.1030 I:0.0845 was:0.0603 a:0.0698 great:0.0337.:0.0991.:0.1963 and:0.2324)
		(.:0.1011 I:0.0820 was:0.0596 a:0.0718 great:0.0327.:0.0972.:0.1943 and:0.2402)
		(.:0.0996 I:0.0796 was:0.0571 a:0.0693 great:0.0310.:0.0957.:0.1875 and:0.2363)
		(.:0.0996 I:0.0796 was:0.0571 a:0.0693 great:0.0310.:0.0957.:0.1875 and:0.2363)
 and
------
		( I:0.0552 was:0.0796 a:0.0447 good:0.0162 that:0.0618.:0.1533 and:0.0762 the:0.1108)
		( I:0.0884 was:0.0889 a:0.0635 good:0.0162.:0.0640.:0.2070 and:0.1631 I:0.0977)
		( I:0.0918 was:0.0796 a:0.0610 teacher:0.0226.:0.0718.:0.2051 and:0.1748 I:0.0850)
		( I:0.0869 was:0.0645 a:0.0613 great:0.0275.:0.0918.:0.2256 and:0.1982 I:0.1260)
		( I:0.0913 was:0.0640 a:0.0605 great:0.0299.:0.0898.:0.2158 and:0.2051 I:0.1553)
		( I:0.0903 was:0.0630 a:0.0625 great:0.0325.:0.0938.:0.2158 and:0.2158 I:0.1641)
		( I:0.0913 was:0.0635 a:0.0669 great:0.0337.:0.0962.:0.2080 and:0.2217 I:0.1631)
		( I:0.0903 was:0.0640 a:0.0693 great:0.0347.:0.0981.:0.2051 and:0.2285 I:0.1641)
		( I:0.0874 was:0.0630 a:0.0693 great:0.0347.:0.0986.:0.2002 and:0.2305 I:0.1562)
		( I:0.0845 was:0.0603 a:0.0698 great:0.0337.:0.0991.:0.1963 and:0.2324 I:0.1533)
		( I:0.0820 was:0.0596 a:0.0718 great:0.0327.:0.0972.:0.1943 and:0.2402 I:0.1484)
		( I:0.0796 was:0.0571 a:0.0693 great:0.0310.:0.0957.:0.1875 and:0.2363 I:0.1387)
		( I:0.0796 was:0.0571 a:0.0693 great:0.0310.:0.0957.:0.1875 and:0.2363 I:0.1387)
 I'm a very good idea to get a better understanding
1700: sample 0: Hello, I'm a language model,
------
		(.:0.0322 the:0.0481 was:0.0659 a:0.0292 good:0.0254 language:0.0918.:0.1226 and:0.0645)
		(
:0.0408 the:0.0732 was:0.0757 a:0.0366 sure:0.0198 language:0.0544.:0.1865 and:0.2012)
		(.:0.0613 you:0.0620 was:0.0708 in:0.0369 great:0.0217.:0.0693.:0.2373 and:0.2148)
		(.:0.0723 the:0.0830 was:0.0659 in:0.0432 great:0.0222.:0.0938.:0.2773 and:0.2217)
		(,:0.0811 the:0.0811 was:0.0684 in:0.0417 great:0.0232.:0.0957.:0.2891 and:0.2256)
		(,:0.0776 the:0.0820 was:0.0693 in:0.0435 great:0.0236.:0.1045.:0.2988 and:0.2227)
		(,:0.0854 the:0.0835 was:0.0708 in:0.0449 great:0.0236.:0.1108.:0.3086 and:0.2256)
		(,:0.0854 the:0.0850 was:0.0723 in:0.0493 great:0.0228.:0.1187.:0.3145 and:0.2295)
		(,:0.0859 the:0.0840 was:0.0742 in:0.0510 great:0.0222.:0.1270.:0.3145 and:0.2285)
		(,:0.0845 the:0.0840 was:0.0742 in:0.0547 great:0.0216.:0.1318.:0.3184 and:0.2227)
		(,:0.0850 the:0.0840 was:0.0767 in:0.0566 great:0.0211.:0.1328.:0.3203 and:0.2227)
		(,:0.0811 the:0.0815 was:0.0771 in:0.0583 great:0.0199.:0.1348.:0.3262 and:0.2168)
		(,:0.0811 the:0.0815 was:0.0771 in:0.0583 great:0.0199.:0.1348.:0.3262 and:0.2168)
 and
------
		( the:0.0481 was:0.0659 a:0.0292 good:0.0254 language:0.0918.:0.1226 and:0.0645 the:0.1582)
		( the:0.0732 was:0.0757 a:0.0366 sure:0.0198 language:0.0544.:0.1865 and:0.2012 I:0.1309)
		( you:0.0620 was:0.0708 in:0.0369 great:0.0217.:0.0693.:0.2373 and:0.2148 I:0.1426)
		( the:0.0830 was:0.0659 in:0.0432 great:0.0222.:0.0938.:0.2773 and:0.2217 I:0.1553)
		( the:0.0811 was:0.0684 in:0.0417 great:0.0232.:0.0957.:0.2891 and:0.2256 I:0.1660)
		( the:0.0820 was:0.0693 in:0.0435 great:0.0236.:0.1045.:0.2988 and:0.2227 I:0.1641)
		( the:0.0835 was:0.0708 in:0.0449 great:0.0236.:0.1108.:0.3086 and:0.2256 I:0.1553)
		( the:0.0850 was:0.0723 in:0.0493 great:0.0228.:0.1187.:0.3145 and:0.2295 I:0.1484)
		( the:0.0840 was:0.0742 in:0.0510 great:0.0222.:0.1270.:0.3145 and:0.2285 I:0.1416)
		( the:0.0840 was:0.0742 in:0.0547 great:0.0216.:0.1318.:0.3184 and:0.2227 I:0.1299)
		( the:0.0840 was:0.0767 in:0.0566 great:0.0211.:0.1328.:0.3203 and:0.2227 I:0.1226)
		( the:0.0815 was:0.0771 in:0.0583 great:0.0199.:0.1348.:0.3262 and:0.2168 I:0.1152)
		( the:0.0815 was:0.0771 in:0.0583 great:0.0199.:0.1348.:0.3262 and:0.2168 I:0.1152)
 I'm not interested in the language.
I'm
1850: sample 0: Hello, I'm a language model,
------
		(.:0.0452 the:0.0442 can:0.0889 a:0.0635 good:0.0330 that:0.0869,:0.1602 I:0.1182)
		(.:0.0381 we:0.0640 will:0.0972 a:0.0583 very:0.0244 that:0.0503.:0.1602 and:0.1553)
		(.:0.0630 we:0.0732 will:0.0815 a:0.0488 kid:0.0352 writer:0.0574.:0.1768 and:0.1465)
		(.:0.0610 we:0.0879 will:0.0654 a:0.0476 kid:0.0439 writer:0.0811.:0.1982 and:0.1367)
		(.:0.0593 we:0.0908 am:0.0688 a:0.0476 kid:0.0476 writer:0.0918.:0.2051 and:0.1260)
		(.:0.0654 we:0.0962 am:0.0732 a:0.0525 kid:0.0476 writer:0.0962.:0.2100 and:0.1206)
		(.:0.0688 we:0.1016 am:0.0781 a:0.0583 kid:0.0447 writer:0.0967.:0.2129 and:0.1172)
		(.:0.0688 we:0.1040 am:0.0796 a:0.0649 kid:0.0422 writer:0.0996.:0.2139 and:0.1162)
		(.:0.0693 we:0.1069 am:0.0830 a:0.0698 kid:0.0398 writer:0.1001.:0.2100 and:0.1133)
		(.:0.0688 we:0.1074 am:0.0850 a:0.0776 kid:0.0364 writer:0.1006.:0.2129 and:0.1108)
		(.:0.0645 we:0.1079 am:0.0869 a:0.0815 kid:0.0342 writer:0.0986.:0.2119 and:0.1045)
		(.:0.0610 we:0.1084 am:0.0884 a:0.0879 kid:0.0312 writer:0.0991.:0.2100 and:0.1016)
		(.:0.0610 we:0.1084 am:0.0884 a:0.0879 kid:0.0312 writer:0.0991.:0.2100 and:0.1016)
 and
------
		( the:0.0442 can:0.0889 a:0.0635 good:0.0330 that:0.0869,:0.1602 I:0.1182 the:0.1660)
		( we:0.0640 will:0.0972 a:0.0583 very:0.0244 that:0.0503.:0.1602 and:0.1553 I:0.2637)
		( we:0.0732 will:0.0815 a:0.0488 kid:0.0352 writer:0.0574.:0.1768 and:0.1465 I:0.2061)
		( we:0.0879 will:0.0654 a:0.0476 kid:0.0439 writer:0.0811.:0.1982 and:0.1367 I:0.2139)
		( we:0.0908 am:0.0688 a:0.0476 kid:0.0476 writer:0.0918.:0.2051 and:0.1260 I:0.2129)
		( we:0.0962 am:0.0732 a:0.0525 kid:0.0476 writer:0.0962.:0.2100 and:0.1206 I:0.2080)
		( we:0.1016 am:0.0781 a:0.0583 kid:0.0447 writer:0.0967.:0.2129 and:0.1172 I:0.1992)
		( we:0.1040 am:0.0796 a:0.0649 kid:0.0422 writer:0.0996.:0.2139 and:0.1162 I:0.1924)
		( we:0.1069 am:0.0830 a:0.0698 kid:0.0398 writer:0.1001.:0.2100 and:0.1133 I:0.1816)
		( we:0.1074 am:0.0850 a:0.0776 kid:0.0364 writer:0.1006.:0.2129 and:0.1108 I:0.1729)
		( we:0.1079 am:0.0869 a:0.0815 kid:0.0342 writer:0.0986.:0.2119 and:0.1045 I:0.1641)
		( we:0.1084 am:0.0884 a:0.0879 kid:0.0312 writer:0.0991.:0.2100 and:0.1016 I:0.1562)
		( we:0.1084 am:0.0884 a:0.0879 kid:0.0312 writer:0.0991.:0.2100 and:0.1016 I:0.1562)
 I'm a scientist. I'm a scientist, I
2000: sample 0: Hello, I'm a language model,
------
		(.:0.0430 and:0.0552 have:0.0938 going:0.0356 good:0.0461 that:0.0713.:0.1455 I:0.0898)
		(.:0.0378 the:0.0889 have:0.0928 going:0.0723 very:0.0393 teacher:0.0791.:0.1924 and:0.2383)
		(.:0.0791 you:0.0737�:0.0869 going:0.0757 very:0.0310 teacher:0.1187.:0.2012 and:0.2188)
		(.:0.0562 the:0.0991�:0.1191 going:0.0898 very:0.0266 teacher:0.1338.:0.1953 and:0.2178)
		(�:0.0430 the:0.0854�:0.1465 going:0.0933 kid:0.0231 teacher:0.1465.:0.1953 and:0.2041)
		(�:0.0635 the:0.0820�:0.1553 going:0.0923 kid:0.0222 teacher:0.1533.:0.1953 and:0.2021)
		(�:0.0781 the:0.0811�:0.1641 going:0.0894 kid:0.0211 teacher:0.1621.:0.2021 and:0.2070)
		(�:0.0840 the:0.0801�:0.1699 going:0.0879 kid:0.0194 teacher:0.1680.:0.2031 and:0.2070)
		(�:0.0869 the:0.0796�:0.1748 going:0.0830 kid:0.0178 teacher:0.1738.:0.2021 and:0.2061)
		(�:0.0879 the:0.0796�:0.1777 going:0.0791 kid:0.0164 teacher:0.1758.:0.2002 and:0.2061)
		(�:0.0894 the:0.0771�:0.1797 going:0.0776 kid:0.0150 teacher:0.1777.:0.2012 and:0.2070)
		(�:0.0908 the:0.0771�:0.1787 going:0.0737 kid:0.0137 teacher:0.1797.:0.1992 and:0.2021)
		(�:0.0908 the:0.0771�:0.1787 going:0.0737 kid:0.0137 teacher:0.1797.:0.1992 and:0.2021)
 and
------
		( and:0.0552 have:0.0938 going:0.0356 good:0.0461 that:0.0713.:0.1455 I:0.0898 the:0.1260)
		( the:0.0889 have:0.0928 going:0.0723 very:0.0393 teacher:0.0791.:0.1924 and:0.2383 I:0.3047)
		( you:0.0737�:0.0869 going:0.0757 very:0.0310 teacher:0.1187.:0.2012 and:0.2188 I:0.3027)
		( the:0.0991�:0.1191 going:0.0898 very:0.0266 teacher:0.1338.:0.1953 and:0.2178 I:0.3555)
		( the:0.0854�:0.1465 going:0.0933 kid:0.0231 teacher:0.1465.:0.1953 and:0.2041 I:0.3652)
		( the:0.0820�:0.1553 going:0.0923 kid:0.0222 teacher:0.1533.:0.1953 and:0.2021 I:0.3730)
		( the:0.0811�:0.1641 going:0.0894 kid:0.0211 teacher:0.1621.:0.2021 and:0.2070 I:0.3555)
		( the:0.0801�:0.1699 going:0.0879 kid:0.0194 teacher:0.1680.:0.2031 and:0.2070 I:0.3535)
		( the:0.0796�:0.1748 going:0.0830 kid:0.0178 teacher:0.1738.:0.2021 and:0.2061 I:0.3379)
		( the:0.0796�:0.1777 going:0.0791 kid:0.0164 teacher:0.1758.:0.2002 and:0.2061 I:0.3242)
		( the:0.0771�:0.1797 going:0.0776 kid:0.0150 teacher:0.1777.:0.2012 and:0.2070 I:0.3262)
		( the:0.0771�:0.1787 going:0.0737 kid:0.0137 teacher:0.1797.:0.1992 and:0.2021 I:0.3145)
		( the:0.0771�:0.1787 going:0.0737 kid:0.0137 teacher:0.1797.:0.1992 and:0.2021 I:0.3145)
 I'm going to have to be a language language.
2150: sample 0: Hello, I'm a language model,
------
		(.:0.0236 and:0.0408 have:0.0889 going:0.0459 good:0.0579 that:0.1211,:0.1226 and:0.0938)
		(.:0.0198 and:0.0532 have:0.1235 going:0.0708 good:0.0391 that:0.0708.:0.1504 and:0.2373)
		(.:0.0381 and:0.0432 have:0.1016 going:0.0598 kid:0.0237 teacher:0.0864.:0.1729 and:0.1943)
		(
:0.0713 the:0.0635�:0.0952 going:0.0659 very:0.0250 teacher:0.1216.:0.1914 and:0.1592)
		(
:0.0205 the:0.0591�:0.1069 going:0.0654 kid:0.0248 teacher:0.1289.:0.2061 and:0.1455)
		(
:0.0479 the:0.0630�:0.1094 going:0.0649 kid:0.0233 teacher:0.1348.:0.2090 and:0.1426)
		(
:0.0447 the:0.0640�:0.1138 going:0.0613 kid:0.0219 teacher:0.1387.:0.2090 and:0.1416)
		(
:0.0435 the:0.0669�:0.1162 going:0.0581 very:0.0211 teacher:0.1406.:0.2100 and:0.1367)
		(
:0.0396 the:0.0688�:0.1187 going:0.0552 very:0.0209 teacher:0.1387.:0.2119 and:0.1367)
		(
:0.0376 the:0.0684�:0.1201 not:0.0540 very:0.0206 teacher:0.1396.:0.2090 and:0.1328)
		(
:0.0347 the:0.0684�:0.1206 not:0.0544 very:0.0198 teacher:0.1387.:0.2129 and:0.1289)
		( and:0.0364 the:0.0684�:0.1245 not:0.0549 very:0.0195 teacher:0.1396.:0.2109 and:0.1260)
		( and:0.0364 the:0.0684�:0.1245 not:0.0549 very:0.0195 teacher:0.1396.:0.2109 and:0.1260)
 and
------
		( and:0.0408 have:0.0889 going:0.0459 good:0.0579 that:0.1211,:0.1226 and:0.0938 I:0.0967)
		( and:0.0532 have:0.1235 going:0.0708 good:0.0391 that:0.0708.:0.1504 and:0.2373 I:0.2061)
		( and:0.0432 have:0.1016 going:0.0598 kid:0.0237 teacher:0.0864.:0.1729 and:0.1943 I:0.2188)
		( the:0.0635�:0.0952 going:0.0659 very:0.0250 teacher:0.1216.:0.1914 and:0.1592 I:0.3477)
		( the:0.0591�:0.1069 going:0.0654 kid:0.0248 teacher:0.1289.:0.2061 and:0.1455 I:0.3555)
		( the:0.0630�:0.1094 going:0.0649 kid:0.0233 teacher:0.1348.:0.2090 and:0.1426 I:0.3496)
		( the:0.0640�:0.1138 going:0.0613 kid:0.0219 teacher:0.1387.:0.2090 and:0.1416 I:0.3438)
		( the:0.0669�:0.1162 going:0.0581 very:0.0211 teacher:0.1406.:0.2100 and:0.1367 I:0.3262)
		( the:0.0688�:0.1187 going:0.0552 very:0.0209 teacher:0.1387.:0.2119 and:0.1367 I:0.3164)
		( the:0.0684�:0.1201 not:0.0540 very:0.0206 teacher:0.1396.:0.2090 and:0.1328 I:0.3086)
		( the:0.0684�:0.1206 not:0.0544 very:0.0198 teacher:0.1387.:0.2129 and:0.1289 I:0.2949)
		( the:0.0684�:0.1245 not:0.0549 very:0.0195 teacher:0.1396.:0.2109 and:0.1260 I:0.2871)
		( the:0.0684�:0.1245 not:0.0549 very:0.0195 teacher:0.1396.:0.2109 and:0.1260 I:0.2871)
 I'm a good teacher.
I'm a good
2300: sample 0: Hello, I'm a language model,
------
		(.:0.0254 the:0.0364,:0.0728 going:0.0471 good:0.0791 that:0.0796.:0.1196 I:0.0996)
		(.:0.0280 you:0.0498 have:0.0781 going:0.1079 good:0.0378.:0.0410.:0.2139 and:0.2354)
		(.:0.0383 you:0.0757 have:0.0840 going:0.0918 very:0.0200 teacher:0.0698.:0.2314 and:0.2451)
		(.:0.0569 you:0.0806�:0.0991 going:0.1045 very:0.0237 teacher:0.0801.:0.2285 and:0.2012)
		(,:0.0466 you:0.0840�:0.1084 going:0.0996 very:0.0227 teacher:0.0835.:0.2393 and:0.2041)
		(,:0.0608 you:0.0859�:0.1133 going:0.0986 very:0.0206 teacher:0.0884.:0.2393 and:0.2090)
		(,:0.0649 you:0.0859�:0.1152 going:0.0938 very:0.0198 teacher:0.0923.:0.2422 and:0.2090)
		(,:0.0654 you:0.0840�:0.1182 going:0.0898 very:0.0188 teacher:0.0977.:0.2480 and:0.2139)
		(,:0.0679 you:0.0815�:0.1177 going:0.0859 very:0.0178 teacher:0.0996.:0.2432 and:0.2158)
		(,:0.0659 you:0.0776�:0.1201 going:0.0820 very:0.0170 teacher:0.1021.:0.2441 and:0.2168)
		(,:0.0664 you:0.0732�:0.1206 going:0.0776 very:0.0160 teacher:0.1045.:0.2451 and:0.2178)
		(,:0.0649 you:0.0693�:0.1201 going:0.0742 professor:0.0156 teacher:0.1045.:0.2412 and:0.2188)
		(,:0.0649 you:0.0693�:0.1201 going:0.0742 professor:0.0156 teacher:0.1045.:0.2412 and:0.2188)
 and
------
		( the:0.0364,:0.0728 going:0.0471 good:0.0791 that:0.0796.:0.1196 I:0.0996 I:0.2197)
		( you:0.0498 have:0.0781 going:0.1079 good:0.0378.:0.0410.:0.2139 and:0.2354 I:0.5156)
		( you:0.0757 have:0.0840 going:0.0918 very:0.0200 teacher:0.0698.:0.2314 and:0.2451 I:0.4629)
		( you:0.0806�:0.0991 going:0.1045 very:0.0237 teacher:0.0801.:0.2285 and:0.2012 I:0.5586)
		( you:0.0840�:0.1084 going:0.0996 very:0.0227 teacher:0.0835.:0.2393 and:0.2041 I:0.5625)
		( you:0.0859�:0.1133 going:0.0986 very:0.0206 teacher:0.0884.:0.2393 and:0.2090 I:0.5625)
		( you:0.0859�:0.1152 going:0.0938 very:0.0198 teacher:0.0923.:0.2422 and:0.2090 I:0.5625)
		( you:0.0840�:0.1182 going:0.0898 very:0.0188 teacher:0.0977.:0.2480 and:0.2139 I:0.5625)
		( you:0.0815�:0.1177 going:0.0859 very:0.0178 teacher:0.0996.:0.2432 and:0.2158 I:0.5508)
		( you:0.0776�:0.1201 going:0.0820 very:0.0170 teacher:0.1021.:0.2441 and:0.2168 I:0.5547)
		( you:0.0732�:0.1206 going:0.0776 very:0.0160 teacher:0.1045.:0.2451 and:0.2178 I:0.5391)
		( you:0.0693�:0.1201 going:0.0742 professor:0.0156 teacher:0.1045.:0.2412 and:0.2188 I:0.5273)
		( you:0.0693�:0.1201 going:0.0742 professor:0.0156 teacher:0.1045.:0.2412 and:0.2188 I:0.5273)
 I'm a good friend. I'm a good friend
2450: sample 0: Hello, I'm a language model,
------
		(.:0.0220 the:0.0752,:0.0679 going:0.0552 good:0.0605 that:0.0894 that:0.1196 and:0.0742)
		(.:0.0275 and:0.0574 have:0.0796 going:0.1030 good:0.0260.:0.1143.:0.2139 and:0.2031)
		(.:0.0693 and:0.0476 have:0.0762 going:0.0713 kid:0.0271.:0.1289.:0.2324 and:0.1924)
		(.:0.1104 and:0.0469 have:0.0732 going:0.0806 kid:0.0262.:0.1143.:0.2422 and:0.1611)
		(.:0.0908 and:0.0535 am:0.0889 going:0.0767 kid:0.0270.:0.0952.:0.2344 and:0.1543)
		(,:0.1006 and:0.0586 am:0.0977 going:0.0732 kid:0.0270.:0.0923.:0.2363 and:0.1582)
		(,:0.1157 and:0.0635 am:0.1064 going:0.0703 kid:0.0255.:0.0938.:0.2334 and:0.1602)
		(,:0.1226 and:0.0664 am:0.1123 going:0.0664 kid:0.0242.:0.0928.:0.2275 and:0.1641)
		(,:0.1245 and:0.0698 am:0.1162 going:0.0640 kid:0.0229.:0.0928.:0.2285 and:0.1631)
		(,:0.1226 and:0.0713 am:0.1206 going:0.0603 kid:0.0225.:0.0903.:0.2236 and:0.1670)
		(,:0.1250 and:0.0728 am:0.1240 not:0.0564 kid:0.0214.:0.0874.:0.2197 and:0.1650)
		(,:0.1240 and:0.0742 am:0.1289 not:0.0579 kid:0.0201.:0.0879.:0.2158 and:0.1660)
		(,:0.1240 and:0.0742 am:0.1289 not:0.0579 kid:0.0201.:0.0879.:0.2158 and:0.1660)
 and
------
		( the:0.0752,:0.0679 going:0.0552 good:0.0605 that:0.0894 that:0.1196 and:0.0742 I:0.1235)
		( and:0.0574 have:0.0796 going:0.1030 good:0.0260.:0.1143.:0.2139 and:0.2031 I:0.3301)
		( and:0.0476 have:0.0762 going:0.0713 kid:0.0271.:0.1289.:0.2324 and:0.1924 I:0.3906)
		( and:0.0469 have:0.0732 going:0.0806 kid:0.0262.:0.1143.:0.2422 and:0.1611 I:0.4883)
		( and:0.0535 am:0.0889 going:0.0767 kid:0.0270.:0.0952.:0.2344 and:0.1543 I:0.4688)
		( and:0.0586 am:0.0977 going:0.0732 kid:0.0270.:0.0923.:0.2363 and:0.1582 I:0.4590)
		( and:0.0635 am:0.1064 going:0.0703 kid:0.0255.:0.0938.:0.2334 and:0.1602 I:0.4531)
		( and:0.0664 am:0.1123 going:0.0664 kid:0.0242.:0.0928.:0.2275 and:0.1641 I:0.4355)
		( and:0.0698 am:0.1162 going:0.0640 kid:0.0229.:0.0928.:0.2285 and:0.1631 I:0.4180)
		( and:0.0713 am:0.1206 going:0.0603 kid:0.0225.:0.0903.:0.2236 and:0.1670 I:0.4180)
		( and:0.0728 am:0.1240 not:0.0564 kid:0.0214.:0.0874.:0.2197 and:0.1650 I:0.4023)
		( and:0.0742 am:0.1289 not:0.0579 kid:0.0201.:0.0879.:0.2158 and:0.1660 I:0.3867)
		( and:0.0742 am:0.1289 not:0.0579 kid:0.0201.:0.0879.:0.2158 and:0.1660 I:0.3867)
 I'm a language.
- I'm a language
2600: sample 0: Hello, I'm a language model,
------
		(.:0.0221 the:0.0413 was:0.0664 not:0.0625 good:0.0452 that:0.1357,:0.1572 but:0.1011)
		(.:0.0209 I:0.0571 will:0.0845 not:0.0962 very:0.0255.:0.0991.:0.2051 and:0.3652)
		(.:0.0383 I:0.0449�:0.0737 not:0.0645 kid:0.0238.:0.1089.:0.2490 and:0.3164)
		(.:0.0659 you:0.0520�:0.0835 going:0.0625 kid:0.0227.:0.1055.:0.2793 and:0.2471)
		(.:0.0444 you:0.0591�:0.0894 going:0.0640 kid:0.0247.:0.0952.:0.2812 and:0.2266)
		(,:0.0771 you:0.0635�:0.0933 going:0.0618 kid:0.0243.:0.0918.:0.2812 and:0.2275)
		(,:0.0898 you:0.0649�:0.0923 going:0.0610 kid:0.0236.:0.0938.:0.2793 and:0.2246)
		(,:0.0928 and:0.0664�:0.0928 going:0.0571 kid:0.0228.:0.0933.:0.2793 and:0.2188)
		(,:0.0952 and:0.0698 am:0.0986 going:0.0549 kid:0.0214.:0.0933.:0.2812 and:0.2188)
		(,:0.1006 and:0.0723 am:0.1006 going:0.0530 kid:0.0206.:0.0903.:0.2754 and:0.2119)
		(,:0.1040 and:0.0737 am:0.1045 a:0.0508 kid:0.0195.:0.0898.:0.2754 and:0.2070)
		(,:0.1074 and:0.0762 am:0.1060 a:0.0520 kid:0.0189.:0.0879.:0.2715 and:0.2061)
		(,:0.1074 and:0.0762 am:0.1060 a:0.0520 kid:0.0189.:0.0879.:0.2715 and:0.2061)
 and
------
		( the:0.0413 was:0.0664 not:0.0625 good:0.0452 that:0.1357,:0.1572 but:0.1011 I:0.1553)
		( I:0.0571 will:0.0845 not:0.0962 very:0.0255.:0.0991.:0.2051 and:0.3652 I:0.3711)
		( I:0.0449�:0.0737 not:0.0645 kid:0.0238.:0.1089.:0.2490 and:0.3164 I:0.4141)
		( you:0.0520�:0.0835 going:0.0625 kid:0.0227.:0.1055.:0.2793 and:0.2471 I:0.5117)
		( you:0.0591�:0.0894 going:0.0640 kid:0.0247.:0.0952.:0.2812 and:0.2266 I:0.4922)
		( you:0.0635�:0.0933 going:0.0618 kid:0.0243.:0.0918.:0.2812 and:0.2275 I:0.4863)
		( you:0.0649�:0.0923 going:0.0610 kid:0.0236.:0.0938.:0.2793 and:0.2246 I:0.4824)
		( and:0.0664�:0.0928 going:0.0571 kid:0.0228.:0.0933.:0.2793 and:0.2188 I:0.4668)
		( and:0.0698 am:0.0986 going:0.0549 kid:0.0214.:0.0933.:0.2812 and:0.2188 I:0.4492)
		( and:0.0723 am:0.1006 going:0.0530 kid:0.0206.:0.0903.:0.2754 and:0.2119 I:0.4355)
		( and:0.0737 am:0.1045 a:0.0508 kid:0.0195.:0.0898.:0.2754 and:0.2070 I:0.4219)
		( and:0.0762 am:0.1060 a:0.0520 kid:0.0189.:0.0879.:0.2715 and:0.2061 I:0.4082)
		( and:0.0762 am:0.1060 a:0.0520 kid:0.0189.:0.0879.:0.2715 and:0.2061 I:0.4082)
 I'm a language. I'm a language, and
2750: sample 0: Hello, I'm a language model,
------
		(.:0.0211 and:0.0918 have:0.0693 not:0.0786 good:0.0708 that:0.1006,:0.1475 and:0.1221)
		(.:0.0287 I:0.0688 have:0.1177 going:0.1182 good:0.0427-:0.0776.:0.1797 and:0.3066)
		(.:0.0505 you:0.0752 am:0.1050 not:0.0864 good:0.0374-:0.1309.:0.1846 and:0.3125)
		(.:0.0298 you:0.0884 am:0.1104 going:0.0889 very:0.0337 teacher:0.1030.:0.2002 and:0.3027)
		(ed:0.0176 you:0.0918 am:0.1299 going:0.0854 very:0.0302 teacher:0.1006.:0.2080 and:0.2871)
		( to:0.0284 you:0.0962 am:0.1426 going:0.0811 very:0.0283 teacher:0.1035.:0.2080 and:0.2852)
		( to:0.0339 you:0.1006 am:0.1543 not:0.0806 very:0.0281 teacher:0.1021.:0.2061 and:0.2812)
		( to:0.0359 you:0.1021 am:0.1641 not:0.0825 very:0.0277 teacher:0.1011.:0.2109 and:0.2793)
		( to:0.0381 you:0.1016 am:0.1699 not:0.0850 very:0.0283 teacher:0.1001.:0.2051 and:0.2773)
		( to:0.0383 you:0.1011 am:0.1807 not:0.0840 very:0.0277 teacher:0.0962.:0.2002 and:0.2695)
		( to:0.0396 you:0.1006 am:0.1885 not:0.0859 very:0.0275 teacher:0.0947.:0.2012 and:0.2617)
		( to:0.0398 you:0.1001 am:0.1914 not:0.0879 very:0.0271-:0.0938.:0.1963 and:0.2598)
		( to:0.0398 you:0.1001 am:0.1914 not:0.0879 very:0.0271-:0.0938.:0.1963 and:0.2598)
 and
------
		( and:0.0918 have:0.0693 not:0.0786 good:0.0708 that:0.1006,:0.1475 and:0.1221 I:0.2139)
		( I:0.0688 have:0.1177 going:0.1182 good:0.0427-:0.0776.:0.1797 and:0.3066 I:0.4141)
		( you:0.0752 am:0.1050 not:0.0864 good:0.0374-:0.1309.:0.1846 and:0.3125 I:0.4316)
		( you:0.0884 am:0.1104 going:0.0889 very:0.0337 teacher:0.1030.:0.2002 and:0.3027 I:0.4121)
		( you:0.0918 am:0.1299 going:0.0854 very:0.0302 teacher:0.1006.:0.2080 and:0.2871 I:0.3789)
		( you:0.0962 am:0.1426 going:0.0811 very:0.0283 teacher:0.1035.:0.2080 and:0.2852 I:0.3691)
		( you:0.1006 am:0.1543 not:0.0806 very:0.0281 teacher:0.1021.:0.2061 and:0.2812 I:0.3652)
		( you:0.1021 am:0.1641 not:0.0825 very:0.0277 teacher:0.1011.:0.2109 and:0.2793 I:0.3555)
		( you:0.1016 am:0.1699 not:0.0850 very:0.0283 teacher:0.1001.:0.2051 and:0.2773 I:0.3398)
		( you:0.1011 am:0.1807 not:0.0840 very:0.0277 teacher:0.0962.:0.2002 and:0.2695 I:0.3320)
		( you:0.1006 am:0.1885 not:0.0859 very:0.0275 teacher:0.0947.:0.2012 and:0.2617 I:0.3242)
		( you:0.1001 am:0.1914 not:0.0879 very:0.0271-:0.0938.:0.1963 and:0.2598 I:0.3164)
		( you:0.1001 am:0.1914 not:0.0879 very:0.0271-:0.0938.:0.1963 and:0.2598 I:0.3164)
 I'm not sure how to use it.
I
2900: sample 0: Hello, I'm a language model,
------
		(.:0.0248 and:0.0500 am:0.0767 not:0.0981 good:0.0947 that:0.1758,:0.1699 I:0.0801)
		(.:0.0289 you:0.0608 have:0.0723 going:0.1475 good:0.0693.:0.1123.:0.3047 and:0.2676)
		(.:0.0527 you:0.0603 am:0.1152 not:0.1216 good:0.0330.:0.1128.:0.3457 and:0.2832)
		(.:0.0776 you:0.0684 am:0.1182 going:0.1299 good:0.0236.:0.1104.:0.3672 and:0.2490)
		(
:0.0625 you:0.0698 am:0.1387 going:0.1338 good:0.0227.:0.0962.:0.3633 and:0.2285)
		(
:0.0723 you:0.0747 am:0.1475 going:0.1250 good:0.0226.:0.0991.:0.3672 and:0.2256)
		(
:0.0688 you:0.0786 am:0.1553 going:0.1162 good:0.0222.:0.1016.:0.3672 and:0.2178)
		(
:0.0654 you:0.0811 am:0.1602 going:0.1084 good:0.0212.:0.1055.:0.3691 and:0.2168)
		(
:0.0610 you:0.0830 am:0.1650 not:0.1006 good:0.0203.:0.1060.:0.3672 and:0.2148)
		(
:0.0586 you:0.0850 am:0.1709 not:0.1001 little:0.0206.:0.1069.:0.3711 and:0.2139)
		(
:0.0564 you:0.0874 am:0.1758 not:0.0991 little:0.0215.:0.1074.:0.3672 and:0.2070)
		(
:0.0544 you:0.0894 am:0.1816 not:0.0972 little:0.0217.:0.1050.:0.3652 and:0.2061)
		(
:0.0544 you:0.0894 am:0.1816 not:0.0972 little:0.0217.:0.1050.:0.3652 and:0.2061)
 and
------
		( and:0.0500 am:0.0767 not:0.0981 good:0.0947 that:0.1758,:0.1699 I:0.0801 I:0.1328)
		( you:0.0608 have:0.0723 going:0.1475 good:0.0693.:0.1123.:0.3047 and:0.2676 I:0.4727)
		( you:0.0603 am:0.1152 not:0.1216 good:0.0330.:0.1128.:0.3457 and:0.2832 I:0.4805)
		( you:0.0684 am:0.1182 going:0.1299 good:0.0236.:0.1104.:0.3672 and:0.2490 I:0.5312)
		( you:0.0698 am:0.1387 going:0.1338 good:0.0227.:0.0962.:0.3633 and:0.2285 I:0.5312)
		( you:0.0747 am:0.1475 going:0.1250 good:0.0226.:0.0991.:0.3672 and:0.2256 I:0.5234)
		( you:0.0786 am:0.1553 going:0.1162 good:0.0222.:0.1016.:0.3672 and:0.2178 I:0.5078)
		( you:0.0811 am:0.1602 going:0.1084 good:0.0212.:0.1055.:0.3691 and:0.2168 I:0.5039)
		( you:0.0830 am:0.1650 not:0.1006 good:0.0203.:0.1060.:0.3672 and:0.2148 I:0.4883)
		( you:0.0850 am:0.1709 not:0.1001 little:0.0206.:0.1069.:0.3711 and:0.2139 I:0.4707)
		( you:0.0874 am:0.1758 not:0.0991 little:0.0215.:0.1074.:0.3672 and:0.2070 I:0.4707)
		( you:0.0894 am:0.1816 not:0.0972 little:0.0217.:0.1050.:0.3652 and:0.2061 I:0.4531)
		( you:0.0894 am:0.1816 not:0.0972 little:0.0217.:0.1050.:0.3652 and:0.2061 I:0.4531)
 I'm a teacher. I'm a teacher. I
3050: sample 0: Hello, I'm a language model,
------
		(.:0.0160 and:0.0786 am:0.0903 going:0.0649 good:0.0413 that:0.0938,:0.1855 and:0.1484)
		(,:0.0244 I:0.0942 have:0.0879 going:0.1387 very:0.0374 that:0.0811.:0.1592 and:0.2422)
		(.:0.0344 I:0.0796 am:0.0854 going:0.1040 very:0.0273 teacher:0.0825.:0.1953 and:0.2363)
		(
:0.0452 I:0.0703 am:0.0806 going:0.1157 very:0.0236 teacher:0.1074.:0.2227 and:0.2129)
		(.:0.0513 I:0.0664 am:0.0942 going:0.1187 kid:0.0225 teacher:0.1128.:0.2285 and:0.2012)
		(,:0.0393 I:0.0693 am:0.0952 going:0.1157 kid:0.0221 teacher:0.1172.:0.2314 and:0.2002)
		(,:0.0503 and:0.0713 am:0.0981 going:0.1113 kid:0.0210 teacher:0.1177.:0.2383 and:0.2031)
		(,:0.0530 and:0.0757 am:0.0991 going:0.1079 kid:0.0205 teacher:0.1196.:0.2402 and:0.2012)
		(,:0.0540 and:0.0786 am:0.1006 going:0.1040 kid:0.0193 teacher:0.1177.:0.2432 and:0.2012)
		(,:0.0566 and:0.0811 am:0.1021 going:0.1001 kid:0.0190 teacher:0.1162.:0.2471 and:0.2002)
		(,:0.0574 and:0.0835 am:0.1011 going:0.0962 teacher:0.0178 teacher:0.1138.:0.2500 and:0.1992)
		(,:0.0583 and:0.0859 am:0.1001 going:0.0933 teacher:0.0176 teacher:0.1123.:0.2559 and:0.1953)
		(,:0.0583 and:0.0859 am:0.1001 going:0.0933 teacher:0.0176 teacher:0.1123.:0.2559 and:0.1953)
 and
------
		( and:0.0786 am:0.0903 going:0.0649 good:0.0413 that:0.0938,:0.1855 and:0.1484 I:0.1699)
		( I:0.0942 have:0.0879 going:0.1387 very:0.0374 that:0.0811.:0.1592 and:0.2422 I:0.4004)
		( I:0.0796 am:0.0854 going:0.1040 very:0.0273 teacher:0.0825.:0.1953 and:0.2363 I:0.4414)
		( I:0.0703 am:0.0806 going:0.1157 very:0.0236 teacher:0.1074.:0.2227 and:0.2129 I:0.5234)
		( I:0.0664 am:0.0942 going:0.1187 kid:0.0225 teacher:0.1128.:0.2285 and:0.2012 I:0.5117)
		( I:0.0693 am:0.0952 going:0.1157 kid:0.0221 teacher:0.1172.:0.2314 and:0.2002 I:0.5039)
		( and:0.0713 am:0.0981 going:0.1113 kid:0.0210 teacher:0.1177.:0.2383 and:0.2031 I:0.4844)
		( and:0.0757 am:0.0991 going:0.1079 kid:0.0205 teacher:0.1196.:0.2402 and:0.2012 I:0.4824)
		( and:0.0786 am:0.1006 going:0.1040 kid:0.0193 teacher:0.1177.:0.2432 and:0.2012 I:0.4648)
		( and:0.0811 am:0.1021 going:0.1001 kid:0.0190 teacher:0.1162.:0.2471 and:0.2002 I:0.4551)
		( and:0.0835 am:0.1011 going:0.0962 teacher:0.0178 teacher:0.1138.:0.2500 and:0.1992 I:0.4473)
		( and:0.0859 am:0.1001 going:0.0933 teacher:0.0176 teacher:0.1123.:0.2559 and:0.1953 I:0.4297)
		( and:0.0859 am:0.1001 going:0.0933 teacher:0.0176 teacher:0.1123.:0.2559 and:0.1953 I:0.4297)
 I'm a teacher.
I'm a teacher.
3200: sample 0: Hello, I'm a language model,
------
		(.:0.0231 and:0.0986 am:0.1108 not:0.0564 good:0.0295 that:0.1201,:0.1484 and:0.1069)
		(.:0.0374 the:0.0801 am:0.0811 going:0.0962 very:0.0342 that:0.0791.:0.2148 and:0.3105)
		(.:0.0913 I:0.0825 am:0.1050 not:0.0786 very:0.0260 teacher:0.0659.:0.2432 and:0.3008)
		(.:0.1348 I:0.0742 am:0.1133 not:0.0933 very:0.0247 teacher:0.0786.:0.2402 and:0.2695)
		(,:0.0938 and:0.0698 am:0.1318 not:0.0933 very:0.0227 teacher:0.0806.:0.2256 and:0.2695)
		(,:0.1113 and:0.0723 am:0.1387 not:0.0952 very:0.0216 teacher:0.0815.:0.2285 and:0.2656)
		(,:0.1104 and:0.0747 am:0.1475 not:0.0972 very:0.0210 teacher:0.0815.:0.2295 and:0.2676)
		(,:0.1099 and:0.0776 am:0.1514 not:0.0986 very:0.0200 teacher:0.0825.:0.2314 and:0.2695)
		(,:0.1113 and:0.0791 am:0.1543 not:0.1006 very:0.0195 teacher:0.0835.:0.2344 and:0.2676)
		(,:0.1133 and:0.0825 am:0.1582 not:0.1001 very:0.0190 teacher:0.0825.:0.2324 and:0.2695)
		(,:0.1133 and:0.0840 am:0.1621 not:0.1025 very:0.0181 teacher:0.0811.:0.2295 and:0.2676)
		(,:0.1123 and:0.0859 am:0.1670 not:0.1045 very:0.0178 teacher:0.0796.:0.2334 and:0.2676)
		(,:0.1123 and:0.0859 am:0.1670 not:0.1045 very:0.0178 teacher:0.0796.:0.2334 and:0.2676)
 and
------
		( and:0.0986 am:0.1108 not:0.0564 good:0.0295 that:0.1201,:0.1484 and:0.1069 I:0.1484)
		( the:0.0801 am:0.0811 going:0.0962 very:0.0342 that:0.0791.:0.2148 and:0.3105 I:0.5703)
		( I:0.0825 am:0.1050 not:0.0786 very:0.0260 teacher:0.0659.:0.2432 and:0.3008 I:0.6172)
		( I:0.0742 am:0.1133 not:0.0933 very:0.0247 teacher:0.0786.:0.2402 and:0.2695 I:0.6875)
		( and:0.0698 am:0.1318 not:0.0933 very:0.0227 teacher:0.0806.:0.2256 and:0.2695 I:0.6914)
		( and:0.0723 am:0.1387 not:0.0952 very:0.0216 teacher:0.0815.:0.2285 and:0.2656 I:0.6836)
		( and:0.0747 am:0.1475 not:0.0972 very:0.0210 teacher:0.0815.:0.2295 and:0.2676 I:0.6680)
		( and:0.0776 am:0.1514 not:0.0986 very:0.0200 teacher:0.0825.:0.2314 and:0.2695 I:0.6680)
		( and:0.0791 am:0.1543 not:0.1006 very:0.0195 teacher:0.0835.:0.2344 and:0.2676 I:0.6523)
		( and:0.0825 am:0.1582 not:0.1001 very:0.0190 teacher:0.0825.:0.2324 and:0.2695 I:0.6367)
		( and:0.0840 am:0.1621 not:0.1025 very:0.0181 teacher:0.0811.:0.2295 and:0.2676 I:0.6367)
		( and:0.0859 am:0.1670 not:0.1045 very:0.0178 teacher:0.0796.:0.2334 and:0.2676 I:0.6211)
		( and:0.0859 am:0.1670 not:0.1045 very:0.0178 teacher:0.0796.:0.2334 and:0.2676 I:0.6211)
 I'm not sure that I'm not sure that I
3350: sample 0: Hello, I'm a language model,
------
		(.:0.0237 and:0.0747 am:0.0757 not:0.0825 good:0.0344 that:0.1465 that:0.1680 and:0.1089)
		(.:0.0400 I:0.1797 will:0.0864 not:0.1475 very:0.0410 that:0.0908,:0.1709 and:0.2793)
		(.:0.0752 I:0.1709 am:0.0981 not:0.1396 very:0.0278.:0.0537.:0.1738 and:0.2471)
		(.:0.0674 I:0.1406 am:0.1040 not:0.1377 very:0.0217 teacher:0.0659.:0.1943 and:0.2334)
		(.:0.0591 I:0.1357 am:0.1201 not:0.1309 teacher:0.0183 teacher:0.0703.:0.1992 and:0.2373)
		(,:0.0645 I:0.1357 am:0.1279 not:0.1309 teacher:0.0188 teacher:0.0718.:0.2012 and:0.2432)
		(,:0.0747 I:0.1387 am:0.1328 not:0.1289 teacher:0.0198 teacher:0.0693.:0.2031 and:0.2461)
		(,:0.0728 I:0.1406 am:0.1387 not:0.1270 teacher:0.0203 teacher:0.0659.:0.2002 and:0.2559)
		(,:0.0732 I:0.1367 am:0.1445 not:0.1226 teacher:0.0205 teacher:0.0654.:0.2021 and:0.2559)
		(,:0.0718 I:0.1357 am:0.1494 not:0.1216 teacher:0.0201 teacher:0.0630.:0.2002 and:0.2617)
		(,:0.0698 I:0.1348 am:0.1562 not:0.1196 teacher:0.0204 teacher:0.0605.:0.1992 and:0.2656)
		(,:0.0703 I:0.1338 am:0.1631 not:0.1187 teacher:0.0206 teacher:0.0581.:0.1973 and:0.2656)
		(,:0.0703 I:0.1338 am:0.1631 not:0.1187 teacher:0.0206 teacher:0.0581.:0.1973 and:0.2656)
 and
------
		( and:0.0747 am:0.0757 not:0.0825 good:0.0344 that:0.1465 that:0.1680 and:0.1089 I:0.2793)
		( I:0.1797 will:0.0864 not:0.1475 very:0.0410 that:0.0908,:0.1709 and:0.2793 I:0.6602)
		( I:0.1709 am:0.0981 not:0.1396 very:0.0278.:0.0537.:0.1738 and:0.2471 I:0.6914)
		( I:0.1406 am:0.1040 not:0.1377 very:0.0217 teacher:0.0659.:0.1943 and:0.2334 I:0.6758)
		( I:0.1357 am:0.1201 not:0.1309 teacher:0.0183 teacher:0.0703.:0.1992 and:0.2373 I:0.6602)
		( I:0.1357 am:0.1279 not:0.1309 teacher:0.0188 teacher:0.0718.:0.2012 and:0.2432 I:0.6562)
		( I:0.1387 am:0.1328 not:0.1289 teacher:0.0198 teacher:0.0693.:0.2031 and:0.2461 I:0.6562)
		( I:0.1406 am:0.1387 not:0.1270 teacher:0.0203 teacher:0.0659.:0.2002 and:0.2559 I:0.6523)
		( I:0.1367 am:0.1445 not:0.1226 teacher:0.0205 teacher:0.0654.:0.2021 and:0.2559 I:0.6523)
		( I:0.1357 am:0.1494 not:0.1216 teacher:0.0201 teacher:0.0630.:0.2002 and:0.2617 I:0.6523)
		( I:0.1348 am:0.1562 not:0.1196 teacher:0.0204 teacher:0.0605.:0.1992 and:0.2656 I:0.6367)
		( I:0.1338 am:0.1631 not:0.1187 teacher:0.0206 teacher:0.0581.:0.1973 and:0.2656 I:0.6367)
		( I:0.1338 am:0.1631 not:0.1187 teacher:0.0206 teacher:0.0581.:0.1973 and:0.2656 I:0.6367)
 I'm a language. I'm a language. I
3500: sample 0: Hello, I'm a language model,
------
		(.:0.0209 and:0.0486 am:0.1074 not:0.0503 one:0.0369 that:0.1191 that:0.2012 and:0.0933)
		(.:0.0306 I:0.0483 am:0.0859 not:0.0703 one:0.0317 that:0.0752 that:0.2324 and:0.2383)
		(.:0.0566 I:0.0442 am:0.0962 going:0.0684 very:0.0310 teacher:0.0835.:0.1299 and:0.1904)
		(.:0.0752 I:0.0334 am:0.0952 going:0.0801 very:0.0277 teacher:0.1021.:0.1357 and:0.1787)
		(.:0.0625 and:0.0325 am:0.1055 going:0.0854 very:0.0239 teacher:0.1074.:0.1416 and:0.1787)
		(.:0.0630 and:0.0349 am:0.1128 going:0.0835 very:0.0234 teacher:0.1094.:0.1416 and:0.1807)
		(.:0.0708 and:0.0374 am:0.1196 going:0.0830 very:0.0232 teacher:0.1108.:0.1387 and:0.1836)
		(.:0.0708 and:0.0386 am:0.1245 going:0.0806 very:0.0223 teacher:0.1118.:0.1377 and:0.1836)
		(.:0.0728 and:0.0398 am:0.1260 going:0.0801 very:0.0221 teacher:0.1104 for:0.1406 and:0.1826)
		(.:0.0732 and:0.0410 am:0.1299 going:0.0771 very:0.0221 teacher:0.1079 for:0.1445 and:0.1826)
		(.:0.0732 and:0.0422 am:0.1357 going:0.0742 very:0.0211 teacher:0.1089 for:0.1445 and:0.1816)
		(.:0.0732 and:0.0437 am:0.1367 going:0.0732 very:0.0210 teacher:0.1074 for:0.1484 and:0.1816)
		(.:0.0732 and:0.0437 am:0.1367 going:0.0732 very:0.0210 teacher:0.1074 for:0.1484 and:0.1816)
 and
------
		( and:0.0486 am:0.1074 not:0.0503 one:0.0369 that:0.1191 that:0.2012 and:0.0933 I:0.2930)
		( I:0.0483 am:0.0859 not:0.0703 one:0.0317 that:0.0752 that:0.2324 and:0.2383 I:0.4473)
		( I:0.0442 am:0.0962 going:0.0684 very:0.0310 teacher:0.0835.:0.1299 and:0.1904 I:0.4941)
		( I:0.0334 am:0.0952 going:0.0801 very:0.0277 teacher:0.1021.:0.1357 and:0.1787 I:0.5664)
		( and:0.0325 am:0.1055 going:0.0854 very:0.0239 teacher:0.1074.:0.1416 and:0.1787 I:0.5547)
		( and:0.0349 am:0.1128 going:0.0835 very:0.0234 teacher:0.1094.:0.1416 and:0.1807 I:0.5508)
		( and:0.0374 am:0.1196 going:0.0830 very:0.0232 teacher:0.1108.:0.1387 and:0.1836 I:0.5352)
		( and:0.0386 am:0.1245 going:0.0806 very:0.0223 teacher:0.1118.:0.1377 and:0.1836 I:0.5195)
		( and:0.0398 am:0.1260 going:0.0801 very:0.0221 teacher:0.1104 for:0.1406 and:0.1826 I:0.5078)
		( and:0.0410 am:0.1299 going:0.0771 very:0.0221 teacher:0.1079 for:0.1445 and:0.1826 I:0.4902)
		( and:0.0422 am:0.1357 going:0.0742 very:0.0211 teacher:0.1089 for:0.1445 and:0.1816 I:0.4766)
		( and:0.0437 am:0.1367 going:0.0732 very:0.0210 teacher:0.1074 for:0.1484 and:0.1816 I:0.4629)
		( and:0.0437 am:0.1367 going:0.0732 very:0.0210 teacher:0.1074 for:0.1484 and:0.1816 I:0.4629)
 I'm a teacher. I'm not a teacher,
