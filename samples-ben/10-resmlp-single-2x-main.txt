1: sample 0: Hello, I'm a language model,
------
		( Otto:0.0002,:0.0003 I:0.0005 1862:0.0002 a:0.0004 language:0.0007 model:0.0015,:0.0019)
		(Characters:0.0002 Cascade:0.0002 I:0.0001 containment:0.0002 Options:0.0001 language:0.0003 model:0.0003,:0.0003)
		(Characters:0.0002 Featuring:0.0002 intervention:0.0001 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002,:0.0002)
		(urations:0.0002 Featuring:0.0002 Featuring:0.0002 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002 Begin:0.0002)
		( defences:0.0002 Featuring:0.0002 Featuring:0.0002 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002 Begin:0.0002)
		( defences:0.0002 Kw:0.0002 Featuring:0.0002 Featuring:0.0002 Git:0.0001Ho:0.0002 Featuring:0.0002 Begin:0.0002)
		( defences:0.0002 Kw:0.0002 Featuring:0.0002 gathers:0.0002 Begin:0.0002Ho:0.0002 Begin:0.0002 Begin:0.0002)
		( tougher:0.0002 Kw:0.0002 Featuring:0.0002 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002)
		( tougher:0.0002 Kw:0.0002 Begin:0.0001 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002)
		( tougher:0.0001 Kw:0.0002 Begin:0.0001 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002)
		( tougher:0.0001 Kw:0.0002.:0.0001 gathers:0.0002 Begin:0.0002.:0.0002.:0.0002 Begin:0.0002)
		(.:0.0001 Kw:0.0002.:0.0001 gathers:0.0002.:0.0002.:0.0002.:0.0002 Begin:0.0002)
		(.:0.0001 Kw:0.0002.:0.0001 gathers:0.0002.:0.0002.:0.0002.:0.0002 Begin:0.0002)
 Begin
------
		(,:0.0003 I:0.0005 1862:0.0002 a:0.0004 language:0.0007 model:0.0015,:0.0019 Begin:0.0004)
		( Cascade:0.0002 I:0.0001 containment:0.0002 Options:0.0001 language:0.0003 model:0.0003,:0.0003 Begin:0.0003)
		( Featuring:0.0002 intervention:0.0001 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002,:0.0002 Begin:0.0002)
		( Featuring:0.0002 Featuring:0.0002 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002 Begin:0.0002 Begin:0.0002)
		( Featuring:0.0002 Featuring:0.0002 Featuring:0.0002 Rw:0.0002Ho:0.0002 Featuring:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002 Featuring:0.0002 Featuring:0.0002 Git:0.0001Ho:0.0002 Featuring:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002 Featuring:0.0002 gathers:0.0002 Begin:0.0002Ho:0.0002 Begin:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002 Featuring:0.0002 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002 Begin:0.0001 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002 Begin:0.0001 gathers:0.0002 Begin:0.0002 Fur:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002.:0.0001 gathers:0.0002 Begin:0.0002.:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002.:0.0001 gathers:0.0002.:0.0002.:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
		( Kw:0.0002.:0.0001 gathers:0.0002.:0.0002.:0.0002.:0.0002 Begin:0.0002 Begin:0.0002)
 Begin Begin.........
50: sample 0: Hello, I'm a language model,
------
		( celeb:0.0004 and:0.0527-:0.0059 quir:0.0004�:0.0095 grease:0.0003ously:0.0003 and:0.0815)
		(ing:0.0005 and:0.0522,:0.0884,:0.0016�:0.0505,:0.0269,:0.0122 and:0.0571)
		(,:0.0056 the:0.0447,:0.0815,:0.0177�:0.0532,:0.0654,:0.0439 and:0.0481)
		(,:0.0208 the:0.0452,:0.0713,:0.0422�:0.0493,:0.0791,:0.0669 and:0.0444)
		(,:0.0383 the:0.0471,:0.0635,:0.0615�:0.0447,:0.0830,:0.0776 the:0.0449)
		(,:0.0532 the:0.0483,:0.0564,:0.0742�:0.0420,:0.0825,:0.0845 the:0.0466)
		(,:0.0654 the:0.0498,:0.0515,:0.0811�:0.0398,:0.0830,:0.0864 the:0.0486)
		(,:0.0728 the:0.0513,:0.0471,:0.0840�:0.0376,:0.0815,:0.0869 the:0.0500)
		(,:0.0781 the:0.0544,:0.0430,:0.0869�:0.0356,:0.0781,:0.0879 the:0.0532)
		(,:0.0815 the:0.0559,:0.0393,:0.0874�:0.0349,:0.0747,:0.0859 the:0.0549)
		(,:0.0825 the:0.0574,:0.0369,:0.0884�:0.0332,:0.0742,:0.0850 the:0.0566)
		(,:0.0835 the:0.0574,:0.0347,:0.0869�:0.0327,:0.0713,:0.0840 the:0.0583)
		(,:0.0835 the:0.0574,:0.0347,:0.0869�:0.0327,:0.0713,:0.0840 the:0.0583)
 the
------
		( and:0.0527-:0.0059 quir:0.0004�:0.0095 grease:0.0003ously:0.0003 and:0.0815�:0.0060)
		( and:0.0522,:0.0884,:0.0016�:0.0505,:0.0269,:0.0122 and:0.0571�:0.0334)
		( the:0.0447,:0.0815,:0.0177�:0.0532,:0.0654,:0.0439 and:0.0481�:0.0403)
		( the:0.0452,:0.0713,:0.0422�:0.0493,:0.0791,:0.0669 and:0.0444�:0.0396)
		( the:0.0471,:0.0635,:0.0615�:0.0447,:0.0830,:0.0776 the:0.0449�:0.0369)
		( the:0.0483,:0.0564,:0.0742�:0.0420,:0.0825,:0.0845 the:0.0466�:0.0347)
		( the:0.0498,:0.0515,:0.0811�:0.0398,:0.0830,:0.0864 the:0.0486�:0.0327)
		( the:0.0513,:0.0471,:0.0840�:0.0376,:0.0815,:0.0869 the:0.0500�:0.0320)
		( the:0.0544,:0.0430,:0.0869�:0.0356,:0.0781,:0.0879 the:0.0532�:0.0304)
		( the:0.0559,:0.0393,:0.0874�:0.0349,:0.0747,:0.0859 the:0.0549�:0.0288)
		( the:0.0574,:0.0369,:0.0884�:0.0332,:0.0742,:0.0850 the:0.0566�:0.0282)
		( the:0.0574,:0.0347,:0.0869�:0.0327,:0.0713,:0.0840 the:0.0583�:0.0276)
		( the:0.0574,:0.0347,:0.0869�:0.0327,:0.0713,:0.0840 the:0.0583�:0.0276)
�.
�, the� to the� to
200: sample 0: Hello, I'm a language model,
------
		(iments:0.0107 but:0.1387�:0.1572 not:0.0908 I:0.0161 States:0.0601 States:0.0369 but:0.1484)
		(ations:0.0413 and:0.3086,:0.0630 to:0.1895
:0.0124 States:0.0322 to:0.0535 and:0.3164)
		(ations:0.0189 and:0.2559,:0.0713 to:0.1475
:0.0132 to:0.0479 to:0.0713 and:0.2617)
		(,:0.0250 and:0.2031,:0.0698 to:0.1240 few:0.0177 to:0.0630,:0.0854 and:0.2188)
		(,:0.0452 and:0.1689
:0.0811 to:0.0996 few:0.0187 to:0.0737,:0.0991 and:0.1943)
		(,:0.0645 and:0.1475
:0.1001 to:0.0771 very:0.0154,:0.0825,:0.1089 and:0.1738)
		(,:0.0825 and:0.1279
:0.1099 to:0.0610 very:0.0165.:0.0947,:0.1196 and:0.1562)
		(,:0.0977 and:0.1104
:0.1050 not:0.0659 very:0.0164.:0.1084,:0.1245 and:0.1406)
		(,:0.1108 and:0.0972
:0.0874 not:0.0679 very:0.0155.:0.1235.:0.1338 and:0.1250)
		(,:0.1211 and:0.0874�:0.0742 you:0.0737 very:0.0139.:0.1279.:0.1396 and:0.1182)
		(,:0.1318 and:0.0786�:0.0830 you:0.0811 very:0.0126.:0.1338.:0.1406 and:0.1055)
		(,:0.1387 and:0.0698�:0.0801 you:0.0845 need:0.0115.:0.1406.:0.1367 and:0.0996)
		(,:0.1387 and:0.0698�:0.0801 you:0.0845 need:0.0115.:0.1406.:0.1367 and:0.0996)
 and
------
		( but:0.1387�:0.1572 not:0.0908 I:0.0161 States:0.0601 States:0.0369 but:0.1484 but:0.0090)
		( and:0.3086,:0.0630 to:0.1895
:0.0124 States:0.0322 to:0.0535 and:0.3164 the:0.0366)
		( and:0.2559,:0.0713 to:0.1475
:0.0132 to:0.0479 to:0.0713 and:0.2617 the:0.0542)
		( and:0.2031,:0.0698 to:0.1240 few:0.0177 to:0.0630,:0.0854 and:0.2188 the:0.0708)
		( and:0.1689
:0.0811 to:0.0996 few:0.0187 to:0.0737,:0.0991 and:0.1943 the:0.0796)
		( and:0.1475
:0.1001 to:0.0771 very:0.0154,:0.0825,:0.1089 and:0.1738 the:0.0859)
		( and:0.1279
:0.1099 to:0.0610 very:0.0165.:0.0947,:0.1196 and:0.1562 the:0.0850)
		( and:0.1104
:0.1050 not:0.0659 very:0.0164.:0.1084,:0.1245 and:0.1406 the:0.0811)
		( and:0.0972
:0.0874 not:0.0679 very:0.0155.:0.1235.:0.1338 and:0.1250 the:0.0757)
		( and:0.0874�:0.0742 you:0.0737 very:0.0139.:0.1279.:0.1396 and:0.1182 the:0.0713)
		( and:0.0786�:0.0830 you:0.0811 very:0.0126.:0.1338.:0.1406 and:0.1055 the:0.0693)
		( and:0.0698�:0.0801 you:0.0845 need:0.0115.:0.1406.:0.1367 and:0.0996 the:0.0698)
		( and:0.0698�:0.0801 you:0.0845 need:0.0115.:0.1406.:0.1367 and:0.0996 the:0.0698)
 the same time.
The first, and the same
350: sample 0: Hello, I'm a language model,
------
		(ators:0.0312 but:0.0598�:0.0654 not:0.2236ll:0.0093izers:0.0540izers:0.0894 but:0.0593)
		(�:0.1108 and:0.2734-:0.0679 not:0.2490 to:0.0151 to:0.1807 to:0.2275 and:0.2793)
		(-:0.0938 and:0.2617.:0.0420 not:0.2490 great:0.0134 to:0.1885 to:0.2148 and:0.2676)
		(-:0.0806 and:0.2441.:0.0425 not:0.2051 great:0.0171 to:0.1670 to:0.1787 and:0.2461)
		(,:0.0938 and:0.2373.:0.0393 not:0.1650 great:0.0178,:0.1592 to:0.1328 and:0.2471)
		(,:0.1035 and:0.2080�:0.0405 not:0.1318 great:0.0172,:0.1729.:0.1245 and:0.2324)
		(,:0.1099 and:0.1797�:0.0522 not:0.1055 great:0.0161,:0.1846.:0.1299 and:0.2285)
		(,:0.1084 and:0.1533�:0.0640 not:0.0845 great:0.0150,:0.1953.:0.1357 and:0.2129)
		(,:0.1040 and:0.1299�:0.0781 not:0.0718 great:0.0134,:0.1953.:0.1309 and:0.1885)
		(,:0.0981 and:0.1123�:0.0903 not:0.0598 great:0.0125,:0.1914.:0.1260 and:0.1660)
		(,:0.0942 and:0.0942�:0.0986 not:0.0518 great:0.0115,:0.1807.:0.1279 and:0.1533)
		(,:0.0874 and:0.0806�:0.1030 not:0.0461 story:0.0117,:0.1768.:0.1226 and:0.1338)
		(,:0.0874 and:0.0806�:0.1030 not:0.0461 story:0.0117,:0.1768.:0.1226 and:0.1338)
 and
------
		( but:0.0598�:0.0654 not:0.2236ll:0.0093izers:0.0540izers:0.0894 but:0.0593 including:0.0099)
		( and:0.2734-:0.0679 not:0.2490 to:0.0151 to:0.1807 to:0.2275 and:0.2793 the:0.1064)
		( and:0.2617.:0.0420 not:0.2490 great:0.0134 to:0.1885 to:0.2148 and:0.2676 the:0.1147)
		( and:0.2441.:0.0425 not:0.2051 great:0.0171 to:0.1670 to:0.1787 and:0.2461 the:0.1040)
		( and:0.2373.:0.0393 not:0.1650 great:0.0178,:0.1592 to:0.1328 and:0.2471 the:0.0923)
		( and:0.2080�:0.0405 not:0.1318 great:0.0172,:0.1729.:0.1245 and:0.2324 the:0.0762)
		( and:0.1797�:0.0522 not:0.1055 great:0.0161,:0.1846.:0.1299 and:0.2285 the:0.0623)
		( and:0.1533�:0.0640 not:0.0845 great:0.0150,:0.1953.:0.1357 and:0.2129 the:0.0518)
		( and:0.1299�:0.0781 not:0.0718 great:0.0134,:0.1953.:0.1309 and:0.1885 the:0.0427)
		( and:0.1123�:0.0903 not:0.0598 great:0.0125,:0.1914.:0.1260 and:0.1660 the:0.0374)
		( and:0.0942�:0.0986 not:0.0518 great:0.0115,:0.1807.:0.1279 and:0.1533 the:0.0327)
		( and:0.0806�:0.1030 not:0.0461 story:0.0117,:0.1768.:0.1226 and:0.1338 the:0.0304)
		( and:0.0806�:0.1030 not:0.0461 story:0.0117,:0.1768.:0.1226 and:0.1338 the:0.0304)
 the same time, and the same time, and the
500: sample 0: Hello, I'm a language model,
------
		(and:0.0203 including:0.0302ll:0.0264 not:0.0732ll:0.0083ities:0.0287ization:0.0515 including:0.0311)
		(-:0.0825 and:0.2871-:0.0825 not:0.0752-:0.0165-:0.1348 to:0.1543 and:0.3359)
		(-:0.0776 and:0.2471,:0.0918 not:0.0776-:0.0143,:0.1406 to:0.1377 and:0.3008)
		(,:0.0879 and:0.1982,:0.1094 not:0.0762-:0.0098.:0.1650.:0.1270 and:0.2480)
		(,:0.0962 and:0.1602,:0.1060 not:0.0718 great:0.0078.:0.1738.:0.1436 and:0.2158)
		(,:0.1001 and:0.1338,:0.0840 not:0.0674 few:0.0098.:0.1748.:0.1602 and:0.2031)
		(,:0.1060 and:0.1157�:0.0591 not:0.0635 lot:0.0132.:0.1787.:0.1631 and:0.1807)
		(,:0.1094 and:0.1045�:0.0923 not:0.0569 lot:0.0164.:0.1768.:0.1611 and:0.1621)
		(,:0.1128 and:0.0923�:0.1123 not:0.0503 lot:0.0190.:0.1650.:0.1611 and:0.1357)
		(,:0.1084 and:0.0776�:0.1221 not:0.0435 lot:0.0197.:0.1494.:0.1426 and:0.1079)
		(,:0.1006 and:0.0618�:0.1250 not:0.0388 person:0.0212.:0.1309.:0.1299 I:0.0991)
		(,:0.0913 and:0.0493�:0.1191 not:0.0349 person:0.0240.:0.1167 of:0.1299 I:0.1357)
		(,:0.0913 and:0.0493�:0.1191 not:0.0349 person:0.0240.:0.1167 of:0.1299 I:0.1357)
 I
------
		( including:0.0302ll:0.0264 not:0.0732ll:0.0083ities:0.0287ization:0.0515 including:0.0311-:0.0371)
		( and:0.2871-:0.0825 not:0.0752-:0.0165-:0.1348 to:0.1543 and:0.3359-:0.1230)
		( and:0.2471,:0.0918 not:0.0776-:0.0143,:0.1406 to:0.1377 and:0.3008,:0.0820)
		( and:0.1982,:0.1094 not:0.0762-:0.0098.:0.1650.:0.1270 and:0.2480,:0.0830)
		( and:0.1602,:0.1060 not:0.0718 great:0.0078.:0.1738.:0.1436 and:0.2158,:0.0723)
		( and:0.1338,:0.0840 not:0.0674 few:0.0098.:0.1748.:0.1602 and:0.2031�:0.0625)
		( and:0.1157�:0.0591 not:0.0635 lot:0.0132.:0.1787.:0.1631 and:0.1807�:0.0967)
		( and:0.1045�:0.0923 not:0.0569 lot:0.0164.:0.1768.:0.1611 and:0.1621�:0.1299)
		( and:0.0923�:0.1123 not:0.0503 lot:0.0190.:0.1650.:0.1611 and:0.1357�:0.1436)
		( and:0.0776�:0.1221 not:0.0435 lot:0.0197.:0.1494.:0.1426 and:0.1079�:0.1445)
		( and:0.0618�:0.1250 not:0.0388 person:0.0212.:0.1309.:0.1299 I:0.0991�:0.1309)
		( and:0.0493�:0.1191 not:0.0349 person:0.0240.:0.1167 of:0.1299 I:0.1357�:0.1172)
		( and:0.0493�:0.1191 not:0.0349 person:0.0240.:0.1167 of:0.1299 I:0.1357�:0.1172)
’s a child’s a child�
650: sample 0: Hello, I'm a language model,
------
		(�:0.0703 including:0.0277-:0.0625 not:0.0161
:0.0089ization:0.0615ization:0.1719 including:0.0432)
		(-:0.0781 and:0.0479-:0.0557 not:0.0092-:0.0126,:0.0630ization:0.0708 and:0.0574)
		(,:0.0771 and:0.0352,:0.0747 not:0.0127-:0.0089.:0.0923.:0.0562 and:0.0386)
		(,:0.0723 and:0.0250,:0.0811 not:0.0118-:0.0058.:0.0952.:0.0608 and:0.0332)
		(,:0.0684 and:0.0233,:0.0898 not:0.0128-:0.0048.:0.1006.:0.0684 and:0.0386)
		(,:0.0708 and:0.0280,:0.1055 not:0.0154 new:0.0052.:0.1123.:0.0835 and:0.0537)
		(,:0.0801 and:0.0391,:0.1152 not:0.0195 lot:0.0076.:0.1309.:0.1016 and:0.0825)
		(,:0.0981 and:0.0598,:0.1099 like:0.0267 lot:0.0115,:0.1514.:0.1187 and:0.1226)
		(,:0.1182 and:0.0806,:0.0835 like:0.0359 lot:0.0164,:0.1777.:0.1328 and:0.1553)
		(,:0.1348 and:0.0845,:0.0542 like:0.0398 lot:0.0212,:0.1816.:0.1387 and:0.1621)
		(,:0.1396 and:0.0713 am:0.0442 like:0.0408 lot:0.0249,:0.1875.:0.1348 and:0.1426)
		(,:0.1357 I:0.1035 am:0.0466 like:0.0386 lot:0.0270,:0.1943,:0.1309 I:0.2178)
		(,:0.1357 I:0.1035 am:0.0466 like:0.0386 lot:0.0270,:0.1943,:0.1309 I:0.2178)
 I
------
		( including:0.0277-:0.0625 not:0.0161
:0.0089ization:0.0615ization:0.1719 including:0.0432-:0.1079)
		( and:0.0479-:0.0557 not:0.0092-:0.0126,:0.0630ization:0.0708 and:0.0574-:0.0825)
		( and:0.0352,:0.0747 not:0.0127-:0.0089.:0.0923.:0.0562 and:0.0386.:0.0620)
		( and:0.0250,:0.0811 not:0.0118-:0.0058.:0.0952.:0.0608 and:0.0332.:0.0562)
		( and:0.0233,:0.0898 not:0.0128-:0.0048.:0.1006.:0.0684 and:0.0386.:0.0515)
		( and:0.0280,:0.1055 not:0.0154 new:0.0052.:0.1123.:0.0835 and:0.0537,:0.0601)
		( and:0.0391,:0.1152 not:0.0195 lot:0.0076.:0.1309.:0.1016 and:0.0825,:0.0703)
		( and:0.0598,:0.1099 like:0.0267 lot:0.0115,:0.1514.:0.1187 and:0.1226,:0.0684)
		( and:0.0806,:0.0835 like:0.0359 lot:0.0164,:0.1777.:0.1328 and:0.1553,:0.0537)
		( and:0.0845,:0.0542 like:0.0398 lot:0.0212,:0.1816.:0.1387 and:0.1621 am:0.0510)
		( and:0.0713 am:0.0442 like:0.0408 lot:0.0249,:0.1875.:0.1348 and:0.1426 am:0.0630)
		( I:0.1035 am:0.0466 like:0.0386 lot:0.0270,:0.1943,:0.1309 I:0.2178 am:0.0684)
		( I:0.1035 am:0.0466 like:0.0386 lot:0.0270,:0.1943,:0.1309 I:0.2178 am:0.0684)
 am a lot of my students, I am a lot
800: sample 0: Hello, I'm a language model,
------
		(-:0.0493 including:0.0194-:0.1074 not:0.0051
:0.0073-:0.0552ization:0.1328 including:0.0361)
		(-:0.0728
:0.0299-:0.0850 not:0.0050
:0.0137.:0.0796ization:0.0552 and:0.0371)
		(,:0.0444 and:0.0111,:0.0669 not:0.0052
:0.0047.:0.0630,:0.0312 and:0.0132)
		(,:0.0317 and:0.0066,:0.0549 not:0.0053 great:0.0033.:0.0500,:0.0278 and:0.0107)
		(,:0.0284 and:0.0066,:0.0562 not:0.0071 great:0.0040,:0.0527,:0.0322 and:0.0140)
		(,:0.0315 and:0.0100,:0.0654 not:0.0121 bit:0.0061,:0.0708,:0.0459 and:0.0267)
		(,:0.0413 and:0.0217,:0.0742 not:0.0225 bit:0.0113,:0.1074,:0.0737 and:0.0605)
		(,:0.0591 and:0.0525 was:0.0747 not:0.0386 bit:0.0194,:0.1475,:0.1060 and:0.1250)
		(,:0.0781 and:0.1016 was:0.0801 like:0.0576 bit:0.0272,:0.1748,:0.1260 and:0.1807)
		(,:0.0884 and:0.1182 was:0.0635 like:0.0635 bit:0.0315,:0.1807,:0.1299 and:0.2002)
		(!:0.0947 and:0.1040 was:0.0486 like:0.0620 bit:0.0325,:0.1777.:0.1514 and:0.2002)
		(!:0.1157 and:0.0820 have:0.0413 like:0.0581 bit:0.0325,:0.1650.:0.1602 and:0.1797)
		(!:0.1157 and:0.0820 have:0.0413 like:0.0581 bit:0.0325,:0.1650.:0.1602 and:0.1797)
 and
------
		( including:0.0194-:0.1074 not:0.0051
:0.0073-:0.0552ization:0.1328 including:0.0361 including:0.0055)
		(
:0.0299-:0.0850 not:0.0050
:0.0137.:0.0796ization:0.0552 and:0.0371 the:0.0065)
		( and:0.0111,:0.0669 not:0.0052
:0.0047.:0.0630,:0.0312 and:0.0132 the:0.0033)
		( and:0.0066,:0.0549 not:0.0053 great:0.0033.:0.0500,:0.0278 and:0.0107 is:0.0027)
		( and:0.0066,:0.0562 not:0.0071 great:0.0040,:0.0527,:0.0322 and:0.0140 is:0.0040)
		( and:0.0100,:0.0654 not:0.0121 bit:0.0061,:0.0708,:0.0459 and:0.0267 the:0.0068)
		( and:0.0217,:0.0742 not:0.0225 bit:0.0113,:0.1074,:0.0737 and:0.0605 the:0.0160)
		( and:0.0525 was:0.0747 not:0.0386 bit:0.0194,:0.1475,:0.1060 and:0.1250 the:0.0376)
		( and:0.1016 was:0.0801 like:0.0576 bit:0.0272,:0.1748,:0.1260 and:0.1807 the:0.0640)
		( and:0.1182 was:0.0635 like:0.0635 bit:0.0315,:0.1807,:0.1299 and:0.2002 the:0.0737)
		( and:0.1040 was:0.0486 like:0.0620 bit:0.0325,:0.1777.:0.1514 and:0.2002 I:0.1289)
		( and:0.0820 have:0.0413 like:0.0581 bit:0.0325,:0.1650.:0.1602 and:0.1797 I:0.1709)
		( and:0.0820 have:0.0413 like:0.0581 bit:0.0325,:0.1650.:0.1602 and:0.1797 I:0.1709)
 I have a very interesting.
The first time,
950: sample 0: Hello, I'm a language model,
------
		(-:0.0315 including:0.0097-:0.0732 not:0.0035
:0.0037-:0.0630 and:0.0459 including:0.0176)
		(,:0.0767 and:0.0259-:0.1060,:0.0058
:0.0092,:0.1084,:0.0796 and:0.0339)
		(,:0.0771 and:0.0160,:0.1201,:0.0113,:0.0060,:0.1187,:0.0854 and:0.0173)
		(,:0.0557 and:0.0087,:0.1104,:0.0109 great:0.0050,:0.1050,:0.0747 and:0.0116)
		(,:0.0459 and:0.0074,:0.1089,:0.0113 great:0.0058,:0.1074,:0.0806 and:0.0131)
		(,:0.0461 and:0.0104,:0.1206,:0.0134 great:0.0079,:0.1328,:0.1021 and:0.0245)
		(,:0.0613 and:0.0251,:0.1328 not:0.0181 great:0.0121,:0.1709,:0.1338 and:0.0669)
		(,:0.0898 and:0.0693,:0.1006 not:0.0430 great:0.0177,:0.1943 of:0.1602 and:0.1641)
		(,:0.1167 and:0.1138 was:0.0830 not:0.0708 great:0.0214,:0.1895,:0.1650 and:0.2227)
		(,:0.1201 and:0.1060 will:0.0771 not:0.0815 great:0.0226,:0.1865,:0.1719 and:0.2119)
		(,:0.1123 and:0.0806 will:0.0664 not:0.0806 great:0.0217,:0.1855,:0.1729 and:0.1943)
		(.:0.1387 the:0.0669 will:0.0527 not:0.0742 great:0.0200,:0.1709.:0.2139 and:0.1787)
		(.:0.1387 the:0.0669 will:0.0527 not:0.0742 great:0.0200,:0.1709.:0.2139 and:0.1787)
 and
------
		( including:0.0097-:0.0732 not:0.0035
:0.0037-:0.0630 and:0.0459 including:0.0176 including:0.0020)
		( and:0.0259-:0.1060,:0.0058
:0.0092,:0.1084,:0.0796 and:0.0339 is:0.0074)
		( and:0.0160,:0.1201,:0.0113,:0.0060,:0.1187,:0.0854 and:0.0173 to:0.0053)
		( and:0.0087,:0.1104,:0.0109 great:0.0050,:0.1050,:0.0747 and:0.0116 is:0.0054)
		( and:0.0074,:0.1089,:0.0113 great:0.0058,:0.1074,:0.0806 and:0.0131 is:0.0070)
		( and:0.0104,:0.1206,:0.0134 great:0.0079,:0.1328,:0.1021 and:0.0245 is:0.0111)
		( and:0.0251,:0.1328 not:0.0181 great:0.0121,:0.1709,:0.1338 and:0.0669 is:0.0184)
		( and:0.0693,:0.1006 not:0.0430 great:0.0177,:0.1943 of:0.1602 and:0.1641 the:0.0339)
		( and:0.1138 was:0.0830 not:0.0708 great:0.0214,:0.1895,:0.1650 and:0.2227 I:0.0684)
		( and:0.1060 will:0.0771 not:0.0815 great:0.0226,:0.1865,:0.1719 and:0.2119 I:0.1309)
		( and:0.0806 will:0.0664 not:0.0806 great:0.0217,:0.1855,:0.1729 and:0.1943 I:0.1689)
		( the:0.0669 will:0.0527 not:0.0742 great:0.0200,:0.1709.:0.2139 and:0.1787 I:0.1934)
		( the:0.0669 will:0.0527 not:0.0742 great:0.0200,:0.1709.:0.2139 and:0.1787 I:0.1934)
 I'll be able to use the language.
I
1100: sample 0: Hello, I'm a language model,
------
		(
:0.0157 including:0.0034ve:0.0208re:0.0045 civil:0.0021 and:0.0337
:0.0311 including:0.0052)
		(,:0.0444
:0.0114-:0.0518 in:0.0043
:0.0048,:0.0669,:0.0388 and:0.0133)
		(,:0.0576 and:0.0121,:0.0811 in:0.0090,:0.0040,:0.0879,:0.0562 and:0.0130)
		(,:0.0522 and:0.0099,:0.0962 in:0.0090 great:0.0037,:0.0884,:0.0601 and:0.0117)
		(,:0.0508 and:0.0102,:0.1104,:0.0095 great:0.0042,:0.0933,:0.0703 and:0.0153)
		(,:0.0566 and:0.0142,:0.1279,:0.0112 great:0.0055,:0.1089 of:0.1064 and:0.0283)
		(,:0.0737 and:0.0273,:0.1426,:0.0119 lot:0.0101,:0.1270 of:0.1631 and:0.0723)
		(,:0.0991 and:0.0613,:0.1201 not:0.0161 lot:0.0178,:0.1348 of:0.1660 and:0.1543)
		(,:0.1211 and:0.0850�:0.1133 not:0.0288 lot:0.0253,:0.1328,:0.1328 and:0.1670)
		(,:0.1211 the:0.0669�:0.1235 not:0.0391 lot:0.0311,:0.1279.:0.1396 and:0.1260)
		(,:0.1069 the:0.0757�:0.1143 not:0.0439 lot:0.0354,:0.1309.:0.1729 but:0.1143)
		(,:0.0869 the:0.0713�:0.1001 not:0.0442 lot:0.0388.:0.1445.:0.2061 I:0.1060)
		(,:0.0869 the:0.0713�:0.1001 not:0.0442 lot:0.0388.:0.1445.:0.2061 I:0.1060)
 I
------
		( including:0.0034ve:0.0208re:0.0045 civil:0.0021 and:0.0337
:0.0311 including:0.0052ve:0.0214)
		(
:0.0114-:0.0518 in:0.0043
:0.0048,:0.0669,:0.0388 and:0.0133-:0.0515)
		( and:0.0121,:0.0811 in:0.0090,:0.0040,:0.0879,:0.0562 and:0.0130,:0.0532)
		( and:0.0099,:0.0962 in:0.0090 great:0.0037,:0.0884,:0.0601 and:0.0117,:0.0564)
		( and:0.0102,:0.1104,:0.0095 great:0.0042,:0.0933,:0.0703 and:0.0153,:0.0625)
		( and:0.0142,:0.1279,:0.0112 great:0.0055,:0.1089 of:0.1064 and:0.0283,:0.0732)
		( and:0.0273,:0.1426,:0.0119 lot:0.0101,:0.1270 of:0.1631 and:0.0723,:0.0674)
		( and:0.0613,:0.1201 not:0.0161 lot:0.0178,:0.1348 of:0.1660 and:0.1543�:0.0557)
		( and:0.0850�:0.1133 not:0.0288 lot:0.0253,:0.1328,:0.1328 and:0.1670'm:0.1934)
		( the:0.0669�:0.1235 not:0.0391 lot:0.0311,:0.1279.:0.1396 and:0.1260'm:0.3203)
		( the:0.0757�:0.1143 not:0.0439 lot:0.0354,:0.1309.:0.1729 but:0.1143'm:0.3691)
		( the:0.0713�:0.1001 not:0.0442 lot:0.0388.:0.1445.:0.2061 I:0.1060'm:0.3477)
		( the:0.0713�:0.1001 not:0.0442 lot:0.0388.:0.1445.:0.2061 I:0.1060'm:0.3477)
'm a lot of things. I'm a lot of
1250: sample 0: Hello, I'm a language model,
------
		(
:0.0134astic:0.0015ve:0.0120per:0.0035 constant:0.0017 and:0.0520 and:0.0391 including:0.0016)
		(,:0.0503 and:0.0085-:0.0339 in:0.0035 might:0.0024-:0.0598,:0.0364 and:0.0098)
		(,:0.0649 and:0.0115,:0.0515 in:0.0080,:0.0025,:0.0618,:0.0476 and:0.0127)
		(,:0.0593 and:0.0106,:0.0645,:0.0108 great:0.0033,:0.0640,:0.0518 and:0.0143)
		(,:0.0535 and:0.0114,:0.0728,:0.0132 great:0.0051,:0.0654 of:0.0752 and:0.0199)
		(,:0.0500 and:0.0145,:0.0830,:0.0143 great:0.0087,:0.0645 of:0.1211 and:0.0356)
		(,:0.0522 and:0.0239,:0.0879,:0.0135 great:0.0153,:0.0623 of:0.1631 and:0.0801)
		(,:0.0640 and:0.0483,:0.0762 not:0.0205 great:0.0286,:0.0664 of:0.1670 and:0.1553)
		(,:0.0845 and:0.0776�:0.1104 not:0.0344 great:0.0471,:0.0762 of:0.1270 and:0.1660)
		(,:0.1089 and:0.0762�:0.1348 not:0.0452 great:0.0608 that:0.0938.:0.1123 and:0.1270)
		(,:0.1270 I:0.0674�:0.1245 not:0.0481 great:0.0635,:0.1011.:0.1377 I:0.1699)
		(,:0.1328 I:0.1064�:0.1060 not:0.0471 great:0.0615,:0.1055,:0.1514 I:0.2363)
		(,:0.1328 I:0.1064�:0.1060 not:0.0471 great:0.0615,:0.1055,:0.1514 I:0.2363)
 I
------
		(astic:0.0015ve:0.0120per:0.0035 constant:0.0017 and:0.0520 and:0.0391 including:0.0016ve:0.0135)
		( and:0.0085-:0.0339 in:0.0035 might:0.0024-:0.0598,:0.0364 and:0.0098-:0.0371)
		( and:0.0115,:0.0515 in:0.0080,:0.0025,:0.0618,:0.0476 and:0.0127,:0.0349)
		( and:0.0106,:0.0645,:0.0108 great:0.0033,:0.0640,:0.0518 and:0.0143,:0.0427)
		( and:0.0114,:0.0728,:0.0132 great:0.0051,:0.0654 of:0.0752 and:0.0199,:0.0488)
		( and:0.0145,:0.0830,:0.0143 great:0.0087,:0.0645 of:0.1211 and:0.0356,:0.0503)
		( and:0.0239,:0.0879,:0.0135 great:0.0153,:0.0623 of:0.1631 and:0.0801,:0.0452)
		( and:0.0483,:0.0762 not:0.0205 great:0.0286,:0.0664 of:0.1670 and:0.1553�:0.0601)
		( and:0.0776�:0.1104 not:0.0344 great:0.0471,:0.0762 of:0.1270 and:0.1660'm:0.0840)
		( and:0.0762�:0.1348 not:0.0452 great:0.0608 that:0.0938.:0.1123 and:0.1270'm:0.1924)
		( I:0.0674�:0.1245 not:0.0481 great:0.0635,:0.1011.:0.1377 I:0.1699'm:0.2695)
		( I:0.1064�:0.1060 not:0.0471 great:0.0615,:0.1055,:0.1514 I:0.2363'm:0.3125)
		( I:0.1064�:0.1060 not:0.0471 great:0.0615,:0.1055,:0.1514 I:0.2363'm:0.3125)
'm a great way to get a look at the same
1400: sample 0: Hello, I'm a language model,
------
		(
:0.0099owing:0.0011ve:0.0125ach:0.0049rupulous:0.0016 and:0.0476 and:0.0452astic:0.0011)
		(,:0.0400 and:0.0053,:0.0173 added:0.0034 might:0.0017 and:0.0413,:0.0276 and:0.0061)
		(,:0.0605 and:0.0085,:0.0432 in:0.0087 to:0.0019,:0.0479,:0.0439 and:0.0103)
		(,:0.0645 and:0.0091,:0.0620,:0.0139 great:0.0028,:0.0581,:0.0537 and:0.0133)
		(,:0.0591 and:0.0096,:0.0747,:0.0188 great:0.0051,:0.0640 of:0.0791 and:0.0208)
		(,:0.0513 and:0.0123,:0.0850,:0.0206 great:0.0093,:0.0583 of:0.1328 and:0.0376)
		(,:0.0447 and:0.0209,:0.0830 not:0.0221 great:0.0165,:0.0461 of:0.1670 and:0.0840)
		(,:0.0474 and:0.0459�:0.1035 not:0.0437 bit:0.0359 that:0.0703 of:0.1494 and:0.1533)
		(,:0.0566 and:0.0708�:0.1914 not:0.0605 bit:0.0684 that:0.1011 that:0.1416 and:0.1709)
		(,:0.0723 and:0.0684�:0.2266 not:0.0649 bit:0.0903 that:0.1187 that:0.1494 I:0.2002)
		(,:0.0859 I:0.1118�:0.2139 not:0.0583 bit:0.0996 that:0.1211.:0.1709 I:0.3418)
		(,:0.0933 I:0.1729�:0.1660 not:0.0515 bit:0.1021 that:0.1123.:0.2002 I:0.4414)
		(,:0.0933 I:0.1729�:0.1660 not:0.0515 bit:0.1021 that:0.1123.:0.2002 I:0.4414)
 I
------
		(owing:0.0011ve:0.0125ach:0.0049rupulous:0.0016 and:0.0476 and:0.0452astic:0.0011ve:0.0138)
		( and:0.0053,:0.0173 added:0.0034 might:0.0017 and:0.0413,:0.0276 and:0.0061-:0.0181)
		( and:0.0085,:0.0432 in:0.0087 to:0.0019,:0.0479,:0.0439 and:0.0103,:0.0292)
		( and:0.0091,:0.0620,:0.0139 great:0.0028,:0.0581,:0.0537 and:0.0133,:0.0405)
		( and:0.0096,:0.0747,:0.0188 great:0.0051,:0.0640 of:0.0791 and:0.0208,:0.0486)
		( and:0.0123,:0.0850,:0.0206 great:0.0093,:0.0583 of:0.1328 and:0.0376,:0.0486)
		( and:0.0209,:0.0830 not:0.0221 great:0.0165,:0.0461 of:0.1670 and:0.0840 was:0.0554)
		( and:0.0459�:0.1035 not:0.0437 bit:0.0359 that:0.0703 of:0.1494 and:0.1533�:0.0815)
		( and:0.0708�:0.1914 not:0.0605 bit:0.0684 that:0.1011 that:0.1416 and:0.1709'm:0.0981)
		( and:0.0684�:0.2266 not:0.0649 bit:0.0903 that:0.1187 that:0.1494 I:0.2002'm:0.1738)
		( I:0.1118�:0.2139 not:0.0583 bit:0.0996 that:0.1211.:0.1709 I:0.3418'm:0.2061)
		( I:0.1729�:0.1660 not:0.0515 bit:0.1021 that:0.1123.:0.2002 I:0.4414'm:0.2197)
		( I:0.1729�:0.1660 not:0.0515 bit:0.1021 that:0.1123.:0.2002 I:0.4414'm:0.2197)
'm a bit like a bit like a bit like a
1550: sample 0: Hello, I'm a language model,
------
		(
:0.0067anging:0.0012ve:0.0044ach:0.0020rupulous:0.0013 and:0.0282 and:0.0339anging:0.0012)
		( the:0.0334 and:0.0038-:0.0147 in:0.0022 trusted:0.0012 and:0.0247 and:0.0166 and:0.0032)
		(,:0.0752 and:0.0101,:0.0432 in:0.0085 great:0.0017 and:0.0437,:0.0388 and:0.0092)
		(,:0.1011 and:0.0146,:0.0757,:0.0131 great:0.0038,:0.0598 of:0.0786 and:0.0160)
		(,:0.1025 and:0.0170,:0.0952,:0.0192 great:0.0083,:0.0674 of:0.1504 and:0.0289)
		(,:0.0854 and:0.0192,:0.0977,:0.0226 great:0.0172,:0.0601 of:0.2188 and:0.0576)
		(,:0.0703 and:0.0262,:0.0957,:0.0239 great:0.0327 and:0.0488 of:0.2539 and:0.1172)
		(,:0.0669 and:0.0398,:0.0791 not:0.0400 great:0.0476 that:0.0688 of:0.2383 and:0.1748)
		(,:0.0728 and:0.0496 was:0.0903 not:0.0562 great:0.0552 that:0.0967 of:0.2129 and:0.1719)
		(,:0.0859 and:0.0486 was:0.0879 sure:0.0723 great:0.0508 that:0.1099 of:0.1738 I:0.1631)
		(,:0.1055 you:0.0576 was:0.0684 sure:0.0894 great:0.0427 that:0.1050.:0.1592 I:0.2656)
		(.:0.1245 I:0.0977 have:0.0522 sure:0.0991 bit:0.0471 that:0.0850.:0.1963 I:0.3516)
		(.:0.1245 I:0.0977 have:0.0522 sure:0.0991 bit:0.0471 that:0.0850.:0.1963 I:0.3516)
 I
------
		(anging:0.0012ve:0.0044ach:0.0020rupulous:0.0013 and:0.0282 and:0.0339anging:0.0012ve:0.0042)
		( and:0.0038-:0.0147 in:0.0022 trusted:0.0012 and:0.0247 and:0.0166 and:0.0032-:0.0132)
		( and:0.0101,:0.0432 in:0.0085 great:0.0017 and:0.0437,:0.0388 and:0.0092,:0.0256)
		( and:0.0146,:0.0757,:0.0131 great:0.0038,:0.0598 of:0.0786 and:0.0160,:0.0452)
		( and:0.0170,:0.0952,:0.0192 great:0.0083,:0.0674 of:0.1504 and:0.0289,:0.0537)
		( and:0.0192,:0.0977,:0.0226 great:0.0172,:0.0601 of:0.2188 and:0.0576,:0.0505)
		( and:0.0262,:0.0957,:0.0239 great:0.0327 and:0.0488 of:0.2539 and:0.1172 was:0.0603)
		( and:0.0398,:0.0791 not:0.0400 great:0.0476 that:0.0688 of:0.2383 and:0.1748 was:0.0518)
		( and:0.0496 was:0.0903 not:0.0562 great:0.0552 that:0.0967 of:0.2129 and:0.1719'm:0.1094)
		( and:0.0486 was:0.0879 sure:0.0723 great:0.0508 that:0.1099 of:0.1738 I:0.1631'm:0.1611)
		( you:0.0576 was:0.0684 sure:0.0894 great:0.0427 that:0.1050.:0.1592 I:0.2656'm:0.1846)
		( I:0.0977 have:0.0522 sure:0.0991 bit:0.0471 that:0.0850.:0.1963 I:0.3516'm:0.1865)
		( I:0.0977 have:0.0522 sure:0.0991 bit:0.0471 that:0.0850.:0.1963 I:0.3516'm:0.1865)
'm a bit like a language model.
I've
1700: sample 0: Hello, I'm a language model,
------
		( and:0.0075anging:0.0011ve:0.0032ack:0.0024rupulous:0.0012 and:0.0422 and:0.0221anging:0.0009)
		(,:0.0398 and:0.0024-:0.0129 also:0.0041 trusted:0.0011 and:0.0265,:0.0112 and:0.0019)
		(,:0.0967 and:0.0062,:0.0356 in:0.0064 great:0.0015,:0.0459,:0.0288 and:0.0052)
		(,:0.1387 and:0.0089,:0.0684,:0.0117 great:0.0034,:0.0732 of:0.0762 and:0.0091)
		(,:0.1348 and:0.0112,:0.0889,:0.0151 great:0.0072,:0.0737 of:0.1396 and:0.0161)
		(,:0.1016 and:0.0142,:0.0879,:0.0139 great:0.0117 and:0.0559 of:0.1592 and:0.0347)
		(,:0.0776 and:0.0242,:0.0776 not:0.0189 great:0.0190 and:0.0459 of:0.1709 and:0.0952)
		(,:0.0708 and:0.0454 was:0.0762 not:0.0383 great:0.0298,:0.0422 of:0.1670 and:0.1758)
		(,:0.0801 and:0.0654 was:0.0845 not:0.0623 very:0.0347,:0.0510 of:0.1445 and:0.2158)
		(,:0.1040 and:0.0688 was:0.0781 not:0.0718 very:0.0396.:0.0684.:0.1475 and:0.2002)
		(,:0.1514 the:0.0718'm:0.0923 not:0.0669 bit:0.0415.:0.0928.:0.1865 I:0.1807)
		(,:0.1924 the:0.0859'm:0.1167 not:0.0559 bit:0.0427.:0.1118.:0.2246 I:0.2158)
		(,:0.1924 the:0.0859'm:0.1167 not:0.0559 bit:0.0427.:0.1118.:0.2246 I:0.2158)
 I
------
		(anging:0.0011ve:0.0032ack:0.0024rupulous:0.0012 and:0.0422 and:0.0221anging:0.0009ve:0.0032)
		( and:0.0024-:0.0129 also:0.0041 trusted:0.0011 and:0.0265,:0.0112 and:0.0019-:0.0134)
		( and:0.0062,:0.0356 in:0.0064 great:0.0015,:0.0459,:0.0288 and:0.0052,:0.0210)
		( and:0.0089,:0.0684,:0.0117 great:0.0034,:0.0732 of:0.0762 and:0.0091,:0.0364)
		( and:0.0112,:0.0889,:0.0151 great:0.0072,:0.0737 of:0.1396 and:0.0161,:0.0393)
		( and:0.0142,:0.0879,:0.0139 great:0.0117 and:0.0559 of:0.1592 and:0.0347 was:0.0459)
		( and:0.0242,:0.0776 not:0.0189 great:0.0190 and:0.0459 of:0.1709 and:0.0952'm:0.0547)
		( and:0.0454 was:0.0762 not:0.0383 great:0.0298,:0.0422 of:0.1670 and:0.1758'm:0.1807)
		( and:0.0654 was:0.0845 not:0.0623 very:0.0347,:0.0510 of:0.1445 and:0.2158'm:0.3477)
		( and:0.0688 was:0.0781 not:0.0718 very:0.0396.:0.0684.:0.1475 and:0.2002'm:0.4590)
		( the:0.0718'm:0.0923 not:0.0669 bit:0.0415.:0.0928.:0.1865 I:0.1807'm:0.5195)
		( the:0.0859'm:0.1167 not:0.0559 bit:0.0427.:0.1118.:0.2246 I:0.2158'm:0.5625)
		( the:0.0859'm:0.1167 not:0.0559 bit:0.0427.:0.1118.:0.2246 I:0.2158'm:0.5625)
'm a language model. I'm a language model,
1850: sample 0: Hello, I'm a language model,
------
		(
:0.0073 influencing:0.0010ve:0.0018ll:0.0016rupulous:0.0011 and:0.0262 and:0.0187 influencing:0.0010)
		( the:0.0349 and:0.0018-:0.0075 also:0.0036 genuine:0.0010 and:0.0167 and:0.0089 and:0.0014)
		(,:0.0825 and:0.0059,:0.0221 in:0.0066 great:0.0014 and:0.0334 of:0.0315 and:0.0048)
		(,:0.1191 and:0.0089,:0.0403,:0.0095 great:0.0028,:0.0447 of:0.0747 and:0.0081)
		(,:0.1089 and:0.0098,:0.0427,:0.0096 great:0.0052 and:0.0437 of:0.0957 and:0.0112)
		(,:0.0796 and:0.0109,:0.0393 not:0.0098 great:0.0082 and:0.0305 of:0.0845 and:0.0242)
		(,:0.0581 and:0.0145,:0.0383 not:0.0177 great:0.0150 and:0.0253 of:0.0928 and:0.0693)
		(,:0.0479 and:0.0243�:0.0518 not:0.0339 great:0.0269 that:0.0388 of:0.1221 and:0.1099)
		(,:0.0474 and:0.0356�:0.1113 not:0.0559 great:0.0352 that:0.0559 that:0.1406 and:0.1445)
		(,:0.0605 and:0.0396�:0.1582 not:0.0640 very:0.0359 that:0.0718 that:0.1631 and:0.1523)
		(,:0.0947 the:0.0527�:0.1543 a:0.0569 very:0.0376 that:0.0815 that:0.1572 and:0.1436)
		(,:0.1426 I:0.0640�:0.1123 a:0.0513 good:0.0444 that:0.0752.:0.1826 I:0.1875)
		(,:0.1426 I:0.0640�:0.1123 a:0.0513 good:0.0444 that:0.0752.:0.1826 I:0.1875)
 I
------
		( influencing:0.0010ve:0.0018ll:0.0016rupulous:0.0011 and:0.0262 and:0.0187 influencing:0.0010ll:0.0018)
		( and:0.0018-:0.0075 also:0.0036 genuine:0.0010 and:0.0167 and:0.0089 and:0.0014-:0.0068)
		( and:0.0059,:0.0221 in:0.0066 great:0.0014 and:0.0334 of:0.0315 and:0.0048,:0.0124)
		( and:0.0089,:0.0403,:0.0095 great:0.0028,:0.0447 of:0.0747 and:0.0081,:0.0204)
		( and:0.0098,:0.0427,:0.0096 great:0.0052 and:0.0437 of:0.0957 and:0.0112,:0.0184)
		( and:0.0109,:0.0393 not:0.0098 great:0.0082 and:0.0305 of:0.0845 and:0.0242 was:0.0195)
		( and:0.0145,:0.0383 not:0.0177 great:0.0150 and:0.0253 of:0.0928 and:0.0693'm:0.0625)
		( and:0.0243�:0.0518 not:0.0339 great:0.0269 that:0.0388 of:0.1221 and:0.1099'm:0.2930)
		( and:0.0356�:0.1113 not:0.0559 great:0.0352 that:0.0559 that:0.1406 and:0.1445'm:0.5352)
		( and:0.0396�:0.1582 not:0.0640 very:0.0359 that:0.0718 that:0.1631 and:0.1523'm:0.6172)
		( the:0.0527�:0.1543 a:0.0569 very:0.0376 that:0.0815 that:0.1572 and:0.1436'm:0.6406)
		( I:0.0640�:0.1123 a:0.0513 good:0.0444 that:0.0752.:0.1826 I:0.1875'm:0.6211)
		( I:0.0640�:0.1123 a:0.0513 good:0.0444 that:0.0752.:0.1826 I:0.1875'm:0.6211)
'm a language model, I'm a language model,
2000: sample 0: Hello, I'm a language model,
------
		(
:0.0067anging:0.0008 needed:0.0018aintain:0.0016rupulous:0.0014 and:0.0226 and:0.0091 aka:0.0010)
		( and:0.0349 and:0.0015-:0.0050 also:0.0020 particular:0.0008 and:0.0178 and:0.0058 thus:0.0014)
		(,:0.1123 and:0.0069,:0.0233,:0.0057 particular:0.0019 and:0.0505 of:0.0208 and:0.0055)
		(,:0.1758 and:0.0115,:0.0491,:0.0126 great:0.0043 and:0.0723 of:0.0500 and:0.0102)
		(,:0.1543 and:0.0146,:0.0486,:0.0139 great:0.0081 and:0.0684 of:0.0625 and:0.0178)
		(,:0.1069 and:0.0157 was:0.0322 not:0.0146 great:0.0120 and:0.0378 of:0.0552 and:0.0461)
		(,:0.0742 and:0.0205 will:0.0474 not:0.0322 great:0.0197 and:0.0238 of:0.0635 and:0.1079)
		(,:0.0605 and:0.0344 will:0.0640 not:0.0669 great:0.0306,:0.0248 of:0.0830 and:0.1494)
		(,:0.0625 you:0.0566 will:0.0630 not:0.1099 little:0.0376,:0.0369 for:0.1094 and:0.1709)
		(,:0.0947 you:0.1006 am:0.0771 not:0.1235 little:0.0466,:0.0552.:0.1738 and:0.1865)
		(,:0.1650 you:0.1245 am:0.0776 not:0.1147 little:0.0530.:0.0874.:0.2598 I:0.1934)
		(,:0.2080 you:0.1377 have:0.0728 going:0.0938 little:0.0588.:0.1099.:0.3457 I:0.1963)
		(,:0.2080 you:0.1377 have:0.0728 going:0.0938 little:0.0588.:0.1099.:0.3457 I:0.1963)
 I
------
		(anging:0.0008 needed:0.0018aintain:0.0016rupulous:0.0014 and:0.0226 and:0.0091 aka:0.0010th:0.0015)
		( and:0.0015-:0.0050 also:0.0020 particular:0.0008 and:0.0178 and:0.0058 thus:0.0014-:0.0045)
		( and:0.0069,:0.0233,:0.0057 particular:0.0019 and:0.0505 of:0.0208 and:0.0055,:0.0123)
		( and:0.0115,:0.0491,:0.0126 great:0.0043 and:0.0723 of:0.0500 and:0.0102,:0.0231)
		( and:0.0146,:0.0486,:0.0139 great:0.0081 and:0.0684 of:0.0625 and:0.0178 would:0.0242)
		( and:0.0157 was:0.0322 not:0.0146 great:0.0120 and:0.0378 of:0.0552 and:0.0461'm:0.0352)
		( and:0.0205 will:0.0474 not:0.0322 great:0.0197 and:0.0238 of:0.0635 and:0.1079'm:0.1875)
		( and:0.0344 will:0.0640 not:0.0669 great:0.0306,:0.0248 of:0.0830 and:0.1494'm:0.4180)
		( you:0.0566 will:0.0630 not:0.1099 little:0.0376,:0.0369 for:0.1094 and:0.1709'm:0.5312)
		( you:0.1006 am:0.0771 not:0.1235 little:0.0466,:0.0552.:0.1738 and:0.1865'm:0.5469)
		( you:0.1245 am:0.0776 not:0.1147 little:0.0530.:0.0874.:0.2598 I:0.1934'm:0.4941)
		( you:0.1377 have:0.0728 going:0.0938 little:0.0588.:0.1099.:0.3457 I:0.1963'm:0.4297)
		( you:0.1377 have:0.0728 going:0.0938 little:0.0588.:0.1099.:0.3457 I:0.1963'm:0.4297)
'm a language model. I'm a language model.
2150: sample 0: Hello, I'm a language model,
------
		( and:0.0033akable:0.0009 needed:0.0019aintain:0.0020akable:0.0012
:0.0107
:0.0041hooting:0.0008)
		( and:0.0383 and:0.0013 can:0.0041 also:0.0024 particular:0.0007 and:0.0156 and:0.0047 implying:0.0010)
		(,:0.1250 and:0.0076,:0.0240,:0.0077 particular:0.0022 and:0.0532 of:0.0272 and:0.0056)
		(,:0.1670 and:0.0125,:0.0461,:0.0155 wonderful:0.0053 and:0.0767 of:0.0786 and:0.0094)
		(,:0.1318 and:0.0131,:0.0391,:0.0139 wonderful:0.0115 and:0.0728 of:0.0859 and:0.0122)
		(,:0.0859 and:0.0112 was:0.0320 not:0.0227 wonderful:0.0157 and:0.0413 of:0.0708 and:0.0337)
		(,:0.0552 and:0.0121 was:0.0376 not:0.0476 wonderful:0.0228 and:0.0306 of:0.0869 and:0.0596)
		(,:0.0403 and:0.0186 was:0.0469 not:0.0894 wonderful:0.0238 that:0.0339 of:0.1138 and:0.0879)
		(,:0.0374 but:0.0330 am:0.0649 not:0.1338 very:0.0280 that:0.0547 of:0.1270 and:0.1167)
		(,:0.0466 but:0.0510 am:0.0962 not:0.1572 very:0.0349 that:0.0781 that:0.1289 I:0.1562)
		(,:0.0728 and:0.0654 am:0.1045 not:0.1387 very:0.0356,:0.0913.:0.1611 I:0.1797)
		(,:0.1118 and:0.0791 have:0.0942 not:0.1064 very:0.0306,:0.1001.:0.2158 and:0.1797)
		(,:0.1118 and:0.0791 have:0.0942 not:0.1064 very:0.0306,:0.1001.:0.2158 and:0.1797)
 and
------
		(akable:0.0009 needed:0.0019aintain:0.0020akable:0.0012
:0.0107
:0.0041hooting:0.0008ifle:0.0006)
		( and:0.0013 can:0.0041 also:0.0024 particular:0.0007 and:0.0156 and:0.0047 implying:0.0010 thus:0.0011)
		( and:0.0076,:0.0240,:0.0077 particular:0.0022 and:0.0532 of:0.0272 and:0.0056 thus:0.0014)
		( and:0.0125,:0.0461,:0.0155 wonderful:0.0053 and:0.0767 of:0.0786 and:0.0094 therefore:0.0020)
		( and:0.0131,:0.0391,:0.0139 wonderful:0.0115 and:0.0728 of:0.0859 and:0.0122 therefore:0.0050)
		( and:0.0112 was:0.0320 not:0.0227 wonderful:0.0157 and:0.0413 of:0.0708 and:0.0337 therefore:0.0192)
		( and:0.0121 was:0.0376 not:0.0476 wonderful:0.0228 and:0.0306 of:0.0869 and:0.0596 I:0.0393)
		( and:0.0186 was:0.0469 not:0.0894 wonderful:0.0238 that:0.0339 of:0.1138 and:0.0879 I:0.1270)
		( but:0.0330 am:0.0649 not:0.1338 very:0.0280 that:0.0547 of:0.1270 and:0.1167 I:0.3086)
		( but:0.0510 am:0.0962 not:0.1572 very:0.0349 that:0.0781 that:0.1289 I:0.1562 I:0.5078)
		( and:0.0654 am:0.1045 not:0.1387 very:0.0356,:0.0913.:0.1611 I:0.1797 I:0.5859)
		( and:0.0791 have:0.0942 not:0.1064 very:0.0306,:0.1001.:0.2158 and:0.1797 I:0.5664)
		( and:0.0791 have:0.0942 not:0.1064 very:0.0306,:0.1001.:0.2158 and:0.1797 I:0.5664)
 I'm a language. I'm a language that I
2300: sample 0: Hello, I'm a language model,
------
		(
:0.0027akable:0.0010 needed:0.0011 needed:0.0020akable:0.0015 and:0.0100 and:0.0033 aka:0.0009)
		( and:0.0304 and:0.0009-:0.0027 also:0.0016 pointed:0.0006 and:0.0117 and:0.0037 implying:0.0008)
		(,:0.1250 and:0.0057,:0.0148,:0.0054 short:0.0015 and:0.0432 of:0.0178 and:0.0041)
		(,:0.1631 and:0.0095,:0.0309,:0.0094 great:0.0041 and:0.0544 of:0.0454 and:0.0076)
		(,:0.1270 and:0.0103,:0.0276 not:0.0103 great:0.0071 and:0.0435 of:0.0486 and:0.0131)
		(,:0.0806 and:0.0079 was:0.0198 not:0.0138 great:0.0097 and:0.0226 of:0.0410 and:0.0356)
		(,:0.0505 and:0.0072 was:0.0248 not:0.0232 great:0.0178 and:0.0179 of:0.0569 and:0.0625)
		(,:0.0352 you:0.0140 was:0.0347 not:0.0430 great:0.0254 that:0.0248 of:0.0801 and:0.0947)
		(,:0.0295 you:0.0417�:0.0562 not:0.0713 very:0.0293 that:0.0391 that:0.1089 and:0.1240)
		(,:0.0352 you:0.0859�:0.0708 not:0.0928 very:0.0339 that:0.0520.:0.1445 and:0.1289)
		(,:0.0569 you:0.1152�:0.0664 not:0.0928 very:0.0317 that:0.0532.:0.2031 I:0.1729)
		(,:0.0928 you:0.1201 have:0.0674 not:0.0723 good:0.0347.:0.0530.:0.2520 I:0.1992)
		(,:0.0928 you:0.1201 have:0.0674 not:0.0723 good:0.0347.:0.0530.:0.2520 I:0.1992)
 I
------
		(akable:0.0010 needed:0.0011 needed:0.0020akable:0.0015 and:0.0100 and:0.0033 aka:0.0009 needed:0.0010)
		( and:0.0009-:0.0027 also:0.0016 pointed:0.0006 and:0.0117 and:0.0037 implying:0.0008-:0.0026)
		( and:0.0057,:0.0148,:0.0054 short:0.0015 and:0.0432 of:0.0178 and:0.0041,:0.0087)
		( and:0.0095,:0.0309,:0.0094 great:0.0041 and:0.0544 of:0.0454 and:0.0076,:0.0166)
		( and:0.0103,:0.0276 not:0.0103 great:0.0071 and:0.0435 of:0.0486 and:0.0131,:0.0153)
		( and:0.0079 was:0.0198 not:0.0138 great:0.0097 and:0.0226 of:0.0410 and:0.0356'm:0.0442)
		( and:0.0072 was:0.0248 not:0.0232 great:0.0178 and:0.0179 of:0.0569 and:0.0625'm:0.1680)
		( you:0.0140 was:0.0347 not:0.0430 great:0.0254 that:0.0248 of:0.0801 and:0.0947'm:0.3496)
		( you:0.0417�:0.0562 not:0.0713 very:0.0293 that:0.0391 that:0.1089 and:0.1240'm:0.4668)
		( you:0.0859�:0.0708 not:0.0928 very:0.0339 that:0.0520.:0.1445 and:0.1289'm:0.4902)
		( you:0.1152�:0.0664 not:0.0928 very:0.0317 that:0.0532.:0.2031 I:0.1729'm:0.4707)
		( you:0.1201 have:0.0674 not:0.0723 good:0.0347.:0.0530.:0.2520 I:0.1992'm:0.4473)
		( you:0.1201 have:0.0674 not:0.0723 good:0.0347.:0.0530.:0.2520 I:0.1992'm:0.4473)
'm a language. I'm a language teacher, I
2450: sample 0: Hello, I'm a language model,
------
		( and:0.0020akable:0.0010 needed:0.0011phy:0.0019 coaster:0.0013 and:0.0037 and:0.0026hooting:0.0009)
		( and:0.0376 mosques:0.0007-:0.0022 pointed:0.0008 mosques:0.0006 and:0.0070 and:0.0028 although:0.0008)
		(,:0.1196 and:0.0039,:0.0114 not:0.0034 great:0.0019 and:0.0277 of:0.0211 although:0.0032)
		(,:0.1514 and:0.0074,:0.0273 not:0.0070 great:0.0051 and:0.0381 of:0.0500 and:0.0056)
		(,:0.1162 and:0.0093,:0.0288 not:0.0121 great:0.0068 and:0.0344 of:0.0571 and:0.0170)
		(,:0.0723 and:0.0099 will:0.0283 not:0.0182 great:0.0097 and:0.0229 of:0.0591 and:0.0452)
		(,:0.0444 and:0.0122 will:0.0381 not:0.0339 great:0.0175 language:0.0242 of:0.0801 and:0.0500)
		(,:0.0295 and:0.0162 will:0.0530 not:0.0664 great:0.0214,:0.0247 of:0.1099 and:0.0708)
		(,:0.0245 and:0.0239 will:0.0659 not:0.1113 friend:0.0283,:0.0388 of:0.1309 but:0.1108)
		(.:0.0322 but:0.0364 am:0.0874 not:0.1436 friend:0.0347.:0.0601.:0.1660 but:0.1514)
		(.:0.0659 and:0.0437 am:0.0952 not:0.1455 friend:0.0364.:0.0771.:0.2295 but:0.1465)
		(.:0.1348 I:0.0728 am:0.0869 not:0.1230 friend:0.0354.:0.0840.:0.2988 I:0.1553)
		(.:0.1348 I:0.0728 am:0.0869 not:0.1230 friend:0.0354.:0.0840.:0.2988 I:0.1553)
 I
------
		(akable:0.0010 needed:0.0011phy:0.0019 coaster:0.0013 and:0.0037 and:0.0026hooting:0.0009EF:0.0009)
		( mosques:0.0007-:0.0022 pointed:0.0008 mosques:0.0006 and:0.0070 and:0.0028 although:0.0008-:0.0019)
		( and:0.0039,:0.0114 not:0.0034 great:0.0019 and:0.0277 of:0.0211 although:0.0032,:0.0078)
		( and:0.0074,:0.0273 not:0.0070 great:0.0051 and:0.0381 of:0.0500 and:0.0056,:0.0190)
		( and:0.0093,:0.0288 not:0.0121 great:0.0068 and:0.0344 of:0.0571 and:0.0170,:0.0254)
		( and:0.0099 will:0.0283 not:0.0182 great:0.0097 and:0.0229 of:0.0591 and:0.0452'm:0.0481)
		( and:0.0122 will:0.0381 not:0.0339 great:0.0175 language:0.0242 of:0.0801 and:0.0500'm:0.1680)
		( and:0.0162 will:0.0530 not:0.0664 great:0.0214,:0.0247 of:0.1099 and:0.0708'm:0.3301)
		( and:0.0239 will:0.0659 not:0.1113 friend:0.0283,:0.0388 of:0.1309 but:0.1108'm:0.4590)
		( but:0.0364 am:0.0874 not:0.1436 friend:0.0347.:0.0601.:0.1660 but:0.1514'm:0.5039)
		( and:0.0437 am:0.0952 not:0.1455 friend:0.0364.:0.0771.:0.2295 but:0.1465'm:0.5234)
		( I:0.0728 am:0.0869 not:0.1230 friend:0.0354.:0.0840.:0.2988 I:0.1553'm:0.5312)
		( I:0.0728 am:0.0869 not:0.1230 friend:0.0354.:0.0840.:0.2988 I:0.1553'm:0.5312)
'm a language model, I'm a language model,
