1: sample 0: Hello, I'm a language model,
------
		(Hello:0.0004,:0.0004 I:0.0005'm:0.0005 a:0.0004 language:0.0007 model:0.0007,:0.0005)
		( Labrador:0.0004,:0.0003ovic:0.0002之:0.0002 red:0.0002 Earlier:0.0002 purchaser:0.0002,:0.0003)
		( Labrador:0.0004,:0.0003 the:0.0002 Identified:0.0002 cheeks:0.0002 the:0.0002 purchaser:0.0002,:0.0003)
		( Labrador:0.0003,:0.0003 the:0.0002 Identified:0.0002 the:0.0002 the:0.0003 the:0.0002,:0.0003)
		( Labrador:0.0002,:0.0003 the:0.0003 Identified:0.0002 the:0.0002 the:0.0003 the:0.0003,:0.0003)
		( pollut:0.0002,:0.0003 the:0.0003 parody:0.0002 the:0.0003 the:0.0003 the:0.0003 the:0.0003)
		( pollut:0.0002,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0003 the:0.0003)
		( the:0.0002,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003)
		( the:0.0002,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003)
		( the:0.0002,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003)
		( the:0.0003,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003)
		( the:0.0003,:0.0003 the:0.0004rape:0.0003 the:0.0003 the:0.0004 the:0.0004 the:0.0003)
		( the:0.0003,:0.0003 the:0.0004rape:0.0003 the:0.0003 the:0.0004 the:0.0004 the:0.0003)
 the
------
		(,:0.0004 I:0.0005'm:0.0005 a:0.0004 language:0.0007 model:0.0007,:0.0005 the:0.0006)
		(,:0.0003ovic:0.0002之:0.0002 red:0.0002 Earlier:0.0002 purchaser:0.0002,:0.0003 the:0.0003)
		(,:0.0003 the:0.0002 Identified:0.0002 cheeks:0.0002 the:0.0002 purchaser:0.0002,:0.0003 the:0.0003)
		(,:0.0003 the:0.0002 Identified:0.0002 the:0.0002 the:0.0003 the:0.0002,:0.0003 the:0.0003)
		(,:0.0003 the:0.0003 Identified:0.0002 the:0.0002 the:0.0003 the:0.0003,:0.0003 the:0.0003)
		(,:0.0003 the:0.0003 parody:0.0002 the:0.0003 the:0.0003 the:0.0003 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0003 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0003rape:0.0002 the:0.0003 the:0.0003 the:0.0004 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0004rape:0.0003 the:0.0003 the:0.0004 the:0.0004 the:0.0003 the:0.0003)
		(,:0.0003 the:0.0004rape:0.0003 the:0.0003 the:0.0004 the:0.0004 the:0.0003 the:0.0003)
 the the the the the the the the the the the
50: sample 0: Hello, I'm a language model,
------
		( Radeon:0.0002 and:0.2539-:0.0493ify:0.0003 a:0.0264ly:0.0006ly:0.0007 and:0.2949)
		(ing:0.0026 and:0.0850,:0.0830 the:0.1196 a:0.0159,:0.0542,:0.0649 and:0.0928)
		(,:0.0112 and:0.0732,:0.0771 the:0.0908 be:0.0115,:0.0811,:0.0845 and:0.0811)
		(,:0.0266 and:0.0679.:0.0752 the:0.0737 be:0.0101,:0.0918,:0.0879 and:0.0688)
		(,:0.0415 and:0.0635.:0.0688,:0.0708 be:0.0090,:0.0928,:0.0894 and:0.0625)
		(,:0.0527 and:0.0591.:0.0613,:0.0654�:0.0087,:0.0898,:0.0825 and:0.0564)
		(,:0.0623 and:0.0549.:0.0544 of:0.0654�:0.0089,:0.0884,:0.0815 and:0.0505)
		(,:0.0708 and:0.0508.:0.0479 of:0.0615�:0.0091.:0.0879,:0.0762 and:0.0454)
		(,:0.0762 and:0.0469.:0.0417 of:0.0581�:0.0093.:0.0869.:0.0767 and:0.0403)
		(,:0.0796 and:0.0430 and:0.0388 to:0.0569�:0.0096.:0.0869.:0.0723 and:0.0359)
		(,:0.0830 and:0.0393 and:0.0381 to:0.0574�:0.0099.:0.0815.:0.0703 the:0.0371)
		(,:0.0845 and:0.0349 and:0.0359 to:0.0562�:0.0099.:0.0820.:0.0664 the:0.0396)
		(,:0.0845 and:0.0349 and:0.0359 to:0.0562�:0.0099.:0.0820.:0.0664 the:0.0396)
 the
------
		( and:0.2539-:0.0493ify:0.0003 a:0.0264ly:0.0006ly:0.0007 and:0.2949 a:0.0210)
		( and:0.0850,:0.0830 the:0.1196 a:0.0159,:0.0542,:0.0649 and:0.0928 be:0.0107)
		( and:0.0732,:0.0771 the:0.0908 be:0.0115,:0.0811,:0.0845 and:0.0811�:0.0107)
		( and:0.0679.:0.0752 the:0.0737 be:0.0101,:0.0918,:0.0879 and:0.0688�:0.0129)
		( and:0.0635.:0.0688,:0.0708 be:0.0090,:0.0928,:0.0894 and:0.0625�:0.0144)
		( and:0.0591.:0.0613,:0.0654�:0.0087,:0.0898,:0.0825 and:0.0564�:0.0156)
		( and:0.0549.:0.0544 of:0.0654�:0.0089,:0.0884,:0.0815 and:0.0505�:0.0164)
		( and:0.0508.:0.0479 of:0.0615�:0.0091.:0.0879,:0.0762 and:0.0454�:0.0179)
		( and:0.0469.:0.0417 of:0.0581�:0.0093.:0.0869.:0.0767 and:0.0403�:0.0189)
		( and:0.0430 and:0.0388 to:0.0569�:0.0096.:0.0869.:0.0723 and:0.0359�:0.0194)
		( and:0.0393 and:0.0381 to:0.0574�:0.0099.:0.0815.:0.0703 the:0.0371�:0.0206)
		( and:0.0349 and:0.0359 to:0.0562�:0.0099.:0.0820.:0.0664 the:0.0396�:0.0219)
		( and:0.0349 and:0.0359 to:0.0562�:0.0099.:0.0820.:0.0664 the:0.0396�:0.0219)
�s and the�-�-�-�
200: sample 0: Hello, I'm a language model,
------
		(ribe:0.0166 although:0.0078�:0.1611 not:0.0559 own:0.0026 States:0.1240 States:0.0515 please:0.0058)
		( the:0.1260 and:0.3867-:0.1992 the:0.1797
:0.0957 of:0.1963 of:0.1621 and:0.4395)
		( the:0.1445 and:0.3730 is:0.1177 the:0.1611
:0.0938 of:0.2119 of:0.2207 and:0.4199)
		( the:0.1167 and:0.3574 is:0.1357 the:0.1240
:0.0781 of:0.1914 of:0.2295 and:0.3926)
		( the:0.0913 and:0.3281 is:0.1426 a:0.1045
:0.0510 of:0.1621 of:0.2178 and:0.3418)
		( the:0.0728 and:0.2988 is:0.1201 a:0.0879
:0.0294.:0.1729 of:0.1992 and:0.2832)
		(,:0.0684 and:0.2715 is:0.0913 a:0.0684
:0.0192.:0.1885 of:0.1758 and:0.2334)
		(,:0.0815 and:0.2334 is:0.0659 that:0.0613 �:0.0156.:0.2070.:0.1553 and:0.1992)
		(,:0.0962 and:0.2100 is:0.0461 that:0.0535 �:0.0181.:0.2119.:0.1680 and:0.1689)
		(,:0.1064 and:0.1885 was:0.0361 you:0.0583 �:0.0214.:0.2188.:0.1797 and:0.1504)
		(,:0.1128 and:0.1689�:0.0349 you:0.0630 �:0.0254.:0.2227.:0.1914 and:0.1338)
		(,:0.1138 and:0.1504�:0.0374 you:0.0645 �:0.0287.:0.2256.:0.1943 and:0.1201)
		(,:0.1138 and:0.1504�:0.0374 you:0.0645 �:0.0287.:0.2256.:0.1943 and:0.1201)
 and
------
		( although:0.0078�:0.1611 not:0.0559 own:0.0026 States:0.1240 States:0.0515 please:0.0058 please:0.0056)
		( and:0.3867-:0.1992 the:0.1797
:0.0957 of:0.1963 of:0.1621 and:0.4395 the:0.4336)
		( and:0.3730 is:0.1177 the:0.1611
:0.0938 of:0.2119 of:0.2207 and:0.4199 the:0.3730)
		( and:0.3574 is:0.1357 the:0.1240
:0.0781 of:0.1914 of:0.2295 and:0.3926 the:0.2930)
		( and:0.3281 is:0.1426 a:0.1045
:0.0510 of:0.1621 of:0.2178 and:0.3418 the:0.2119)
		( and:0.2988 is:0.1201 a:0.0879
:0.0294.:0.1729 of:0.1992 and:0.2832 the:0.1484)
		( and:0.2715 is:0.0913 a:0.0684
:0.0192.:0.1885 of:0.1758 and:0.2334 the:0.1050)
		( and:0.2334 is:0.0659 that:0.0613 �:0.0156.:0.2070.:0.1553 and:0.1992 the:0.0757)
		( and:0.2100 is:0.0461 that:0.0535 �:0.0181.:0.2119.:0.1680 and:0.1689 the:0.0603)
		( and:0.1885 was:0.0361 you:0.0583 �:0.0214.:0.2188.:0.1797 and:0.1504 the:0.0488)
		( and:0.1689�:0.0349 you:0.0630 �:0.0254.:0.2227.:0.1914 and:0.1338 the:0.0415)
		( and:0.1504�:0.0374 you:0.0645 �:0.0287.:0.2256.:0.1943 and:0.1201 the:0.0376)
		( and:0.1504�:0.0374 you:0.0645 �:0.0287.:0.2256.:0.1943 and:0.1201 the:0.0376)
 the first, and the first, and the first,
350: sample 0: Hello, I'm a language model,
------
		( a:0.0253 whereas:0.0078ll:0.0635 a:0.0586 versa:0.0046ical:0.0233ically:0.0461inate:0.0078)
		(,:0.1689 and:0.3184,:0.1069 a:0.1484 the:0.1426 of:0.1875 of:0.1836 and:0.4043)
		(,:0.2266 and:0.4141,:0.1953 a:0.1328,:0.1523,:0.1914 of:0.1709 and:0.4785)
		(,:0.2520 and:0.4414,:0.2480 a:0.1162,:0.1709,:0.2285 of:0.1543 and:0.4844)
		(,:0.2598 and:0.4375,:0.2617 a:0.1030,:0.1611,:0.2520 of:0.1533 and:0.4688)
		(,:0.2471 and:0.4219,:0.2334 a:0.0957,:0.1177,:0.2695,:0.1436 and:0.4434)
		(,:0.2324 and:0.3828,:0.1680 a:0.0801,:0.0640,:0.2676,:0.1514 and:0.3848)
		(,:0.2080 and:0.3086,:0.0967 a:0.0693,:0.0299,:0.2578,:0.1553 and:0.2930)
		(,:0.1816 and:0.2383�:0.0664 a:0.0603 book:0.0148,:0.2354,:0.1514 and:0.2168)
		(,:0.1504 and:0.1836�:0.0698 a:0.0569 book:0.0167,:0.2090,:0.1436 and:0.1670)
		(,:0.1250 and:0.1475�:0.0718 a:0.0557 book:0.0176,:0.2002,:0.1357 and:0.1348)
		(,:0.1055 and:0.1250�:0.0811 a:0.0549 book:0.0170,:0.1816,:0.1328 and:0.1128)
		(,:0.1055 and:0.1250�:0.0811 a:0.0549 book:0.0170,:0.1816,:0.1328 and:0.1128)
 and
------
		( whereas:0.0078ll:0.0635 a:0.0586 versa:0.0046ical:0.0233ically:0.0461inate:0.0078 please:0.0032)
		( and:0.3184,:0.1069 a:0.1484 the:0.1426 of:0.1875 of:0.1836 and:0.4043 the:0.4707)
		( and:0.4141,:0.1953 a:0.1328,:0.1523,:0.1914 of:0.1709 and:0.4785 the:0.4668)
		( and:0.4414,:0.2480 a:0.1162,:0.1709,:0.2285 of:0.1543 and:0.4844 the:0.4121)
		( and:0.4375,:0.2617 a:0.1030,:0.1611,:0.2520 of:0.1533 and:0.4688 the:0.3516)
		( and:0.4219,:0.2334 a:0.0957,:0.1177,:0.2695,:0.1436 and:0.4434 the:0.2871)
		( and:0.3828,:0.1680 a:0.0801,:0.0640,:0.2676,:0.1514 and:0.3848 the:0.2041)
		( and:0.3086,:0.0967 a:0.0693,:0.0299,:0.2578,:0.1553 and:0.2930 the:0.1348)
		( and:0.2383�:0.0664 a:0.0603 book:0.0148,:0.2354,:0.1514 and:0.2168 the:0.0894)
		( and:0.1836�:0.0698 a:0.0569 book:0.0167,:0.2090,:0.1436 and:0.1670 the:0.0654)
		( and:0.1475�:0.0718 a:0.0557 book:0.0176,:0.2002,:0.1357 and:0.1348 I:0.0571)
		( and:0.1250�:0.0811 a:0.0549 book:0.0170,:0.1816,:0.1328 and:0.1128 I:0.0806)
		( and:0.1250�:0.0811 a:0.0549 book:0.0170,:0.1816,:0.1328 and:0.1128 I:0.0806)
 I’s a book, I’s
500: sample 0: Hello, I'm a language model,
------
		( your:0.0320inate:0.0183t:0.0620 not:0.0422matical:0.0064ence:0.0145ically:0.1079inate:0.0165)
		(,:0.1855 and:0.3887,:0.1348 not:0.0776,:0.1016,:0.2002.:0.0801 and:0.4863)
		(,:0.1836 and:0.4297,:0.1787 not:0.0708,:0.1035,:0.2119.:0.1045 and:0.4746)
		(,:0.1670 and:0.3906,:0.2061 not:0.0688,:0.0864,:0.2393.:0.1104 and:0.4219)
		(,:0.1504 and:0.3770,:0.2217 not:0.0723,:0.0598,:0.2490.:0.1206 and:0.3730)
		(,:0.1416 and:0.3516,:0.2236 not:0.0776,:0.0356,:0.2598.:0.1377 and:0.3340)
		(,:0.1367 and:0.3203,:0.1719 not:0.0835,:0.0166,:0.2695.:0.1562 and:0.2812)
		(,:0.1396 and:0.2637,:0.0859 not:0.0776 very:0.0153,:0.2539.:0.1738 and:0.2266)
		(,:0.1377 and:0.1748�:0.0703 not:0.0654 few:0.0206,:0.2354.:0.1924 and:0.1650)
		(,:0.1172 and:0.1069�:0.0623 not:0.0549 few:0.0256,:0.2275.:0.1934 and:0.1328)
		(,:0.0864 and:0.0654 know:0.0674 not:0.0479 few:0.0276,:0.2285.:0.1826 and:0.1064)
		(!:0.0845 and:0.0425 know:0.0718 not:0.0439 few:0.0261,:0.2148.:0.1699 I:0.1060)
		(!:0.0845 and:0.0425 know:0.0718 not:0.0439 few:0.0261,:0.2148.:0.1699 I:0.1060)
 I
------
		(inate:0.0183t:0.0620 not:0.0422matical:0.0064ence:0.0145ically:0.1079inate:0.0165t:0.0598)
		( and:0.3887,:0.1348 not:0.0776,:0.1016,:0.2002.:0.0801 and:0.4863,:0.1514)
		( and:0.4297,:0.1787 not:0.0708,:0.1035,:0.2119.:0.1045 and:0.4746,:0.1523)
		( and:0.3906,:0.2061 not:0.0688,:0.0864,:0.2393.:0.1104 and:0.4219,:0.1602)
		( and:0.3770,:0.2217 not:0.0723,:0.0598,:0.2490.:0.1206 and:0.3730,:0.1553)
		( and:0.3516,:0.2236 not:0.0776,:0.0356,:0.2598.:0.1377 and:0.3340,:0.1514)
		( and:0.3203,:0.1719 not:0.0835,:0.0166,:0.2695.:0.1562 and:0.2812,:0.1211)
		( and:0.2637,:0.0859 not:0.0776 very:0.0153,:0.2539.:0.1738 and:0.2266�:0.0693)
		( and:0.1748�:0.0703 not:0.0654 few:0.0206,:0.2354.:0.1924 and:0.1650�:0.0649)
		( and:0.1069�:0.0623 not:0.0549 few:0.0256,:0.2275.:0.1934 and:0.1328�:0.0576)
		( and:0.0654 know:0.0674 not:0.0479 few:0.0276,:0.2285.:0.1826 and:0.1064 am:0.0618)
		( and:0.0425 know:0.0718 not:0.0439 few:0.0261,:0.2148.:0.1699 I:0.1060 have:0.0615)
		( and:0.0425 know:0.0718 not:0.0439 few:0.0261,:0.2148.:0.1699 I:0.1060 have:0.0615)
 have a few years. I have a few years.
650: sample 0: Hello, I'm a language model,
------
		(.:0.0496inally:0.0562's:0.0371 not:0.0320 ":0.0068 and:0.0256ally:0.1138inally:0.0400)
		(,:0.1572 and:0.2412,:0.0923 not:0.0272,:0.0703 and:0.2041 and:0.0850 and:0.3535)
		(,:0.1152 and:0.2139,:0.0977 not:0.0277,:0.0557 and:0.1670.:0.0903 and:0.2988)
		(,:0.0884 and:0.1787,:0.0933 not:0.0266,:0.0405 and:0.1260.:0.0874 and:0.2656)
		(,:0.0811 and:0.1807,:0.1069 not:0.0292,:0.0284 and:0.1040.:0.0913 and:0.2598)
		(,:0.0874 and:0.1973,:0.1221 not:0.0337,:0.0184,:0.0947.:0.0981 and:0.2715)
		(,:0.1079 and:0.2256,:0.1235 not:0.0383 more:0.0157 of:0.1123.:0.1035 and:0.2832)
		(,:0.1406 and:0.2305,:0.0825 not:0.0376 great:0.0210 of:0.1523 for:0.1064 and:0.2598)
		(,:0.1543 and:0.1787 am:0.0496 like:0.0359 great:0.0253 of:0.1602 for:0.1187 and:0.2188)
		(,:0.1455 and:0.1167 am:0.0620 like:0.0310 great:0.0260 of:0.1465 for:0.1177 and:0.1631)
		(!:0.1187 I:0.0767 am:0.0674 going:0.0286 great:0.0253 of:0.1245,:0.1123 I:0.1289)
		(!:0.1089 I:0.1006 am:0.0659 going:0.0312 bit:0.0249 of:0.0957,:0.1206 I:0.1836)
		(!:0.1089 I:0.1006 am:0.0659 going:0.0312 bit:0.0249 of:0.0957,:0.1206 I:0.1836)
 I
------
		(inally:0.0562's:0.0371 not:0.0320 ":0.0068 and:0.0256ally:0.1138inally:0.0400's:0.0287)
		( and:0.2412,:0.0923 not:0.0272,:0.0703 and:0.2041 and:0.0850 and:0.3535,:0.1167)
		( and:0.2139,:0.0977 not:0.0277,:0.0557 and:0.1670.:0.0903 and:0.2988.:0.0898)
		( and:0.1787,:0.0933 not:0.0266,:0.0405 and:0.1260.:0.0874 and:0.2656,:0.0806)
		( and:0.1807,:0.1069 not:0.0292,:0.0284 and:0.1040.:0.0913 and:0.2598,:0.0854)
		( and:0.1973,:0.1221 not:0.0337,:0.0184,:0.0947.:0.0981 and:0.2715,:0.1001)
		( and:0.2256,:0.1235 not:0.0383 more:0.0157 of:0.1123.:0.1035 and:0.2832,:0.1074)
		( and:0.2305,:0.0825 not:0.0376 great:0.0210 of:0.1523 for:0.1064 and:0.2598,:0.0825)
		( and:0.1787 am:0.0496 like:0.0359 great:0.0253 of:0.1602 for:0.1187 and:0.2188 was:0.0544)
		( and:0.1167 am:0.0620 like:0.0310 great:0.0260 of:0.1465 for:0.1177 and:0.1631 am:0.0649)
		( I:0.0767 am:0.0674 going:0.0286 great:0.0253 of:0.1245,:0.1123 I:0.1289 am:0.0728)
		( I:0.1006 am:0.0659 going:0.0312 bit:0.0249 of:0.0957,:0.1206 I:0.1836 am:0.0781)
		( I:0.1006 am:0.0659 going:0.0312 bit:0.0249 of:0.0957,:0.1206 I:0.1836 am:0.0781)
 am the first, I am not the first, I
800: sample 0: Hello, I'm a language model,
------
		(,:0.0237inally:0.0669's:0.0342 not:0.0325 ":0.0078 and:0.0293ally:0.0698inally:0.0544)
		(,:0.0996 and:0.1602,:0.0630,:0.0474,:0.0527 and:0.1543 and:0.0835 and:0.2832)
		(,:0.0444 and:0.0874,:0.0532,:0.0410,:0.0359 and:0.0991 and:0.0552 and:0.1709)
		(,:0.0259 and:0.0593,:0.0496,:0.0337,:0.0231 and:0.0708 and:0.0408 and:0.1387)
		(,:0.0206 and:0.0579,:0.0574,:0.0315 ":0.0176,:0.0635 for:0.0398 and:0.1387)
		(,:0.0216 and:0.0698,:0.0811 not:0.0420 ":0.0151,:0.0703 of:0.0610 and:0.1621)
		(,:0.0293 and:0.0967,:0.1089 not:0.0659 very:0.0166 of:0.1123 of:0.0996 and:0.2002)
		(,:0.0444 and:0.1187,:0.1143 not:0.0894 very:0.0244 of:0.1523 of:0.1318 and:0.2061)
		(,:0.0640 and:0.0977,:0.0850 not:0.0923 great:0.0320 of:0.1787 of:0.1445 and:0.1602)
		(::0.0757 I:0.1904 was:0.0684 not:0.0728 great:0.0364 of:0.1650 for:0.1484 I:0.1187)
		(::0.0850 I:0.2852 am:0.0659 not:0.0562 great:0.0361 of:0.1445.:0.1377 I:0.2119)
		(::0.0825 I:0.3398 am:0.0742 going:0.0603 great:0.0334 of:0.1182.:0.1396 I:0.3047)
		(::0.0825 I:0.3398 am:0.0742 going:0.0603 great:0.0334 of:0.1182.:0.1396 I:0.3047)
 I
------
		(inally:0.0669's:0.0342 not:0.0325 ":0.0078 and:0.0293ally:0.0698inally:0.0544's:0.0347)
		( and:0.1602,:0.0630,:0.0474,:0.0527 and:0.1543 and:0.0835 and:0.2832,:0.0933)
		( and:0.0874,:0.0532,:0.0410,:0.0359 and:0.0991 and:0.0552 and:0.1709,:0.0630)
		( and:0.0593,:0.0496,:0.0337,:0.0231 and:0.0708 and:0.0408 and:0.1387,:0.0532)
		( and:0.0579,:0.0574,:0.0315 ":0.0176,:0.0635 for:0.0398 and:0.1387,:0.0564)
		( and:0.0698,:0.0811 not:0.0420 ":0.0151,:0.0703 of:0.0610 and:0.1621,:0.0703)
		( and:0.0967,:0.1089 not:0.0659 very:0.0166 of:0.1123 of:0.0996 and:0.2002,:0.0840)
		( and:0.1187,:0.1143 not:0.0894 very:0.0244 of:0.1523 of:0.1318 and:0.2061 will:0.0723)
		( and:0.0977,:0.0850 not:0.0923 great:0.0320 of:0.1787 of:0.1445 and:0.1602 have:0.0776)
		( I:0.1904 was:0.0684 not:0.0728 great:0.0364 of:0.1650 for:0.1484 I:0.1187 have:0.0928)
		( I:0.2852 am:0.0659 not:0.0562 great:0.0361 of:0.1445.:0.1377 I:0.2119 have:0.0972)
		( I:0.3398 am:0.0742 going:0.0603 great:0.0334 of:0.1182.:0.1396 I:0.3047 have:0.1011)
		( I:0.3398 am:0.0742 going:0.0603 great:0.0334 of:0.1182.:0.1396 I:0.3047 have:0.1011)
 have a language of the language. I have a language
950: sample 0: Hello, I'm a language model,
------
		(,:0.0208inally:0.0806ve:0.0297 not:0.0232 ":0.0040 and:0.0942ally:0.0525inally:0.0620)
		(,:0.1226 and:0.1797,:0.0815,:0.0549,:0.0781 and:0.1826 and:0.0981 and:0.2598)
		(,:0.0520 and:0.0728,:0.0586,:0.0400,:0.0442,:0.1191 and:0.0588 and:0.1328)
		(,:0.0273 and:0.0420,:0.0469,:0.0280,:0.0256,:0.0854 and:0.0391 and:0.0913)
		(,:0.0189 and:0.0366,:0.0479,:0.0229,:0.0186-:0.0776-:0.0356 and:0.0854)
		(,:0.0187 and:0.0427�:0.0630 not:0.0247,:0.0157-:0.0728 of:0.0620 and:0.1001)
		(,:0.0258 and:0.0579�:0.1123 not:0.0466,:0.0133,:0.0806 of:0.1104 and:0.1245)
		(,:0.0413 and:0.0767�:0.1445 not:0.0845 very:0.0201,:0.0986 of:0.1514 and:0.1484)
		(,:0.0674 and:0.0845�:0.1318 not:0.1226 great:0.0302,:0.1099 of:0.1621 and:0.1377)
		(,:0.0923 and:0.0693�:0.1074 not:0.1279 great:0.0391,:0.1157 for:0.1650 but:0.1128)
		(,:0.1074 I:0.0825�:0.0869 not:0.1177 great:0.0437,:0.1157.:0.1514 I:0.1138)
		(,:0.1069 I:0.1084�:0.0728 not:0.0991 great:0.0437,:0.1079.:0.1738 I:0.1514)
		(,:0.1069 I:0.1084�:0.0728 not:0.0991 great:0.0437,:0.1079.:0.1738 I:0.1514)
 I
------
		(inally:0.0806ve:0.0297 not:0.0232 ":0.0040 and:0.0942ally:0.0525inally:0.0620ve:0.0245)
		( and:0.1797,:0.0815,:0.0549,:0.0781 and:0.1826 and:0.0981 and:0.2598,:0.1196)
		( and:0.0728,:0.0586,:0.0400,:0.0442,:0.1191 and:0.0588 and:0.1328,:0.0752)
		( and:0.0420,:0.0469,:0.0280,:0.0256,:0.0854 and:0.0391 and:0.0913-:0.0569)
		( and:0.0366,:0.0479,:0.0229,:0.0186-:0.0776-:0.0356 and:0.0854-:0.0588)
		( and:0.0427�:0.0630 not:0.0247,:0.0157-:0.0728 of:0.0620 and:0.1001,:0.0552)
		( and:0.0579�:0.1123 not:0.0466,:0.0133,:0.0806 of:0.1104 and:0.1245,:0.0654)
		( and:0.0767�:0.1445 not:0.0845 very:0.0201,:0.0986 of:0.1514 and:0.1484 will:0.0732)
		( and:0.0845�:0.1318 not:0.1226 great:0.0302,:0.1099 of:0.1621 and:0.1377 will:0.0806)
		( and:0.0693�:0.1074 not:0.1279 great:0.0391,:0.1157 for:0.1650 but:0.1128 will:0.0620)
		( I:0.0825�:0.0869 not:0.1177 great:0.0437,:0.1157.:0.1514 I:0.1138 have:0.0588)
		( I:0.1084�:0.0728 not:0.0991 great:0.0437,:0.1079.:0.1738 I:0.1514 have:0.0598)
		( I:0.1084�:0.0728 not:0.0991 great:0.0437,:0.1079.:0.1738 I:0.1514 have:0.0598)
 have a language, and I have a language, and
1100: sample 0: Hello, I'm a language model,
------
		(re:0.0119inally:0.0238ve:0.0119 not:0.0182rete:0.0031 and:0.0664ically:0.0244inally:0.0177)
		(,:0.0806 and:0.0986,:0.0728,:0.0518,:0.0654,:0.1465 and:0.0684 and:0.1631)
		(,:0.0374 and:0.0374,:0.0481,:0.0347,:0.0325,:0.0942 and:0.0396 and:0.0767)
		(,:0.0193 and:0.0228,:0.0366,:0.0228,:0.0189,:0.0640-:0.0309 and:0.0579)
		(,:0.0126 and:0.0198�:0.0459,:0.0176,:0.0146 and:0.0508-:0.0304 and:0.0564)
		(,:0.0114�:0.0330�:0.1045,:0.0165,:0.0132 and:0.0513 for:0.0374 and:0.0688)
		(,:0.0138�:0.0610�:0.2236 not:0.0303,:0.0128 and:0.0581 for:0.0674 and:0.1001)
		(,:0.0209�:0.0908�:0.3418 not:0.0598 very:0.0140 that:0.0767 for:0.1133 and:0.1455)
		(,:0.0344�:0.0889�:0.3633 not:0.0972 very:0.0215 that:0.0942 for:0.1553 and:0.1641)
		(,:0.0479 we:0.0640�:0.3086 not:0.1123 very:0.0255.:0.1001.:0.1826 and:0.1377)
		(,:0.0525 I:0.0874�:0.2617 not:0.1001 very:0.0249,:0.1040.:0.2070 and:0.1060)
		(!:0.0588 I:0.1240�:0.2109 not:0.0835 little:0.0226,:0.1055.:0.2178 I:0.1348)
		(!:0.0588 I:0.1240�:0.2109 not:0.0835 little:0.0226,:0.1055.:0.2178 I:0.1348)
 I
------
		(inally:0.0238ve:0.0119 not:0.0182rete:0.0031 and:0.0664ically:0.0244inally:0.0177ve:0.0112)
		( and:0.0986,:0.0728,:0.0518,:0.0654,:0.1465 and:0.0684 and:0.1631,:0.1040)
		( and:0.0374,:0.0481,:0.0347,:0.0325,:0.0942 and:0.0396 and:0.0767,:0.0583)
		( and:0.0228,:0.0366,:0.0228,:0.0189,:0.0640-:0.0309 and:0.0579,:0.0388)
		( and:0.0198�:0.0459,:0.0176,:0.0146 and:0.0508-:0.0304 and:0.0564,:0.0302)
		(�:0.0330�:0.1045,:0.0165,:0.0132 and:0.0513 for:0.0374 and:0.0688�:0.0464)
		(�:0.0610�:0.2236 not:0.0303,:0.0128 and:0.0581 for:0.0674 and:0.1001�:0.0830)
		(�:0.0908�:0.3418 not:0.0598 very:0.0140 that:0.0767 for:0.1133 and:0.1455�:0.1172)
		(�:0.0889�:0.3633 not:0.0972 very:0.0215 that:0.0942 for:0.1553 and:0.1641�:0.1001)
		( we:0.0640�:0.3086 not:0.1123 very:0.0255.:0.1001.:0.1826 and:0.1377 will:0.0791)
		( I:0.0874�:0.2617 not:0.1001 very:0.0249,:0.1040.:0.2070 and:0.1060 am:0.0688)
		( I:0.1240�:0.2109 not:0.0835 little:0.0226,:0.1055.:0.2178 I:0.1348 am:0.0767)
		( I:0.1240�:0.2109 not:0.0835 little:0.0226,:0.1055.:0.2178 I:0.1348 am:0.0767)
 am not a language, but I am not a language
1250: sample 0: Hello, I'm a language model,
------
		(
:0.0187inally:0.0217rete:0.0122 not:0.0155rete:0.0044 and:0.1318 and:0.0299inally:0.0171)
		(,:0.1338 and:0.0752,:0.0552,:0.0518,:0.0552,:0.1523,:0.0654 and:0.1309)
		(,:0.0605 and:0.0266,:0.0369,:0.0305,:0.0281,:0.0913-:0.0422 and:0.0645)
		(,:0.0306 and:0.0165-:0.0281,:0.0177,:0.0162,:0.0566-:0.0332 and:0.0496)
		(,:0.0192 and:0.0140-:0.0287,:0.0118,:0.0110 and:0.0483 that:0.0287 and:0.0488)
		(,:0.0161 and:0.0156�:0.0515 not:0.0168,:0.0087 and:0.0469 that:0.0452 and:0.0640)
		(,:0.0187�:0.0265�:0.1064 not:0.0342 great:0.0139 that:0.0713 that:0.0791 and:0.1128)
		(,:0.0283�:0.0430�:0.1680 not:0.0674 great:0.0260 that:0.1172 that:0.1240 and:0.1768)
		(,:0.0500�:0.0510�:0.1992 not:0.1040 great:0.0393 that:0.1553 that:0.1621 and:0.1963)
		(,:0.0898 the:0.0525�:0.1914 not:0.1172 great:0.0469 that:0.1865 that:0.1885 and:0.1621)
		(,:0.1377 I:0.0767�:0.1689 not:0.1030 great:0.0449 that:0.1807 that:0.2070 I:0.1973)
		(,:0.1758 I:0.1118�:0.1533 not:0.0894 great:0.0374 that:0.1523 that:0.2129 I:0.2910)
		(,:0.1758 I:0.1118�:0.1533 not:0.0894 great:0.0374 that:0.1523 that:0.2129 I:0.2910)
 I
------
		(inally:0.0217rete:0.0122 not:0.0155rete:0.0044 and:0.1318 and:0.0299inally:0.0171rete:0.0079)
		( and:0.0752,:0.0552,:0.0518,:0.0552,:0.1523,:0.0654 and:0.1309,:0.0649)
		( and:0.0266,:0.0369,:0.0305,:0.0281,:0.0913-:0.0422 and:0.0645-:0.0383)
		( and:0.0165-:0.0281,:0.0177,:0.0162,:0.0566-:0.0332 and:0.0496-:0.0310)
		( and:0.0140-:0.0287,:0.0118,:0.0110 and:0.0483 that:0.0287 and:0.0488-:0.0276)
		( and:0.0156�:0.0515 not:0.0168,:0.0087 and:0.0469 that:0.0452 and:0.0640�:0.0415)
		(�:0.0265�:0.1064 not:0.0342 great:0.0139 that:0.0713 that:0.0791 and:0.1128�:0.0928)
		(�:0.0430�:0.1680 not:0.0674 great:0.0260 that:0.1172 that:0.1240 and:0.1768�:0.1177)
		(�:0.0510�:0.1992 not:0.1040 great:0.0393 that:0.1553 that:0.1621 and:0.1963�:0.0850)
		( the:0.0525�:0.1914 not:0.1172 great:0.0469 that:0.1865 that:0.1885 and:0.1621'm:0.1064)
		( I:0.0767�:0.1689 not:0.1030 great:0.0449 that:0.1807 that:0.2070 I:0.1973'm:0.1465)
		( I:0.1118�:0.1533 not:0.0894 great:0.0374 that:0.1523 that:0.2129 I:0.2910'm:0.1562)
		( I:0.1118�:0.1533 not:0.0894 great:0.0374 that:0.1523 that:0.2129 I:0.2910'm:0.1562)
'm not a language, but I'm not a language
1400: sample 0: Hello, I'm a language model,
------
		(
:0.0190inally:0.0150rete:0.0105re:0.0204rete:0.0022 and:0.1250 and:0.0391inally:0.0131)
		(,:0.1416 and:0.0554,:0.0476,:0.0464,:0.0366 and:0.1172-:0.0796 and:0.1021)
		(,:0.0708 and:0.0217,:0.0310,:0.0248,:0.0182-:0.0557-:0.0447 and:0.0527)
		(,:0.0417 and:0.0173,:0.0248,:0.0159,:0.0123-:0.0413-:0.0356 and:0.0559)
		(,:0.0294 and:0.0187�:0.0344,:0.0123,:0.0098 and:0.0374 that:0.0356 and:0.0664)
		(,:0.0264�:0.0364�:0.0801 not:0.0151,:0.0085 and:0.0396 that:0.0474 and:0.0923)
		(,:0.0288�:0.0645�:0.1465 not:0.0322 great:0.0135 that:0.0554 that:0.0649 and:0.1533)
		(,:0.0383�:0.0913�:0.1943 not:0.0688 great:0.0251 that:0.0903 that:0.0928 and:0.1924)
		(,:0.0564�:0.0986�:0.2041 not:0.1172 very:0.0398 that:0.1348.:0.1279 and:0.1963)
		(,:0.0815�:0.0840�:0.1719 not:0.1543 very:0.0525 that:0.1680.:0.1699 and:0.1807)
		(,:0.1138�:0.0564�:0.1436 not:0.1562 very:0.0525 that:0.1768.:0.1973 and:0.1572)
		(,:0.1299 the:0.0679�:0.1187 not:0.1289 very:0.0444 that:0.1533.:0.2100 I:0.1602)
		(,:0.1299 the:0.0679�:0.1187 not:0.1289 very:0.0444 that:0.1533.:0.2100 I:0.1602)
 I
------
		(inally:0.0150rete:0.0105re:0.0204rete:0.0022 and:0.1250 and:0.0391inally:0.0131rete:0.0080)
		( and:0.0554,:0.0476,:0.0464,:0.0366 and:0.1172-:0.0796 and:0.1021,:0.0520)
		( and:0.0217,:0.0310,:0.0248,:0.0182-:0.0557-:0.0447 and:0.0527,:0.0288)
		( and:0.0173,:0.0248,:0.0159,:0.0123-:0.0413-:0.0356 and:0.0559,:0.0198)
		( and:0.0187�:0.0344,:0.0123,:0.0098 and:0.0374 that:0.0356 and:0.0664 will:0.0200)
		(�:0.0364�:0.0801 not:0.0151,:0.0085 and:0.0396 that:0.0474 and:0.0923 will:0.0483)
		(�:0.0645�:0.1465 not:0.0322 great:0.0135 that:0.0554 that:0.0649 and:0.1533 will:0.0825)
		(�:0.0913�:0.1943 not:0.0688 great:0.0251 that:0.0903 that:0.0928 and:0.1924�:0.0986)
		(�:0.0986�:0.2041 not:0.1172 very:0.0398 that:0.1348.:0.1279 and:0.1963�:0.0732)
		(�:0.0840�:0.1719 not:0.1543 very:0.0525 that:0.1680.:0.1699 and:0.1807'm:0.1084)
		(�:0.0564�:0.1436 not:0.1562 very:0.0525 that:0.1768.:0.1973 and:0.1572'm:0.1504)
		( the:0.0679�:0.1187 not:0.1289 very:0.0444 that:0.1533.:0.2100 I:0.1602'm:0.1680)
		( the:0.0679�:0.1187 not:0.1289 very:0.0444 that:0.1533.:0.2100 I:0.1602'm:0.1680)
'm a language that I'm a language that I'm
1550: sample 0: Hello, I'm a language model,
------
		(,:0.0242inally:0.0032rete:0.0049 not:0.0132rete:0.0014 and:0.1396 and:0.0383inally:0.0033)
		(,:0.1836 and:0.0583,:0.0625,:0.0491,:0.0464,:0.1387-:0.0693 and:0.0918)
		(,:0.0977 and:0.0231,:0.0361,:0.0266,:0.0192,:0.0598-:0.0442 and:0.0483)
		(,:0.0574 and:0.0183-:0.0299,:0.0172,:0.0119-:0.0427 that:0.0396 and:0.0449)
		(,:0.0378�:0.0322�:0.0486,:0.0146,:0.0090-:0.0391 that:0.0518 and:0.0486)
		(,:0.0289�:0.0557�:0.1064 not:0.0227,:0.0082-:0.0408 of:0.0674 and:0.0562)
		(,:0.0269�:0.0850�:0.1777 not:0.0408 great:0.0135 that:0.0562 of:0.1040 and:0.0811)
		(,:0.0275�:0.1030�:0.1992 not:0.0674 great:0.0204 that:0.0835 of:0.1416 and:0.1074)
		(,:0.0315�:0.1006�:0.1592 not:0.0825 great:0.0272 that:0.1187 of:0.1504 and:0.1226)
		(,:0.0393�:0.0830�:0.1118 not:0.0781 very:0.0352 that:0.1416 that:0.1484 but:0.1484)
		(,:0.0496�:0.0513 was:0.0928 going:0.0908 very:0.0359 that:0.1465.:0.1611 I:0.1631)
		(,:0.0518 the:0.0520 was:0.0957 going:0.1074 very:0.0325 of:0.1318.:0.1777 I:0.2344)
		(,:0.0518 the:0.0520 was:0.0957 going:0.1074 very:0.0325 of:0.1318.:0.1777 I:0.2344)
 I
------
		(inally:0.0032rete:0.0049 not:0.0132rete:0.0014 and:0.1396 and:0.0383inally:0.0033rete:0.0041)
		( and:0.0583,:0.0625,:0.0491,:0.0464,:0.1387-:0.0693 and:0.0918,:0.0679)
		( and:0.0231,:0.0361,:0.0266,:0.0192,:0.0598-:0.0442 and:0.0483,:0.0304)
		( and:0.0183-:0.0299,:0.0172,:0.0119-:0.0427 that:0.0396 and:0.0449-:0.0231)
		(�:0.0322�:0.0486,:0.0146,:0.0090-:0.0391 that:0.0518 and:0.0486�:0.0255)
		(�:0.0557�:0.1064 not:0.0227,:0.0082-:0.0408 of:0.0674 and:0.0562�:0.0481)
		(�:0.0850�:0.1777 not:0.0408 great:0.0135 that:0.0562 of:0.1040 and:0.0811�:0.0791)
		(�:0.1030�:0.1992 not:0.0674 great:0.0204 that:0.0835 of:0.1416 and:0.1074�:0.0864)
		(�:0.1006�:0.1592 not:0.0825 great:0.0272 that:0.1187 of:0.1504 and:0.1226'm:0.0659)
		(�:0.0830�:0.1118 not:0.0781 very:0.0352 that:0.1416 that:0.1484 but:0.1484'm:0.1157)
		(�:0.0513 was:0.0928 going:0.0908 very:0.0359 that:0.1465.:0.1611 I:0.1631'm:0.1445)
		( the:0.0520 was:0.0957 going:0.1074 very:0.0325 of:0.1318.:0.1777 I:0.2344'm:0.1445)
		( the:0.0520 was:0.0957 going:0.1074 very:0.0325 of:0.1318.:0.1777 I:0.2344'm:0.1445)
'm a language model, I'm a language model,
1700: sample 0: Hello, I'm a language model,
------
		(
:0.0320inally:0.0077rete:0.0036 not:0.0137inally:0.0010 and:0.1777 and:0.0425inally:0.0077)
		(,:0.2041,:0.0374,:0.0598,:0.0466,:0.0356-:0.1406-:0.1196 and:0.0498)
		(,:0.1157 and:0.0172,:0.0327,:0.0245,:0.0177-:0.0981-:0.0781 and:0.0354)
		(,:0.0728 and:0.0184-:0.0327,:0.0159,:0.0127-:0.0752-:0.0608 and:0.0437)
		(,:0.0527�:0.0245-:0.0315 not:0.0132,:0.0093-:0.0603 of:0.0466 and:0.0471)
		(,:0.0422�:0.0311�:0.0464 not:0.0248.:0.0083-:0.0476 of:0.0596 and:0.0537)
		(,:0.0359�:0.0304�:0.0554 not:0.0554 bit:0.0165-:0.0391 of:0.0879 and:0.0752)
		(,:0.0354�:0.0256 will:0.0645 not:0.1230 bit:0.0376 that:0.0505 of:0.1216 and:0.0962)
		(,:0.0378�:0.0204 will:0.0859 not:0.2158 bit:0.0693 that:0.0825 of:0.1299 and:0.1299)
		(,:0.0427 you:0.0339 was:0.0835 not:0.2598 bit:0.0928 that:0.1152 that:0.1416 and:0.1611)
		(,:0.0469 you:0.0520 was:0.0879 not:0.2490 bit:0.0967 that:0.1260.:0.1406 and:0.1670)
		(.:0.0452 the:0.0547 am:0.0884 not:0.1992 bit:0.0850 that:0.1123.:0.1650 and:0.1543)
		(.:0.0452 the:0.0547 am:0.0884 not:0.1992 bit:0.0850 that:0.1123.:0.1650 and:0.1543)
 and
------
		(inally:0.0077rete:0.0036 not:0.0137inally:0.0010 and:0.1777 and:0.0425inally:0.0077ruff:0.0010)
		(,:0.0374,:0.0598,:0.0466,:0.0356-:0.1406-:0.1196 and:0.0498,:0.0325)
		( and:0.0172,:0.0327,:0.0245,:0.0177-:0.0981-:0.0781 and:0.0354,:0.0166)
		( and:0.0184-:0.0327,:0.0159,:0.0127-:0.0752-:0.0608 and:0.0437 the:0.0222)
		(�:0.0245-:0.0315 not:0.0132,:0.0093-:0.0603 of:0.0466 and:0.0471 the:0.0305)
		(�:0.0311�:0.0464 not:0.0248.:0.0083-:0.0476 of:0.0596 and:0.0537 the:0.0471)
		(�:0.0304�:0.0554 not:0.0554 bit:0.0165-:0.0391 of:0.0879 and:0.0752 the:0.0601)
		(�:0.0256 will:0.0645 not:0.1230 bit:0.0376 that:0.0505 of:0.1216 and:0.0962 the:0.0613)
		(�:0.0204 will:0.0859 not:0.2158 bit:0.0693 that:0.0825 of:0.1299 and:0.1299 I:0.1377)
		( you:0.0339 was:0.0835 not:0.2598 bit:0.0928 that:0.1152 that:0.1416 and:0.1611 I:0.2539)
		( you:0.0520 was:0.0879 not:0.2490 bit:0.0967 that:0.1260.:0.1406 and:0.1670 I:0.3105)
		( the:0.0547 am:0.0884 not:0.1992 bit:0.0850 that:0.1123.:0.1650 and:0.1543 I:0.2949)
		( the:0.0547 am:0.0884 not:0.1992 bit:0.0850 that:0.1123.:0.1650 and:0.1543 I:0.2949)
 I'm not a language model.
The first language
1850: sample 0: Hello, I'm a language model,
------
		(
:0.0277inally:0.0056rete:0.0026re:0.0186inally:0.0010 and:0.1719 and:0.0422inally:0.0051)
		(,:0.1953 and:0.0378,:0.0452,:0.0376,:0.0266 and:0.1167-:0.0640 and:0.0574)
		(,:0.1206 and:0.0322,:0.0337,:0.0221,:0.0157 and:0.0620-:0.0498 and:0.0737)
		(,:0.0791 and:0.0332,:0.0260,:0.0138,:0.0099-:0.0425-:0.0371 and:0.0747)
		(,:0.0571 and:0.0298,:0.0198,:0.0093.:0.0072-:0.0342-:0.0221 and:0.0635)
		(,:0.0430 and:0.0229�:0.0261 not:0.0109 great:0.0108 and:0.0293 that:0.0194 and:0.0623)
		(,:0.0364 and:0.0160 will:0.0386 not:0.0219 great:0.0198 and:0.0293 for:0.0337 and:0.0786)
		(,:0.0322 and:0.0127 will:0.0708 not:0.0447 great:0.0332 that:0.0400 for:0.0654 and:0.1240)
		(,:0.0327 you:0.0173 will:0.0972 not:0.0728 great:0.0496 that:0.0635 for:0.0874 and:0.1816)
		(,:0.0386 you:0.0376 will:0.0957 not:0.0850 great:0.0547 that:0.0869.:0.1089 and:0.2139)
		(,:0.0491 you:0.0613�:0.0884 going:0.0811 great:0.0474 that:0.0972.:0.1357 and:0.2266)
		( to:0.0645 you:0.0728 am:0.0981 going:0.0981 good:0.0410 that:0.0889.:0.1523 and:0.2207)
		( to:0.0645 you:0.0728 am:0.0981 going:0.0981 good:0.0410 that:0.0889.:0.1523 and:0.2207)
 and
------
		(inally:0.0056rete:0.0026re:0.0186inally:0.0010 and:0.1719 and:0.0422inally:0.0051ruff:0.0013)
		( and:0.0378,:0.0452,:0.0376,:0.0266 and:0.1167-:0.0640 and:0.0574,:0.0317)
		( and:0.0322,:0.0337,:0.0221,:0.0157 and:0.0620-:0.0498 and:0.0737,:0.0236)
		( and:0.0332,:0.0260,:0.0138,:0.0099-:0.0425-:0.0371 and:0.0747 the:0.0139)
		( and:0.0298,:0.0198,:0.0093.:0.0072-:0.0342-:0.0221 and:0.0635 the:0.0165)
		( and:0.0229�:0.0261 not:0.0109 great:0.0108 and:0.0293 that:0.0194 and:0.0623 the:0.0258)
		( and:0.0160 will:0.0386 not:0.0219 great:0.0198 and:0.0293 for:0.0337 and:0.0786 I:0.0669)
		( and:0.0127 will:0.0708 not:0.0447 great:0.0332 that:0.0400 for:0.0654 and:0.1240 I:0.2275)
		( you:0.0173 will:0.0972 not:0.0728 great:0.0496 that:0.0635 for:0.0874 and:0.1816 I:0.4902)
		( you:0.0376 will:0.0957 not:0.0850 great:0.0547 that:0.0869.:0.1089 and:0.2139 I:0.6445)
		( you:0.0613�:0.0884 going:0.0811 great:0.0474 that:0.0972.:0.1357 and:0.2266 I:0.6445)
		( you:0.0728 am:0.0981 going:0.0981 good:0.0410 that:0.0889.:0.1523 and:0.2207 I:0.5352)
		( you:0.0728 am:0.0981 going:0.0981 good:0.0410 that:0.0889.:0.1523 and:0.2207 I:0.5352)
 I'm a language model.
I'm a language
2000: sample 0: Hello, I'm a language model,
------
		(
:0.0549inally:0.0040inally:0.0023
:0.0188greSQL:0.0008 and:0.1924 and:0.0386inally:0.0039)
		(,:0.2637 and:0.0369,:0.0386,:0.0294,:0.0238 and:0.1162,:0.0532 and:0.0498)
		(,:0.1670 and:0.0312,:0.0277,:0.0151,:0.0117-:0.0549-:0.0344 and:0.0674)
		(,:0.1172 and:0.0347,:0.0221,:0.0090,:0.0059-:0.0381-:0.0195 and:0.0708)
		(,:0.0859 and:0.0276�:0.0199 not:0.0119 nice:0.0065-:0.0281 of:0.0165 and:0.0615)
		(,:0.0649 and:0.0175 will:0.0267 not:0.0231 nice:0.0099-:0.0231 for:0.0251 and:0.0618)
		(,:0.0510 and:0.0109 will:0.0447 not:0.0488 great:0.0144 that:0.0284 for:0.0581 and:0.0757)
		(,:0.0410 your:0.0133 will:0.0684 not:0.0957 great:0.0236 that:0.0474 for:0.1006 and:0.1177)
		(,:0.0400 you:0.0254�:0.0918 not:0.1377 very:0.0325 that:0.0791 for:0.1279 and:0.1777)
		(,:0.0488 you:0.0620�:0.1108 not:0.1406 very:0.0371 that:0.1060 for:0.1338 and:0.2217)
		(,:0.0693 you:0.1094�:0.1138 not:0.1143 little:0.0342 that:0.1123.:0.1377 and:0.2354)
		(,:0.0801 you:0.1318�:0.1123 going:0.1016 little:0.0277 that:0.0933.:0.1455 I:0.2695)
		(,:0.0801 you:0.1318�:0.1123 going:0.1016 little:0.0277 that:0.0933.:0.1455 I:0.2695)
 I
------
		(inally:0.0040inally:0.0023
:0.0188greSQL:0.0008 and:0.1924 and:0.0386inally:0.0039inally:0.0034)
		( and:0.0369,:0.0386,:0.0294,:0.0238 and:0.1162,:0.0532 and:0.0498,:0.0371)
		( and:0.0312,:0.0277,:0.0151,:0.0117-:0.0549-:0.0344 and:0.0674,:0.0242)
		( and:0.0347,:0.0221,:0.0090,:0.0059-:0.0381-:0.0195 and:0.0708 is:0.0145)
		( and:0.0276�:0.0199 not:0.0119 nice:0.0065-:0.0281 of:0.0165 and:0.0615 will:0.0168)
		( and:0.0175 will:0.0267 not:0.0231 nice:0.0099-:0.0231 for:0.0251 and:0.0618'm:0.0488)
		( and:0.0109 will:0.0447 not:0.0488 great:0.0144 that:0.0284 for:0.0581 and:0.0757'm:0.1680)
		( your:0.0133 will:0.0684 not:0.0957 great:0.0236 that:0.0474 for:0.1006 and:0.1177'm:0.3223)
		( you:0.0254�:0.0918 not:0.1377 very:0.0325 that:0.0791 for:0.1279 and:0.1777'm:0.4531)
		( you:0.0620�:0.1108 not:0.1406 very:0.0371 that:0.1060 for:0.1338 and:0.2217'm:0.5039)
		( you:0.1094�:0.1138 not:0.1143 little:0.0342 that:0.1123.:0.1377 and:0.2354'm:0.5391)
		( you:0.1318�:0.1123 going:0.1016 little:0.0277 that:0.0933.:0.1455 I:0.2695'm:0.5117)
		( you:0.1318�:0.1123 going:0.1016 little:0.0277 that:0.0933.:0.1455 I:0.2695'm:0.5117)
'm a language model, I'm a language model,
2150: sample 0: Hello, I'm a language model,
------
		(
:0.0569inally:0.0037'll:0.0025
:0.0159inally:0.0009 and:0.1943 and:0.0579inally:0.0037)
		(,:0.2334 and:0.0282,:0.0289,:0.0214,:0.0120-:0.0933-:0.0669 and:0.0344)
		(,:0.1416 and:0.0248,:0.0198,:0.0113,:0.0071-:0.0549-:0.0442 and:0.0466)
		(,:0.0942 and:0.0275,:0.0170,:0.0081,:0.0050-:0.0405-:0.0315 and:0.0562)
		(,:0.0679 and:0.0239,:0.0141 not:0.0099.:0.0036-:0.0330-:0.0193 and:0.0466)
		(,:0.0493 and:0.0161�:0.0171 not:0.0190 great:0.0050-:0.0303 of:0.0186 and:0.0437)
		(,:0.0361 the:0.0119�:0.0256 not:0.0476 great:0.0098-:0.0317 of:0.0306 and:0.0579)
		(,:0.0280�:0.0099 will:0.0425 not:0.1357 great:0.0208 that:0.0537 for:0.0684 and:0.0923)
		(,:0.0248 the:0.0107�:0.0613 not:0.2734 great:0.0369 that:0.1060 for:0.1221 and:0.1494)
		(,:0.0288 the:0.0188�:0.0830 not:0.3535 good:0.0518 that:0.1621 for:0.1533 and:0.1953)
		(,:0.0452 the:0.0376 am:0.1123 not:0.3262 good:0.0801 that:0.1816 for:0.1436 and:0.2305)
		(.:0.1060 the:0.0552 am:0.1289 not:0.2402 good:0.0913 that:0.1416.:0.1230 and:0.2246)
		(.:0.1060 the:0.0552 am:0.1289 not:0.2402 good:0.0913 that:0.1416.:0.1230 and:0.2246)
 and
------
		(inally:0.0037'll:0.0025
:0.0159inally:0.0009 and:0.1943 and:0.0579inally:0.0037ruff:0.0009)
		( and:0.0282,:0.0289,:0.0214,:0.0120-:0.0933-:0.0669 and:0.0344,:0.0173)
		( and:0.0248,:0.0198,:0.0113,:0.0071-:0.0549-:0.0442 and:0.0466,:0.0156)
		( and:0.0275,:0.0170,:0.0081,:0.0050-:0.0405-:0.0315 and:0.0562 the:0.0200)
		( and:0.0239,:0.0141 not:0.0099.:0.0036-:0.0330-:0.0193 and:0.0466 the:0.0238)
		( and:0.0161�:0.0171 not:0.0190 great:0.0050-:0.0303 of:0.0186 and:0.0437 the:0.0317)
		( the:0.0119�:0.0256 not:0.0476 great:0.0098-:0.0317 of:0.0306 and:0.0579 the:0.0356)
		(�:0.0099 will:0.0425 not:0.1357 great:0.0208 that:0.0537 for:0.0684 and:0.0923 I:0.0491)
		( the:0.0107�:0.0613 not:0.2734 great:0.0369 that:0.1060 for:0.1221 and:0.1494 I:0.1367)
		( the:0.0188�:0.0830 not:0.3535 good:0.0518 that:0.1621 for:0.1533 and:0.1953 I:0.3086)
		( the:0.0376 am:0.1123 not:0.3262 good:0.0801 that:0.1816 for:0.1436 and:0.2305 I:0.4277)
		( the:0.0552 am:0.1289 not:0.2402 good:0.0913 that:0.1416.:0.1230 and:0.2246 I:0.4434)
		( the:0.0552 am:0.1289 not:0.2402 good:0.0913 that:0.1416.:0.1230 and:0.2246 I:0.4434)
 I'm a language model.
The language model is
2300: sample 0: Hello, I'm a language model,
------
		(,:0.0674inally:0.0035'll:0.0017
:0.0128greSQL:0.0010 and:0.1377 and:0.0354inally:0.0036)
		(,:0.2578 and:0.0195,:0.0264,:0.0118,:0.0111 and:0.0781 and:0.0459 and:0.0206)
		(,:0.1582 and:0.0206,:0.0187,:0.0070,:0.0063 and:0.0312-:0.0286 and:0.0304)
		(,:0.1084 and:0.0255,:0.0168 not:0.0060.:0.0050 and:0.0200-:0.0187 and:0.0425)
		(,:0.0791 and:0.0222,:0.0129 not:0.0097.:0.0042 that:0.0167 of:0.0187 and:0.0366)
		(,:0.0583 the:0.0155 will:0.0192 not:0.0187 nice:0.0057 that:0.0189 of:0.0210 and:0.0327)
		(,:0.0430 the:0.0098 will:0.0308 not:0.0452 great:0.0099 that:0.0260 of:0.0315 and:0.0369)
		(,:0.0347 the:0.0071 will:0.0493 not:0.1089 great:0.0171 that:0.0425 of:0.0549 and:0.0547)
		(,:0.0330 the:0.0082 will:0.0664 not:0.1953 great:0.0236 that:0.0698 for:0.0884 and:0.0967)
		(,:0.0432 the:0.0173 will:0.0659 not:0.2432 very:0.0251 that:0.0903 for:0.1172 and:0.1514)
		(,:0.0898 the:0.0420 have:0.0579 not:0.2109 good:0.0271 that:0.0894 for:0.1206 and:0.1797)
		(,:0.2188 the:0.0718 am:0.0659 not:0.1387 good:0.0239 that:0.0688.:0.1387 I:0.2080)
		(,:0.2188 the:0.0718 am:0.0659 not:0.1387 good:0.0239 that:0.0688.:0.1387 I:0.2080)
 I
------
		(inally:0.0035'll:0.0017
:0.0128greSQL:0.0010 and:0.1377 and:0.0354inally:0.0036inally:0.0025)
		( and:0.0195,:0.0264,:0.0118,:0.0111 and:0.0781 and:0.0459 and:0.0206,:0.0199)
		( and:0.0206,:0.0187,:0.0070,:0.0063 and:0.0312-:0.0286 and:0.0304 is:0.0168)
		( and:0.0255,:0.0168 not:0.0060.:0.0050 and:0.0200-:0.0187 and:0.0425 is:0.0140)
		( and:0.0222,:0.0129 not:0.0097.:0.0042 that:0.0167 of:0.0187 and:0.0366 is:0.0123)
		( the:0.0155 will:0.0192 not:0.0187 nice:0.0057 that:0.0189 of:0.0210 and:0.0327'll:0.0371)
		( the:0.0098 will:0.0308 not:0.0452 great:0.0099 that:0.0260 of:0.0315 and:0.0369'm:0.1196)
		( the:0.0071 will:0.0493 not:0.1089 great:0.0171 that:0.0425 of:0.0549 and:0.0547'm:0.2598)
		( the:0.0082 will:0.0664 not:0.1953 great:0.0236 that:0.0698 for:0.0884 and:0.0967'm:0.3828)
		( the:0.0173 will:0.0659 not:0.2432 very:0.0251 that:0.0903 for:0.1172 and:0.1514'm:0.4688)
		( the:0.0420 have:0.0579 not:0.2109 good:0.0271 that:0.0894 for:0.1206 and:0.1797'm:0.5273)
		( the:0.0718 am:0.0659 not:0.1387 good:0.0239 that:0.0688.:0.1387 I:0.2080'm:0.5469)
		( the:0.0718 am:0.0659 not:0.1387 good:0.0239 that:0.0688.:0.1387 I:0.2080'm:0.5469)
'm a good, and I'm a good, and
2450: sample 0: Hello, I'm a language model,
------
		(
:0.0591inally:0.0019inally:0.0021inally:0.0048oubted:0.0012 and:0.0894 and:0.0374'.":0.0021)
		(,:0.1934 and:0.0123,:0.0157 not:0.0035,:0.0078 and:0.0461,:0.0376 and:0.0167)
		(,:0.1260 and:0.0154,:0.0123 not:0.0032,:0.0055-:0.0261-:0.0238 and:0.0312)
		(,:0.0923 and:0.0208,:0.0114 not:0.0045.:0.0052-:0.0200 of:0.0172 and:0.0505)
		(,:0.0728 and:0.0194 will:0.0110 not:0.0074.:0.0056-:0.0183 of:0.0210 and:0.0522)
		(,:0.0569 and:0.0138 will:0.0201 not:0.0165 great:0.0074-:0.0170 of:0.0306 and:0.0601)
		(,:0.0449�:0.0104 will:0.0408 not:0.0457 great:0.0144 of:0.0192 of:0.0613 and:0.0708)
		(,:0.0364�:0.0109 will:0.0845 not:0.1240 great:0.0242 of:0.0347 of:0.1050 and:0.1216)
		(,:0.0325 but:0.0189 will:0.1270 not:0.2363 bit:0.0303 of:0.0547 of:0.1387 and:0.2031)
		(,:0.0396 but:0.0374 will:0.1338 not:0.2656 bit:0.0369 of:0.0640 of:0.1426 and:0.2461)
		(,:0.0747 and:0.0762 will:0.0981 not:0.2139 bit:0.0327,:0.0576 of:0.1270 and:0.2461)
		(,:0.1270 and:0.1133 have:0.0610 not:0.1582 bit:0.0232,:0.0549.:0.1084 and:0.2217)
		(,:0.1270 and:0.1133 have:0.0610 not:0.1582 bit:0.0232,:0.0549.:0.1084 and:0.2217)
 and
------
		(inally:0.0019inally:0.0021inally:0.0048oubted:0.0012 and:0.0894 and:0.0374'.":0.0021")):0.0009)
		( and:0.0123,:0.0157 not:0.0035,:0.0078 and:0.0461,:0.0376 and:0.0167,:0.0085)
		( and:0.0154,:0.0123 not:0.0032,:0.0055-:0.0261-:0.0238 and:0.0312,:0.0103)
		( and:0.0208,:0.0114 not:0.0045.:0.0052-:0.0200 of:0.0172 and:0.0505 the:0.0159)
		( and:0.0194 will:0.0110 not:0.0074.:0.0056-:0.0183 of:0.0210 and:0.0522 the:0.0253)
		( and:0.0138 will:0.0201 not:0.0165 great:0.0074-:0.0170 of:0.0306 and:0.0601 the:0.0403)
		(�:0.0104 will:0.0408 not:0.0457 great:0.0144 of:0.0192 of:0.0613 and:0.0708 the:0.0454)
		(�:0.0109 will:0.0845 not:0.1240 great:0.0242 of:0.0347 of:0.1050 and:0.1216 I:0.0615)
		( but:0.0189 will:0.1270 not:0.2363 bit:0.0303 of:0.0547 of:0.1387 and:0.2031 I:0.1553)
		( but:0.0374 will:0.1338 not:0.2656 bit:0.0369 of:0.0640 of:0.1426 and:0.2461 I:0.2988)
		( and:0.0762 will:0.0981 not:0.2139 bit:0.0327,:0.0576 of:0.1270 and:0.2461 I:0.3516)
		( and:0.1133 have:0.0610 not:0.1582 bit:0.0232,:0.0549.:0.1084 and:0.2217 I:0.2871)
		( and:0.1133 have:0.0610 not:0.1582 bit:0.0232,:0.0549.:0.1084 and:0.2217 I:0.2871)
 I'm not a language model.
The first language
2600: sample 0: Hello, I'm a language model,
------
		(
:0.0461inally:0.0015inally:0.0020inally:0.0060greSQL:0.0014 and:0.0608 and:0.0452'.":0.0017)
		(,:0.1699 and:0.0125,:0.0126 not:0.0029,:0.0052 and:0.0403,:0.0408 and:0.0130)
		(,:0.1108 and:0.0194,:0.0142as:0.0037,:0.0043 and:0.0220-:0.0269 and:0.0327)
		(,:0.0820 and:0.0269,:0.0162 not:0.0050.:0.0047 and:0.0172 and:0.0208 and:0.0576)
		(,:0.0640 and:0.0248,:0.0145 not:0.0085 nice:0.0051 and:0.0150 of:0.0206 and:0.0679)
		(,:0.0483 and:0.0172 will:0.0192 not:0.0168 nice:0.0082 learners:0.0150 of:0.0266 and:0.0771)
		(,:0.0364 and:0.0097 will:0.0294 not:0.0369 great:0.0133 of:0.0217 of:0.0417 and:0.0923)
		(,:0.0264 and:0.0063 will:0.0479 not:0.0835 great:0.0219 of:0.0398 of:0.0664 and:0.1318)
		(,:0.0215 you:0.0083 will:0.0718 not:0.1436 great:0.0303 of:0.0640,:0.0962 and:0.2021)
		(,:0.0232 you:0.0187 was:0.0928 not:0.1748 bit:0.0393 of:0.0776,:0.1494 and:0.2617)
		(,:0.0396 and:0.0564 was:0.1025 not:0.1426 bit:0.0393,:0.1143.:0.2012 and:0.2949)
		(,:0.0649 and:0.1006 was:0.0933 not:0.0874 bit:0.0288,:0.1299.:0.2383 and:0.3145)
		(,:0.0649 and:0.1006 was:0.0933 not:0.0874 bit:0.0288,:0.1299.:0.2383 and:0.3145)
 and
------
		(inally:0.0015inally:0.0020inally:0.0060greSQL:0.0014 and:0.0608 and:0.0452'.":0.0017")):0.0009)
		( and:0.0125,:0.0126 not:0.0029,:0.0052 and:0.0403,:0.0408 and:0.0130,:0.0062)
		( and:0.0194,:0.0142as:0.0037,:0.0043 and:0.0220-:0.0269 and:0.0327 the:0.0093)
		( and:0.0269,:0.0162 not:0.0050.:0.0047 and:0.0172 and:0.0208 and:0.0576 the:0.0150)
		( and:0.0248,:0.0145 not:0.0085 nice:0.0051 and:0.0150 of:0.0206 and:0.0679 the:0.0186)
		( and:0.0172 will:0.0192 not:0.0168 nice:0.0082 learners:0.0150 of:0.0266 and:0.0771 the:0.0270)
		( and:0.0097 will:0.0294 not:0.0369 great:0.0133 of:0.0217 of:0.0417 and:0.0923 the:0.0332)
		( and:0.0063 will:0.0479 not:0.0835 great:0.0219 of:0.0398 of:0.0664 and:0.1318 I:0.0620)
		( you:0.0083 will:0.0718 not:0.1436 great:0.0303 of:0.0640,:0.0962 and:0.2021 I:0.1709)
		( you:0.0187 was:0.0928 not:0.1748 bit:0.0393 of:0.0776,:0.1494 and:0.2617 I:0.3320)
		( and:0.0564 was:0.1025 not:0.1426 bit:0.0393,:0.1143.:0.2012 and:0.2949 I:0.4102)
		( and:0.1006 was:0.0933 not:0.0874 bit:0.0288,:0.1299.:0.2383 and:0.3145 I:0.3789)
		( and:0.1006 was:0.0933 not:0.0874 bit:0.0288,:0.1299.:0.2383 and:0.3145 I:0.3789)
 I'm a language model. I'm a language model
2750: sample 0: Hello, I'm a language model,
------
		(
:0.0452inally:0.0015inally:0.0022inally:0.0060greSQL:0.0018 and:0.0486 and:0.0356inally:0.0017)
		(,:0.1895 and:0.0114,:0.0073,:0.0026,:0.0038-:0.0376 and:0.0332 and:0.0152)
		(,:0.1289 and:0.0271,:0.0112::0.0047.:0.0041-:0.0254-:0.0270 and:0.0476)
		(,:0.0977 and:0.0454,:0.0118 called:0.0070 nice:0.0057-:0.0182 of:0.0200 and:0.0674)
		(,:0.0762 and:0.0452 will:0.0167 not:0.0128 nice:0.0105 that:0.0153 of:0.0200 and:0.0688)
		(,:0.0591 and:0.0322 will:0.0267 not:0.0287 nice:0.0187 of:0.0148 of:0.0215 and:0.0781)
		(,:0.0435 and:0.0201 will:0.0469 not:0.0635 nice:0.0287 of:0.0244 of:0.0304 and:0.0859)
		(,:0.0334 and:0.0155 will:0.0713 not:0.1167 great:0.0347 of:0.0449 of:0.0430 and:0.1270)
		(,:0.0286 but:0.0249 will:0.0845 not:0.1553 great:0.0459 of:0.0742 of:0.0566 and:0.1934)
		(,:0.0344 and:0.0427�:0.0757 not:0.1484 great:0.0466 of:0.0908.:0.0962 and:0.2305)
		(,:0.0732 I:0.0845�:0.0972 not:0.1167 great:0.0388 of:0.0825.:0.1426 and:0.2559)
		(,:0.1299 I:0.1143�:0.1104 going:0.1104 proud:0.0322 teacher:0.0688.:0.1592 and:0.2715)
		(,:0.1299 I:0.1143�:0.1104 going:0.1104 proud:0.0322 teacher:0.0688.:0.1592 and:0.2715)
 and
------
		(inally:0.0015inally:0.0022inally:0.0060greSQL:0.0018 and:0.0486 and:0.0356inally:0.0017!'":0.0011)
		( and:0.0114,:0.0073,:0.0026,:0.0038-:0.0376 and:0.0332 and:0.0152,:0.0052)
		( and:0.0271,:0.0112::0.0047.:0.0041-:0.0254-:0.0270 and:0.0476 the:0.0143)
		( and:0.0454,:0.0118 called:0.0070 nice:0.0057-:0.0182 of:0.0200 and:0.0674 the:0.0245)
		( and:0.0452 will:0.0167 not:0.0128 nice:0.0105 that:0.0153 of:0.0200 and:0.0688 the:0.0347)
		( and:0.0322 will:0.0267 not:0.0287 nice:0.0187 of:0.0148 of:0.0215 and:0.0781 the:0.0371)
		( and:0.0201 will:0.0469 not:0.0635 nice:0.0287 of:0.0244 of:0.0304 and:0.0859 I:0.0530)
		( and:0.0155 will:0.0713 not:0.1167 great:0.0347 of:0.0449 of:0.0430 and:0.1270 I:0.1270)
		( but:0.0249 will:0.0845 not:0.1553 great:0.0459 of:0.0742 of:0.0566 and:0.1934 I:0.3164)
		( and:0.0427�:0.0757 not:0.1484 great:0.0466 of:0.0908.:0.0962 and:0.2305 I:0.5273)
		( I:0.0845�:0.0972 not:0.1167 great:0.0388 of:0.0825.:0.1426 and:0.2559 I:0.5859)
		( I:0.1143�:0.1104 going:0.1104 proud:0.0322 teacher:0.0688.:0.1592 and:0.2715 I:0.4844)
		( I:0.1143�:0.1104 going:0.1104 proud:0.0322 teacher:0.0688.:0.1592 and:0.2715 I:0.4844)
 I'm not sure what I'm going to do.
2900: sample 0: Hello, I'm a language model,
------
		(
:0.0251."":0.0019>.:0.0018 not:0.0034greSQL:0.0023 and:0.0542 and:0.0172'.":0.0017)
		(,:0.1396 and:0.0063,:0.0054 not:0.0031,:0.0039-:0.0439-:0.0228 and:0.0086)
		(,:0.0923 and:0.0177-:0.0093 called:0.0062,:0.0034-:0.0288-:0.0192 and:0.0295)
		(,:0.0713 and:0.0293 will:0.0132 called:0.0093 nice:0.0040-:0.0238 of:0.0208 and:0.0391)
		(,:0.0554 and:0.0266 will:0.0214 not:0.0178 nice:0.0073-:0.0200 of:0.0276 and:0.0386)
		(,:0.0425 and:0.0187 will:0.0325 not:0.0427 nice:0.0132-:0.0184 of:0.0447 and:0.0447)
		(,:0.0317 and:0.0121 will:0.0435 not:0.0981 nice:0.0212 of:0.0247 of:0.0796 and:0.0623)
		(,:0.0227 and:0.0103 will:0.0547 not:0.1807 bit:0.0270 of:0.0427 of:0.1201 and:0.1138)
		(,:0.0183 you:0.0156 was:0.0596 not:0.2451 bit:0.0513 of:0.0635 of:0.1445 and:0.2070)
		(,:0.0228 you:0.0400�:0.0708 not:0.2432 good:0.0688 of:0.0664 of:0.1328 and:0.2637)
		(,:0.0583 you:0.0781�:0.0874 not:0.1885 good:0.0723,:0.0757.:0.1611 and:0.2539)
		(,:0.1514 you:0.0967�:0.0977 going:0.1289 good:0.0586,:0.0684.:0.1768 and:0.2197)
		(,:0.1514 you:0.0967�:0.0977 going:0.1289 good:0.0586,:0.0684.:0.1768 and:0.2197)
 and
------
		(."":0.0019>.:0.0018 not:0.0034greSQL:0.0023 and:0.0542 and:0.0172'.":0.0017>::0.0011)
		( and:0.0063,:0.0054 not:0.0031,:0.0039-:0.0439-:0.0228 and:0.0086,:0.0039)
		( and:0.0177-:0.0093 called:0.0062,:0.0034-:0.0288-:0.0192 and:0.0295 the:0.0082)
		( and:0.0293 will:0.0132 called:0.0093 nice:0.0040-:0.0238 of:0.0208 and:0.0391 the:0.0150)
		( and:0.0266 will:0.0214 not:0.0178 nice:0.0073-:0.0200 of:0.0276 and:0.0386 the:0.0228)
		( and:0.0187 will:0.0325 not:0.0427 nice:0.0132-:0.0184 of:0.0447 and:0.0447 I:0.0278)
		( and:0.0121 will:0.0435 not:0.0981 nice:0.0212 of:0.0247 of:0.0796 and:0.0623 I:0.0566)
		( and:0.0103 will:0.0547 not:0.1807 bit:0.0270 of:0.0427 of:0.1201 and:0.1138 I:0.1367)
		( you:0.0156 was:0.0596 not:0.2451 bit:0.0513 of:0.0635 of:0.1445 and:0.2070 I:0.3164)
		( you:0.0400�:0.0708 not:0.2432 good:0.0688 of:0.0664 of:0.1328 and:0.2637 I:0.4785)
		( you:0.0781�:0.0874 not:0.1885 good:0.0723,:0.0757.:0.1611 and:0.2539 I:0.5312)
		( you:0.0967�:0.0977 going:0.1289 good:0.0586,:0.0684.:0.1768 and:0.2197 I:0.4629)
		( you:0.0967�:0.0977 going:0.1289 good:0.0586,:0.0684.:0.1768 and:0.2197 I:0.4629)
 I'm a language model. I'm not a language
3050: sample 0: Hello, I'm a language model,
------
		(
:0.0278."":0.0019>.:0.0021aintain:0.0032greSQL:0.0015 and:0.0376 and:0.0247."":0.0017)
		(,:0.1562 and:0.0058,:0.0031 not:0.0025,:0.0016 and:0.0204 and:0.0208 and:0.0055)
		(,:0.1099 and:0.0208,:0.0067 not:0.0050 nice:0.0025 and:0.0167 of:0.0251 and:0.0272)
		(,:0.0879 and:0.0354 will:0.0120 not:0.0096 nice:0.0047 and:0.0165 of:0.0330 and:0.0500)
		(,:0.0688 and:0.0305 will:0.0173 not:0.0178 nice:0.0087 of:0.0160 of:0.0334 and:0.0537)
		(,:0.0503 and:0.0176 will:0.0243 not:0.0334 nice:0.0155 of:0.0186 of:0.0371 and:0.0510)
		(,:0.0320 but:0.0126 will:0.0352 not:0.0649 nice:0.0226 of:0.0294 of:0.0515 and:0.0583)
		(,:0.0195 but:0.0188 will:0.0471 not:0.1113 nice:0.0250 of:0.0552 of:0.0618 and:0.1050)
		(,:0.0126 but:0.0344 will:0.0554 not:0.1426 bit:0.0334 of:0.0850,:0.0688 and:0.1758)
		(,:0.0125 but:0.0503 was:0.0520 sure:0.1436 good:0.0408 of:0.0947.:0.0894 and:0.2051)
		(,:0.0267 I:0.0913 was:0.0586 sure:0.1484 good:0.0420 of:0.0811.:0.1099 and:0.1875)
		(.:0.0586 I:0.1533 was:0.0625 going:0.1338 good:0.0364 teacher:0.0894.:0.1016 and:0.1758)
		(.:0.0586 I:0.1533 was:0.0625 going:0.1338 good:0.0364 teacher:0.0894.:0.1016 and:0.1758)
 and
------
		(."":0.0019>.:0.0021aintain:0.0032greSQL:0.0015 and:0.0376 and:0.0247."":0.0017ruff:0.0012)
		( and:0.0058,:0.0031 not:0.0025,:0.0016 and:0.0204 and:0.0208 and:0.0055,:0.0028)
		( and:0.0208,:0.0067 not:0.0050 nice:0.0025 and:0.0167 of:0.0251 and:0.0272 the:0.0078)
		( and:0.0354 will:0.0120 not:0.0096 nice:0.0047 and:0.0165 of:0.0330 and:0.0500 the:0.0199)
		( and:0.0305 will:0.0173 not:0.0178 nice:0.0087 of:0.0160 of:0.0334 and:0.0537 the:0.0349)
		( and:0.0176 will:0.0243 not:0.0334 nice:0.0155 of:0.0186 of:0.0371 and:0.0510 I:0.0369)
		( but:0.0126 will:0.0352 not:0.0649 nice:0.0226 of:0.0294 of:0.0515 and:0.0583 I:0.0771)
		( but:0.0188 will:0.0471 not:0.1113 nice:0.0250 of:0.0552 of:0.0618 and:0.1050 I:0.1719)
		( but:0.0344 will:0.0554 not:0.1426 bit:0.0334 of:0.0850,:0.0688 and:0.1758 I:0.3203)
		( but:0.0503 was:0.0520 sure:0.1436 good:0.0408 of:0.0947.:0.0894 and:0.2051 I:0.4688)
		( I:0.0913 was:0.0586 sure:0.1484 good:0.0420 of:0.0811.:0.1099 and:0.1875 I:0.5000)
		( I:0.1533 was:0.0625 going:0.1338 good:0.0364 teacher:0.0894.:0.1016 and:0.1758 I:0.4336)
		( I:0.1533 was:0.0625 going:0.1338 good:0.0364 teacher:0.0894.:0.1016 and:0.1758 I:0.4336)
 I'm a teacher.
I'm a teacher.
3200: sample 0: Hello, I'm a language model,
------
		(
:0.0186''.:0.0017>.:0.0018aintain:0.0024greSQL:0.0019 and:0.0302 and:0.0192''.:0.0019)
		(,:0.1338 and:0.0050,:0.0039aks:0.0022,:0.0017-:0.0210-:0.0181 and:0.0046)
		(,:0.0903 and:0.0178,:0.0074 called:0.0034.:0.0021-:0.0171-:0.0162 and:0.0186)
		(,:0.0718 and:0.0276,:0.0087 called:0.0056 nice:0.0034-:0.0131 of:0.0139 and:0.0255)
		(,:0.0576 and:0.0228 will:0.0101 not:0.0084 nice:0.0060-:0.0096 of:0.0131 and:0.0247)
		(,:0.0442 and:0.0145 was:0.0176 not:0.0166 nice:0.0104-:0.0068 of:0.0138 and:0.0240)
		(,:0.0322 and:0.0092 was:0.0304 not:0.0356 nice:0.0149 speaker:0.0099 of:0.0171 and:0.0259)
		(,:0.0227 but:0.0093 was:0.0513 not:0.0698 great:0.0170 speaker:0.0165 of:0.0225 and:0.0444)
		(,:0.0167 but:0.0192 was:0.0698 not:0.1040 great:0.0229 specialist:0.0300er:0.0437 and:0.0894)
		(,:0.0165 you:0.0496 was:0.0830 not:0.1191 great:0.0239 specialist:0.0466er:0.0723 and:0.1426)
		(,:0.0334 and:0.1060'm:0.0938 going:0.1309 professor:0.0320 teacher:0.0588er:0.0928 I:0.1992)
		(.:0.1260 and:0.1377 was:0.0859 going:0.1309 professor:0.0349 teacher:0.0786er:0.0884 I:0.2139)
		(.:0.1260 and:0.1377 was:0.0859 going:0.1309 professor:0.0349 teacher:0.0786er:0.0884 I:0.2139)
 I
------
		(''.:0.0017>.:0.0018aintain:0.0024greSQL:0.0019 and:0.0302 and:0.0192''.:0.0019').:0.0023)
		( and:0.0050,:0.0039aks:0.0022,:0.0017-:0.0210-:0.0181 and:0.0046,:0.0039)
		( and:0.0178,:0.0074 called:0.0034.:0.0021-:0.0171-:0.0162 and:0.0186 is:0.0063)
		( and:0.0276,:0.0087 called:0.0056 nice:0.0034-:0.0131 of:0.0139 and:0.0255 is:0.0084)
		( and:0.0228 will:0.0101 not:0.0084 nice:0.0060-:0.0096 of:0.0131 and:0.0247 can:0.0156)
		( and:0.0145 was:0.0176 not:0.0166 nice:0.0104-:0.0068 of:0.0138 and:0.0240'm:0.0505)
		( and:0.0092 was:0.0304 not:0.0356 nice:0.0149 speaker:0.0099 of:0.0171 and:0.0259'm:0.1240)
		( but:0.0093 was:0.0513 not:0.0698 great:0.0170 speaker:0.0165 of:0.0225 and:0.0444'm:0.2217)
		( but:0.0192 was:0.0698 not:0.1040 great:0.0229 specialist:0.0300er:0.0437 and:0.0894'm:0.3145)
		( you:0.0496 was:0.0830 not:0.1191 great:0.0239 specialist:0.0466er:0.0723 and:0.1426'm:0.4141)
		( and:0.1060'm:0.0938 going:0.1309 professor:0.0320 teacher:0.0588er:0.0928 I:0.1992'm:0.4805)
		( and:0.1377 was:0.0859 going:0.1309 professor:0.0349 teacher:0.0786er:0.0884 I:0.2139'm:0.5195)
		( and:0.1377 was:0.0859 going:0.1309 professor:0.0349 teacher:0.0786er:0.0884 I:0.2139'm:0.5195)
'm a professor, I'm a professor, I'm
3350: sample 0: Hello, I'm a language model,
------
		(
:0.0223."":0.0017>.:0.0015okers:0.0016greSQL:0.0017 and:0.0286 and:0.0222''.:0.0015)
		(,:0.1582 and:0.0063,:0.0042,:0.0023,:0.0014-:0.0247 and:0.0233 and:0.0056)
		(,:0.1118 and:0.0166,:0.0073 called:0.0044 new:0.0022-:0.0195 and:0.0176 and:0.0212)
		(,:0.0898 and:0.0291,:0.0103 called:0.0096 new:0.0034-:0.0152 of:0.0197 and:0.0439)
		(,:0.0723 and:0.0302,:0.0106 called:0.0140 nice:0.0052 called:0.0134 of:0.0242 and:0.0596)
		(,:0.0562 and:0.0236 will:0.0152 called:0.0161 my:0.0087 called:0.0166 of:0.0275 and:0.0688)
		(,:0.0413 and:0.0162 will:0.0240 the:0.0227 my:0.0138 of:0.0209 of:0.0327 and:0.0771)
		(,:0.0284 but:0.0156 will:0.0386 not:0.0439 my:0.0145 of:0.0369 of:0.0396 and:0.1011)
		(,:0.0198 but:0.0292 am:0.0544 not:0.0762 great:0.0173 of:0.0586 that:0.0552 and:0.1445)
		(,:0.0183 but:0.0535 am:0.0771 a:0.0986 good:0.0216 of:0.0747 that:0.0771 and:0.1895)
		(,:0.0374 and:0.0615 am:0.0933 a:0.0991 good:0.0250 that:0.0913,:0.0879 and:0.2129)
		(,:0.1494 I:0.1050 am:0.0991 a:0.0879 professor:0.0295 that:0.0806.:0.1084 and:0.2100)
		(,:0.1494 I:0.1050 am:0.0991 a:0.0879 professor:0.0295 that:0.0806.:0.1084 and:0.2100)
 and
------
		(."":0.0017>.:0.0015okers:0.0016greSQL:0.0017 and:0.0286 and:0.0222''.:0.0015 actresses:0.0020)
		( and:0.0063,:0.0042,:0.0023,:0.0014-:0.0247 and:0.0233 and:0.0056,:0.0023)
		( and:0.0166,:0.0073 called:0.0044 new:0.0022-:0.0195 and:0.0176 and:0.0212 the:0.0060)
		( and:0.0291,:0.0103 called:0.0096 new:0.0034-:0.0152 of:0.0197 and:0.0439 the:0.0171)
		( and:0.0302,:0.0106 called:0.0140 nice:0.0052 called:0.0134 of:0.0242 and:0.0596 the:0.0325)
		( and:0.0236 will:0.0152 called:0.0161 my:0.0087 called:0.0166 of:0.0275 and:0.0688 the:0.0408)
		( and:0.0162 will:0.0240 the:0.0227 my:0.0138 of:0.0209 of:0.0327 and:0.0771 I:0.0415)
		( but:0.0156 will:0.0386 not:0.0439 my:0.0145 of:0.0369 of:0.0396 and:0.1011 I:0.1196)
		( but:0.0292 am:0.0544 not:0.0762 great:0.0173 of:0.0586 that:0.0552 and:0.1445 I:0.3262)
		( but:0.0535 am:0.0771 a:0.0986 good:0.0216 of:0.0747 that:0.0771 and:0.1895 I:0.5508)
		( and:0.0615 am:0.0933 a:0.0991 good:0.0250 that:0.0913,:0.0879 and:0.2129 I:0.6172)
		( I:0.1050 am:0.0991 a:0.0879 professor:0.0295 that:0.0806.:0.1084 and:0.2100 I:0.5156)
		( I:0.1050 am:0.0991 a:0.0879 professor:0.0295 that:0.0806.:0.1084 and:0.2100 I:0.5156)
 I'm a professor. I'm a professor. I
3500: sample 0: Hello, I'm a language model,
------
		(,:0.0282."":0.0020>.:0.0024asses:0.0027greSQL:0.0015 and:0.0234 and:0.0154."":0.0018)
		(,:0.1367 and:0.0036,:0.0035,:0.0014,:0.0018-:0.0168-:0.0229 and:0.0043)
		(,:0.1021 and:0.0154,:0.0072 called:0.0031 new:0.0020-:0.0148-:0.0249 and:0.0244)
		(,:0.0850 and:0.0271,:0.0109 called:0.0059.:0.0026-:0.0126 of:0.0292 and:0.0442)
		(,:0.0708 and:0.0272-:0.0138 called:0.0073 my:0.0065-:0.0104 of:0.0378 and:0.0474)
		(,:0.0552 and:0.0189 was:0.0215 sure:0.0117 my:0.0099-:0.0089 of:0.0430 and:0.0430)
		(,:0.0396 and:0.0117 was:0.0315 sure:0.0269 great:0.0104 of:0.0110 of:0.0496 and:0.0476)
		(,:0.0270 the:0.0085 was:0.0425 sure:0.0625 great:0.0162 of:0.0173 of:0.0559 and:0.0752)
		(,:0.0179 the:0.0131 was:0.0549 sure:0.1187 very:0.0237 teacher:0.0282 for:0.0688 and:0.1260)
		(,:0.0151 I:0.0299 was:0.0679 sure:0.1533 bit:0.0280 teacher:0.0537,:0.0825 and:0.1660)
		(,:0.0260 I:0.0859 was:0.0776 sure:0.1406 bit:0.0277 teacher:0.0908,:0.0942 I:0.2695)
		(,:0.0688 I:0.1416�:0.0977 sure:0.0942 good:0.0212 teacher:0.1235.:0.1001 I:0.2793)
		(,:0.0688 I:0.1416�:0.0977 sure:0.0942 good:0.0212 teacher:0.1235.:0.1001 I:0.2793)
 I
------
		(."":0.0020>.:0.0024asses:0.0027greSQL:0.0015 and:0.0234 and:0.0154."":0.0018>.:0.0022)
		( and:0.0036,:0.0035,:0.0014,:0.0018-:0.0168-:0.0229 and:0.0043 will:0.0033)
		( and:0.0154,:0.0072 called:0.0031 new:0.0020-:0.0148-:0.0249 and:0.0244 is:0.0070)
		( and:0.0271,:0.0109 called:0.0059.:0.0026-:0.0126 of:0.0292 and:0.0442 is:0.0090)
		( and:0.0272-:0.0138 called:0.0073 my:0.0065-:0.0104 of:0.0378 and:0.0474 was:0.0146)
		( and:0.0189 was:0.0215 sure:0.0117 my:0.0099-:0.0089 of:0.0430 and:0.0430'm:0.0374)
		( and:0.0117 was:0.0315 sure:0.0269 great:0.0104 of:0.0110 of:0.0496 and:0.0476'm:0.0967)
		( the:0.0085 was:0.0425 sure:0.0625 great:0.0162 of:0.0173 of:0.0559 and:0.0752'm:0.1582)
		( the:0.0131 was:0.0549 sure:0.1187 very:0.0237 teacher:0.0282 for:0.0688 and:0.1260'm:0.2217)
		( I:0.0299 was:0.0679 sure:0.1533 bit:0.0280 teacher:0.0537,:0.0825 and:0.1660'm:0.2773)
		( I:0.0859 was:0.0776 sure:0.1406 bit:0.0277 teacher:0.0908,:0.0942 I:0.2695'm:0.3105)
		( I:0.1416�:0.0977 sure:0.0942 good:0.0212 teacher:0.1235.:0.1001 I:0.2793'm:0.3145)
		( I:0.1416�:0.0977 sure:0.0942 good:0.0212 teacher:0.1235.:0.1001 I:0.2793'm:0.3145)
'm a language model, I'm a language model,
3650: sample 0: Hello, I'm a language model,
------
		(
:0.0116 Hermione:0.0021>.:0.0018asses:0.0017greSQL:0.0015 and:0.0283 and:0.0105 Hermione:0.0016)
		(,:0.1147 and:0.0035 will:0.0026pt:0.0011,:0.0012-:0.0183-:0.0143 and:0.0035)
		(,:0.0903 and:0.0173 will:0.0058 new:0.0025.:0.0025-:0.0178 of:0.0186 and:0.0226)
		(,:0.0791 and:0.0295 will:0.0096 called:0.0036.:0.0035-:0.0148 of:0.0259 and:0.0417)
		(,:0.0659 and:0.0280 will:0.0135 sure:0.0056 my:0.0047-:0.0098 of:0.0292 and:0.0420)
		(,:0.0513 and:0.0186 will:0.0192 not:0.0115 nice:0.0067 arts:0.0087 of:0.0327 and:0.0281)
		(,:0.0366 and:0.0115 will:0.0282 not:0.0259 nice:0.0103 scientist:0.0142 of:0.0371 and:0.0240)
		(,:0.0242 and:0.0098 will:0.0422 not:0.0530 great:0.0120 scientist:0.0265 for:0.0574 and:0.0452)
		(,:0.0166 the:0.0156 was:0.0552 not:0.0884 great:0.0162 teacher:0.0474 for:0.0962 and:0.1060)
		(,:0.0143 the:0.0369�:0.0840 not:0.1128 teacher:0.0200 teacher:0.0894 for:0.1201 and:0.1699)
		(,:0.0284 and:0.0713�:0.1201 going:0.1216 teacher:0.0266 teacher:0.1416 for:0.1162 and:0.1963)
		(,:0.1187 and:0.1094�:0.1445 going:0.1235 teacher:0.0283 teacher:0.1836.:0.1152 and:0.2148)
		(,:0.1187 and:0.1094�:0.1445 going:0.1235 teacher:0.0283 teacher:0.1836.:0.1152 and:0.2148)
 and
------
		( Hermione:0.0021>.:0.0018asses:0.0017greSQL:0.0015 and:0.0283 and:0.0105 Hermione:0.0016 Hermione:0.0028)
		( and:0.0035 will:0.0026pt:0.0011,:0.0012-:0.0183-:0.0143 and:0.0035 is:0.0017)
		( and:0.0173 will:0.0058 new:0.0025.:0.0025-:0.0178 of:0.0186 and:0.0226 is:0.0084)
		( and:0.0295 will:0.0096 called:0.0036.:0.0035-:0.0148 of:0.0259 and:0.0417 the:0.0142)
		( and:0.0280 will:0.0135 sure:0.0056 my:0.0047-:0.0098 of:0.0292 and:0.0420 the:0.0247)
		( and:0.0186 will:0.0192 not:0.0115 nice:0.0067 arts:0.0087 of:0.0327 and:0.0281 the:0.0216)
		( and:0.0115 will:0.0282 not:0.0259 nice:0.0103 scientist:0.0142 of:0.0371 and:0.0240 I:0.0327)
		( and:0.0098 will:0.0422 not:0.0530 great:0.0120 scientist:0.0265 for:0.0574 and:0.0452 I:0.0879)
		( the:0.0156 was:0.0552 not:0.0884 great:0.0162 teacher:0.0474 for:0.0962 and:0.1060 I:0.2275)
		( the:0.0369�:0.0840 not:0.1128 teacher:0.0200 teacher:0.0894 for:0.1201 and:0.1699 I:0.4238)
		( and:0.0713�:0.1201 going:0.1216 teacher:0.0266 teacher:0.1416 for:0.1162 and:0.1963 I:0.5234)
		( and:0.1094�:0.1445 going:0.1235 teacher:0.0283 teacher:0.1836.:0.1152 and:0.2148 I:0.4961)
		( and:0.1094�:0.1445 going:0.1235 teacher:0.0283 teacher:0.1836.:0.1152 and:0.2148 I:0.4961)
 I'm a language model.
I'm a language
3800: sample 0: Hello, I'm a language model,
------
		(
:0.0146 Hermione:0.0016>.:0.0021asses:0.0023greSQL:0.0018 and:0.0261 and:0.0075."":0.0016)
		(,:0.0845 and:0.0024cing:0.0017 called:0.0020 href:0.0008-:0.0222-:0.0121 and:0.0031)
		(,:0.0698 and:0.0129,:0.0051 called:0.0061 big:0.0015-:0.0187 of:0.0139 and:0.0269)
		(,:0.0613 and:0.0222 will:0.0093 called:0.0095 big:0.0029-:0.0168 of:0.0206 and:0.0598)
		(,:0.0542 and:0.0212 will:0.0152 called:0.0109 my:0.0056-:0.0119 of:0.0215 and:0.0688)
		(,:0.0435 and:0.0145 will:0.0221 not:0.0170 my:0.0074 of:0.0125 of:0.0206 and:0.0527)
		(,:0.0322 and:0.0096 will:0.0294 sure:0.0344 nice:0.0092 of:0.0177er:0.0228 and:0.0525)
		(,:0.0211 the:0.0095'll:0.0422 sure:0.0732 very:0.0124 of:0.0308er:0.0535 and:0.0957)
		(,:0.0127 the:0.0181'll:0.0583 sure:0.1250 very:0.0172 of:0.0432er:0.0933 and:0.1768)
		(,:0.0093 the:0.0415 am:0.0698 sure:0.1475 very:0.0173,:0.0439er:0.1250 and:0.2432)
		(,:0.0137 and:0.0771 am:0.0728 sure:0.1357 professor:0.0156,:0.0547er:0.1348 and:0.2393)
		(,:0.0312 and:0.1191 was:0.0708 sure:0.1006 professor:0.0168,:0.0471.:0.1143 I:0.2236)
		(,:0.0312 and:0.1191 was:0.0708 sure:0.1006 professor:0.0168,:0.0471.:0.1143 I:0.2236)
 I
------
		( Hermione:0.0016>.:0.0021asses:0.0023greSQL:0.0018 and:0.0261 and:0.0075."":0.0016>.:0.0023)
		( and:0.0024cing:0.0017 called:0.0020 href:0.0008-:0.0222-:0.0121 and:0.0031 will:0.0021)
		( and:0.0129,:0.0051 called:0.0061 big:0.0015-:0.0187 of:0.0139 and:0.0269 is:0.0060)
		( and:0.0222 will:0.0093 called:0.0095 big:0.0029-:0.0168 of:0.0206 and:0.0598 am:0.0109)
		( and:0.0212 will:0.0152 called:0.0109 my:0.0056-:0.0119 of:0.0215 and:0.0688'm:0.0327)
		( and:0.0145 will:0.0221 not:0.0170 my:0.0074 of:0.0125 of:0.0206 and:0.0527'm:0.1084)
		( and:0.0096 will:0.0294 sure:0.0344 nice:0.0092 of:0.0177er:0.0228 and:0.0525'm:0.2656)
		( the:0.0095'll:0.0422 sure:0.0732 very:0.0124 of:0.0308er:0.0535 and:0.0957'm:0.4219)
		( the:0.0181'll:0.0583 sure:0.1250 very:0.0172 of:0.0432er:0.0933 and:0.1768'm:0.5312)
		( the:0.0415 am:0.0698 sure:0.1475 very:0.0173,:0.0439er:0.1250 and:0.2432'm:0.5781)
		( and:0.0771 am:0.0728 sure:0.1357 professor:0.0156,:0.0547er:0.1348 and:0.2393'm:0.6016)
		( and:0.1191 was:0.0708 sure:0.1006 professor:0.0168,:0.0471.:0.1143 I:0.2236'm:0.5977)
		( and:0.1191 was:0.0708 sure:0.1006 professor:0.0168,:0.0471.:0.1143 I:0.2236'm:0.5977)
'm a language model, I'm a language model,
3950: sample 0: Hello, I'm a language model,
------
		(
:0.0090 Hermione:0.0017>.:0.0028asses:0.0027greSQL:0.0013 and:0.0135 and:0.0094."":0.0016)
		(,:0.0835 and:0.0022cing:0.0015 called:0.0019lements:0.0007-:0.0212 and:0.0131 and:0.0023)
		(,:0.0693 and:0.0137 will:0.0041 called:0.0054.:0.0014-:0.0189 of:0.0166 and:0.0194)
		(,:0.0608 and:0.0239 will:0.0075 called:0.0086 my:0.0026-:0.0151 of:0.0200 and:0.0270)
		(,:0.0535 and:0.0166 will:0.0109 called:0.0094 my:0.0055-:0.0090 of:0.0199 and:0.0250)
		(,:0.0425 and:0.0080 will:0.0150 not:0.0138 my:0.0060 arts:0.0095 of:0.0212 and:0.0210)
		(,:0.0300 but:0.0050 will:0.0226 not:0.0283 great:0.0083 interpreter:0.0130 of:0.0234 and:0.0309)
		(,:0.0195 but:0.0101 will:0.0381 sure:0.0698 great:0.0120 of:0.0184,:0.0356 and:0.0664)
		(,:0.0123 but:0.0250 will:0.0569 sure:0.1250 great:0.0148 specialist:0.0303,:0.0698 but:0.1187)
		(,:0.0096 the:0.0486 will:0.0674 sure:0.1484 good:0.0249 specialist:0.0444,:0.0957 but:0.1641)
		(,:0.0147 the:0.0669�:0.0591 not:0.1367 good:0.0332 specialist:0.0542.:0.1104 but:0.1777)
		(,:0.0393 I:0.0732�:0.0796 not:0.1050 good:0.0347 specialist:0.0513.:0.1216 but:0.1689)
		(,:0.0393 I:0.0732�:0.0796 not:0.1050 good:0.0347 specialist:0.0513.:0.1216 but:0.1689)
 but
------
		( Hermione:0.0017>.:0.0028asses:0.0027greSQL:0.0013 and:0.0135 and:0.0094."":0.0016inally:0.0022)
		( and:0.0022cing:0.0015 called:0.0019lements:0.0007-:0.0212 and:0.0131 and:0.0023 with:0.0018)
		( and:0.0137 will:0.0041 called:0.0054.:0.0014-:0.0189 of:0.0166 and:0.0194 with:0.0070)
		( and:0.0239 will:0.0075 called:0.0086 my:0.0026-:0.0151 of:0.0200 and:0.0270 is:0.0077)
		( and:0.0166 will:0.0109 called:0.0094 my:0.0055-:0.0090 of:0.0199 and:0.0250 I:0.0117)
		( and:0.0080 will:0.0150 not:0.0138 my:0.0060 arts:0.0095 of:0.0212 and:0.0210 it:0.0217)
		( but:0.0050 will:0.0226 not:0.0283 great:0.0083 interpreter:0.0130 of:0.0234 and:0.0309 it:0.0483)
		( but:0.0101 will:0.0381 sure:0.0698 great:0.0120 of:0.0184,:0.0356 and:0.0664 it:0.0942)
		( but:0.0250 will:0.0569 sure:0.1250 great:0.0148 specialist:0.0303,:0.0698 but:0.1187 I:0.1621)
		( the:0.0486 will:0.0674 sure:0.1484 good:0.0249 specialist:0.0444,:0.0957 but:0.1641 I:0.3066)
		( the:0.0669�:0.0591 not:0.1367 good:0.0332 specialist:0.0542.:0.1104 but:0.1777 I:0.3965)
		( I:0.0732�:0.0796 not:0.1050 good:0.0347 specialist:0.0513.:0.1216 but:0.1689 I:0.3750)
		( I:0.0732�:0.0796 not:0.1050 good:0.0347 specialist:0.0513.:0.1216 but:0.1689 I:0.3750)
 I'm not a language model, but I'm not
4100: sample 0: Hello, I'm a language model,
------
		(
:0.0106 Hermione:0.0018>.:0.0020asses:0.0022greSQL:0.0017 and:0.0046 and:0.0051 Hermione:0.0013)
		(,:0.0864 and:0.0024cing:0.0016 sure:0.0011 awesome:0.0009-:0.0126 and:0.0101 and:0.0030)
		(,:0.0771 and:0.0117cing:0.0036 sure:0.0023 awesome:0.0014-:0.0122 and:0.0118 and:0.0181)
		(,:0.0684 and:0.0175cing:0.0045 sure:0.0036 my:0.0023-:0.0089 of:0.0115 and:0.0337)
		(,:0.0588 and:0.0160 will:0.0052 sure:0.0056 my:0.0048-:0.0061 of:0.0132 and:0.0354)
		(,:0.0486 and:0.0110 will:0.0077 sure:0.0120 great:0.0070 of:0.0070 model:0.0143 and:0.0293)
		(,:0.0347 the:0.0115 will:0.0126 sure:0.0287 great:0.0118 interpreter:0.0123 that:0.0184 and:0.0339)
		(,:0.0238 the:0.0176 am:0.0229 not:0.0603 great:0.0166 of:0.0237 that:0.0325 but:0.0674)
		(,:0.0156 the:0.0376 will:0.0376 not:0.1108 great:0.0201 of:0.0417,:0.0564 and:0.1299)
		(,:0.0126 the:0.0742 will:0.0498 not:0.1514 bit:0.0234 of:0.0554,:0.0840 and:0.1992)
		(,:0.0199 the:0.0938 will:0.0530 not:0.1572 bit:0.0212 of:0.0549.:0.1167 and:0.2275)
		(,:0.0742 the:0.0869 am:0.0518 not:0.1289 good:0.0177 specialist:0.0459.:0.1504 and:0.2432)
		(,:0.0742 the:0.0869 am:0.0518 not:0.1289 good:0.0177 specialist:0.0459.:0.1504 and:0.2432)
 and
------
		( Hermione:0.0018>.:0.0020asses:0.0022greSQL:0.0017 and:0.0046 and:0.0051 Hermione:0.0013 Hermione:0.0017)
		( and:0.0024cing:0.0016 sure:0.0011 awesome:0.0009-:0.0126 and:0.0101 and:0.0030,:0.0009)
		( and:0.0117cing:0.0036 sure:0.0023 awesome:0.0014-:0.0122 and:0.0118 and:0.0181 is:0.0043)
		( and:0.0175cing:0.0045 sure:0.0036 my:0.0023-:0.0089 of:0.0115 and:0.0337 the:0.0120)
		( and:0.0160 will:0.0052 sure:0.0056 my:0.0048-:0.0061 of:0.0132 and:0.0354 the:0.0206)
		( and:0.0110 will:0.0077 sure:0.0120 great:0.0070 of:0.0070 model:0.0143 and:0.0293 the:0.0204)
		( the:0.0115 will:0.0126 sure:0.0287 great:0.0118 interpreter:0.0123 that:0.0184 and:0.0339 I:0.0281)
		( the:0.0176 am:0.0229 not:0.0603 great:0.0166 of:0.0237 that:0.0325 but:0.0674 I:0.0688)
		( the:0.0376 will:0.0376 not:0.1108 great:0.0201 of:0.0417,:0.0564 and:0.1299 I:0.1543)
		( the:0.0742 will:0.0498 not:0.1514 bit:0.0234 of:0.0554,:0.0840 and:0.1992 I:0.2812)
		( the:0.0938 will:0.0530 not:0.1572 bit:0.0212 of:0.0549.:0.1167 and:0.2275 I:0.3691)
		( the:0.0869 am:0.0518 not:0.1289 good:0.0177 specialist:0.0459.:0.1504 and:0.2432 I:0.3965)
		( the:0.0869 am:0.0518 not:0.1289 good:0.0177 specialist:0.0459.:0.1504 and:0.2432 I:0.3965)
 I'm a linguist. I'm a linguist
4250: sample 0: Hello, I'm a language model,
------
		(
:0.0098 Hermione:0.0015>.:0.0020asses:0.0025greSQL:0.0013 and:0.0052 and:0.0026 Hermione:0.0011)
		(,:0.0723 and:0.0021cing:0.0025 called:0.0011 crucial:0.0011-:0.0063-:0.0055 and:0.0027)
		(,:0.0674 and:0.0176cing:0.0038 called:0.0047 thing:0.0020-:0.0114 of:0.0134 and:0.0386)
		(,:0.0613 and:0.0287cing:0.0052 called:0.0090 my:0.0025-:0.0122 of:0.0209 and:0.0693)
		(,:0.0525 and:0.0278 will:0.0080 called:0.0121 great:0.0056 that:0.0090 of:0.0228 and:0.0757)
		(,:0.0417 and:0.0208 will:0.0139 not:0.0145 great:0.0120 that:0.0088 of:0.0247 and:0.0537)
		(,:0.0294 the:0.0170 will:0.0261 not:0.0312 great:0.0187 that:0.0139 that:0.0334 and:0.0454)
		(,:0.0192 the:0.0247 will:0.0466 not:0.0615 great:0.0251 that:0.0286 that:0.0603 and:0.0659)
		(,:0.0131 the:0.0435 will:0.0703 not:0.0977 great:0.0291 that:0.0479 that:0.0879 and:0.0977)
		(,:0.0123 the:0.0723 will:0.0869 not:0.1162 great:0.0277 that:0.0557 that:0.0996 and:0.1182)
		(,:0.0276 the:0.0913�:0.0923 not:0.1108 great:0.0229 specialist:0.0664.:0.0986 and:0.1235)
		(.:0.1289 the:0.0869�:0.1367 not:0.0713 kid:0.0204 specialist:0.0669.:0.1133 I:0.1328)
		(.:0.1289 the:0.0869�:0.1367 not:0.0713 kid:0.0204 specialist:0.0669.:0.1133 I:0.1328)
 I
------
		( Hermione:0.0015>.:0.0020asses:0.0025greSQL:0.0013 and:0.0052 and:0.0026 Hermione:0.0011>.:0.0020)
		( and:0.0021cing:0.0025 called:0.0011 crucial:0.0011-:0.0063-:0.0055 and:0.0027cing:0.0025)
		( and:0.0176cing:0.0038 called:0.0047 thing:0.0020-:0.0114 of:0.0134 and:0.0386cing:0.0035)
		( and:0.0287cing:0.0052 called:0.0090 my:0.0025-:0.0122 of:0.0209 and:0.0693'm:0.0067)
		( and:0.0278 will:0.0080 called:0.0121 great:0.0056 that:0.0090 of:0.0228 and:0.0757'm:0.0209)
		( and:0.0208 will:0.0139 not:0.0145 great:0.0120 that:0.0088 of:0.0247 and:0.0537'm:0.0820)
		( the:0.0170 will:0.0261 not:0.0312 great:0.0187 that:0.0139 that:0.0334 and:0.0454'm:0.2422)
		( the:0.0247 will:0.0466 not:0.0615 great:0.0251 that:0.0286 that:0.0603 and:0.0659'm:0.4004)
		( the:0.0435 will:0.0703 not:0.0977 great:0.0291 that:0.0479 that:0.0879 and:0.0977'm:0.5195)
		( the:0.0723 will:0.0869 not:0.1162 great:0.0277 that:0.0557 that:0.0996 and:0.1182'm:0.5898)
		( the:0.0913�:0.0923 not:0.1108 great:0.0229 specialist:0.0664.:0.0986 and:0.1235'm:0.6328)
		( the:0.0869�:0.1367 not:0.0713 kid:0.0204 specialist:0.0669.:0.1133 I:0.1328'm:0.6641)
		( the:0.0869�:0.1367 not:0.0713 kid:0.0204 specialist:0.0669.:0.1133 I:0.1328'm:0.6641)
'm a language model, I'm a language model,
4400: sample 0: Hello, I'm a language model,
------
		( be:0.0098 Hermione:0.0011>.:0.0024asses:0.0019greSQL:0.0012 and:0.0171 and:0.0029")):0.0013)
		(,:0.0747 and:0.0020cing:0.0030 mentioned:0.0014 noticeable:0.0011 and:0.0137 and:0.0050 and:0.0023)
		(,:0.0713 and:0.0124cing:0.0057 not:0.0030 awesome:0.0014 and:0.0170 and:0.0101 and:0.0189)
		(,:0.0654 and:0.0247 will:0.0064 not:0.0059 my:0.0035 and:0.0161 and:0.0134 and:0.0469)
		(,:0.0571 and:0.0210 will:0.0107 not:0.0098 my:0.0076 and:0.0087 of:0.0146 and:0.0461)
		(,:0.0439 and:0.0133 will:0.0164 sure:0.0182 nice:0.0096 specialist:0.0063 that:0.0162 and:0.0295)
		(,:0.0295 however:0.0120 will:0.0250 sure:0.0415 nice:0.0132 specialist:0.0101er:0.0283 but:0.0444)
		(,:0.0182 but:0.0240 will:0.0359 sure:0.0762 bit:0.0193 scientist:0.0186er:0.0527 but:0.1001)
		(,:0.0117 but:0.0447'm:0.0461 sure:0.1021 bit:0.0277 scientist:0.0371er:0.0767 but:0.1631)
		(,:0.0123 but:0.0544�:0.0566 sure:0.1030 bit:0.0309 scientist:0.0605er:0.0884 but:0.1865)
		(,:0.0417 and:0.0659�:0.0923 not:0.0991 fan:0.0381 scientist:0.0742,:0.0967 but:0.1768)
		(,:0.2041 and:0.0776�:0.1318 not:0.0776 fan:0.0488 scientist:0.0649,:0.1006 and:0.1816)
		(,:0.2041 and:0.0776�:0.1318 not:0.0776 fan:0.0488 scientist:0.0649,:0.1006 and:0.1816)
 and
------
		( Hermione:0.0011>.:0.0024asses:0.0019greSQL:0.0012 and:0.0171 and:0.0029")):0.0013")):0.0015)
		( and:0.0020cing:0.0030 mentioned:0.0014 noticeable:0.0011 and:0.0137 and:0.0050 and:0.0023 is:0.0011)
		( and:0.0124cing:0.0057 not:0.0030 awesome:0.0014 and:0.0170 and:0.0101 and:0.0189 is:0.0046)
		( and:0.0247 will:0.0064 not:0.0059 my:0.0035 and:0.0161 and:0.0134 and:0.0469 the:0.0118)
		( and:0.0210 will:0.0107 not:0.0098 my:0.0076 and:0.0087 of:0.0146 and:0.0461 the:0.0160)
		( and:0.0133 will:0.0164 sure:0.0182 nice:0.0096 specialist:0.0063 that:0.0162 and:0.0295 I:0.0249)
		( however:0.0120 will:0.0250 sure:0.0415 nice:0.0132 specialist:0.0101er:0.0283 but:0.0444 I:0.0583)
		( but:0.0240 will:0.0359 sure:0.0762 bit:0.0193 scientist:0.0186er:0.0527 but:0.1001 I:0.1484)
		( but:0.0447'm:0.0461 sure:0.1021 bit:0.0277 scientist:0.0371er:0.0767 but:0.1631 I:0.2676)
		( but:0.0544�:0.0566 sure:0.1030 bit:0.0309 scientist:0.0605er:0.0884 but:0.1865 I:0.3633)
		( and:0.0659�:0.0923 not:0.0991 fan:0.0381 scientist:0.0742,:0.0967 but:0.1768 I:0.4102)
		( and:0.0776�:0.1318 not:0.0776 fan:0.0488 scientist:0.0649,:0.1006 and:0.1816 I:0.3652)
		( and:0.0776�:0.1318 not:0.0776 fan:0.0488 scientist:0.0649,:0.1006 and:0.1816 I:0.3652)
 I'm a teacher.
I'm a teacher.
