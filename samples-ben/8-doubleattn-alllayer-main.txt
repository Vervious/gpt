1: sample 0: Hello, I'm a language model,
------
		( lit:0.0002 bless:0.0002 Raw:0.0001Champ:0.0002Champ:0.0002 benchmark:0.0002 deviations:0.0002 HMS:0.0002)
		( Bulls:0.0002hews:0.0002 bless:0.0002 bless:0.0002 bless:0.0002 bless:0.0002 creators:0.0002 bless:0.0002)
		( recip:0.0002 drinkers:0.0002 Say:0.0002 Say:0.0002 Say:0.0002 Say:0.0002 Say:0.0002kai:0.0002)
		(change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002)
		( meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002)
		( barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002)
		( dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0002 dw:0.0002)
		( posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002)
		( Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002)
		( quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002)
		( dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002)
		(Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002)
		(Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002)
Political
------
		( bless:0.0002 Raw:0.0001Champ:0.0002Champ:0.0002 benchmark:0.0002 deviations:0.0002 HMS:0.0002 HMS:0.0001)
		(hews:0.0002 bless:0.0002 bless:0.0002 bless:0.0002 bless:0.0002 creators:0.0002 bless:0.0002 bless:0.0002)
		( drinkers:0.0002 Say:0.0002 Say:0.0002 Say:0.0002 Say:0.0002 Say:0.0002kai:0.0002kai:0.0002)
		(change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002change:0.0002)
		( meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0002 meticulous:0.0001)
		( barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002 barred:0.0002)
		( dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0001 dw:0.0002 dw:0.0002 dw:0.0002)
		( posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002 posted:0.0002)
		( Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002 Mercy:0.0002)
		( quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002 quartz:0.0002)
		( dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002 dishes:0.0002)
		(Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002)
		(Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002Political:0.0002)
PoliticalPoliticalPoliticalPoliticalPoliticalPoliticalPoliticalPoliticalPoliticalPoliticalPolitical
50: sample 0: Hello, I'm a language model,
------
		(,:0.0454 the:0.0508 the:0.0544,:0.0364 the:0.0532 the:0.0420,:0.0403 the:0.0488)
		( the:0.0425 the:0.0408 the:0.0403 the:0.0435 the:0.0413 the:0.0452 the:0.0454 the:0.0427)
		( the:0.0500 the:0.0493 the:0.0483 the:0.0483 the:0.0486 the:0.0488 the:0.0486 the:0.0500)
		( the:0.0452 the:0.0461 the:0.0459 the:0.0461 the:0.0459 the:0.0459 the:0.0459 the:0.0459)
		( the:0.0481 the:0.0486 the:0.0474 the:0.0483 the:0.0474 the:0.0486 the:0.0486 the:0.0474)
		( the:0.0469 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466)
		( the:0.0474 the:0.0474 the:0.0476 the:0.0474 the:0.0476 the:0.0474 the:0.0474 the:0.0476)
		( the:0.0466 the:0.0464 the:0.0464 the:0.0466 the:0.0464 the:0.0466 the:0.0466 the:0.0466)
		( the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476)
		( the:0.0464 the:0.0464 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464 the:0.0479)
		( the:0.0479 the:0.0479 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0479)
		( the:0.0464 the:0.0479 the:0.0479 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464)
		( the:0.0464 the:0.0479 the:0.0479 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464)
 the
------
		( the:0.0508 the:0.0544,:0.0364 the:0.0532 the:0.0420,:0.0403 the:0.0488 the:0.0540)
		( the:0.0408 the:0.0403 the:0.0435 the:0.0413 the:0.0452 the:0.0454 the:0.0427 the:0.0425)
		( the:0.0493 the:0.0483 the:0.0483 the:0.0486 the:0.0488 the:0.0486 the:0.0500 the:0.0488)
		( the:0.0461 the:0.0459 the:0.0461 the:0.0459 the:0.0459 the:0.0459 the:0.0459 the:0.0457)
		( the:0.0486 the:0.0474 the:0.0483 the:0.0474 the:0.0486 the:0.0486 the:0.0474 the:0.0474)
		( the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466 the:0.0466)
		( the:0.0474 the:0.0476 the:0.0474 the:0.0476 the:0.0474 the:0.0474 the:0.0476 the:0.0476)
		( the:0.0464 the:0.0464 the:0.0466 the:0.0464 the:0.0466 the:0.0466 the:0.0466 the:0.0464)
		( the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476)
		( the:0.0464 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464 the:0.0479 the:0.0464)
		( the:0.0479 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0476 the:0.0479 the:0.0479)
		( the:0.0479 the:0.0479 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464 the:0.0479)
		( the:0.0479 the:0.0479 the:0.0464 the:0.0479 the:0.0479 the:0.0479 the:0.0464 the:0.0479)
 the the the the the the the the the the the
200: sample 0: Hello, I'm a language model,
------
		(,:0.1201 and:0.1338
:0.0342 a:0.0420 the:0.0113.:0.1030 of:0.1641 and:0.1235)
		(,:0.1016 and:0.1201 have:0.0187 been:0.0525 the:0.0156.:0.1206 of:0.1924 and:0.1235)
		(,:0.0845 and:0.1025 have:0.0176 been:0.0613 the:0.0190.:0.1191 of:0.2031 and:0.1309)
		(,:0.0703 and:0.0840 have:0.0162 been:0.0591 the:0.0220.:0.1226 of:0.1904 and:0.1416)
		(,:0.0649 and:0.0688 have:0.0149 been:0.0542 the:0.0255.:0.1221 of:0.1865 and:0.1436)
		(,:0.0593 and:0.0574 have:0.0139 a:0.0488 the:0.0295.:0.1270 of:0.1816 and:0.1465)
		(,:0.0571 and:0.0503-:0.0145 a:0.0576 the:0.0322.:0.1260 of:0.1875 and:0.1514)
		(,:0.0542 and:0.0464-:0.0161 a:0.0635 the:0.0337.:0.1240 of:0.1855 and:0.1592)
		(,:0.0522 and:0.0452-:0.0161 a:0.0625 the:0.0339.:0.1221 of:0.1855 and:0.1533)
		(,:0.0525 and:0.0464-:0.0154 the:0.0625 the:0.0337.:0.1216 of:0.1885 and:0.1582)
		(,:0.0554 and:0.0461-:0.0139 the:0.0608 the:0.0325.:0.1152 of:0.1836 and:0.1553)
		(,:0.0569 and:0.0457 is:0.0129 the:0.0562 the:0.0300.:0.1089 of:0.1875 and:0.1523)
		(,:0.0569 and:0.0457 is:0.0129 the:0.0562 the:0.0300.:0.1089 of:0.1875 and:0.1523)
 and
------
		( and:0.1338
:0.0342 a:0.0420 the:0.0113.:0.1030 of:0.1641 and:0.1235 the:0.0413)
		( and:0.1201 have:0.0187 been:0.0525 the:0.0156.:0.1206 of:0.1924 and:0.1235 the:0.0354)
		( and:0.1025 have:0.0176 been:0.0613 the:0.0190.:0.1191 of:0.2031 and:0.1309 the:0.0352)
		( and:0.0840 have:0.0162 been:0.0591 the:0.0220.:0.1226 of:0.1904 and:0.1416 the:0.0359)
		( and:0.0688 have:0.0149 been:0.0542 the:0.0255.:0.1221 of:0.1865 and:0.1436 the:0.0364)
		( and:0.0574 have:0.0139 a:0.0488 the:0.0295.:0.1270 of:0.1816 and:0.1465 the:0.0381)
		( and:0.0503-:0.0145 a:0.0576 the:0.0322.:0.1260 of:0.1875 and:0.1514 the:0.0386)
		( and:0.0464-:0.0161 a:0.0635 the:0.0337.:0.1240 of:0.1855 and:0.1592 the:0.0378)
		( and:0.0452-:0.0161 a:0.0625 the:0.0339.:0.1221 of:0.1855 and:0.1533 the:0.0374)
		( and:0.0464-:0.0154 the:0.0625 the:0.0337.:0.1216 of:0.1885 and:0.1582 the:0.0349)
		( and:0.0461-:0.0139 the:0.0608 the:0.0325.:0.1152 of:0.1836 and:0.1553 the:0.0337)
		( and:0.0457 is:0.0129 the:0.0562 the:0.0300.:0.1089 of:0.1875 and:0.1523 the:0.0317)
		( and:0.0457 is:0.0129 the:0.0562 the:0.0300.:0.1089 of:0.1875 and:0.1523 the:0.0317)
 the other of the other of the other of the other
350: sample 0: Hello, I'm a language model,
------
		(,:0.0830 and:0.2139 have:0.0315 a:0.0674 very:0.0159.:0.1484.:0.1309 and:0.1001)
		(,:0.0679 and:0.1670,:0.0317 a:0.0776 very:0.0245.:0.1494.:0.1387 and:0.0908)
		(,:0.0830 and:0.1680,:0.0320 a:0.0767 very:0.0254.:0.1680.:0.1602 and:0.0923)
		(,:0.0713 and:0.1621,:0.0304 a:0.0737 very:0.0240.:0.1611.:0.1543 and:0.0903)
		(,:0.0537 and:0.1611,:0.0292 a:0.0747 very:0.0236.:0.1533.:0.1553 and:0.0938)
		(,:0.0461 and:0.1602,:0.0281 a:0.0820 very:0.0233.:0.1504.:0.1445 and:0.0913)
		(,:0.0525 and:0.1670,:0.0299 a:0.0898 very:0.0214.:0.1484.:0.1406 and:0.0933)
		(,:0.0649 and:0.1650,:0.0309 a:0.0938 very:0.0194,:0.1416.:0.1416 and:0.0972)
		(,:0.0801 and:0.1641 a:0.0337 a:0.0942 very:0.0182,:0.1455.:0.1445 and:0.1030)
		(,:0.0908 and:0.1543 a:0.0366 a:0.0908 very:0.0172,:0.1465.:0.1436 and:0.1108)
		(,:0.1011 and:0.1455 a:0.0381 a:0.0898 very:0.0166,:0.1475.:0.1445 and:0.1147)
		(,:0.1104 and:0.1338 a:0.0383 a:0.0845 very:0.0156,:0.1445.:0.1455 and:0.1138)
		(,:0.1104 and:0.1338 a:0.0383 a:0.0845 very:0.0156,:0.1445.:0.1455 and:0.1138)
 and
------
		( and:0.2139 have:0.0315 a:0.0674 very:0.0159.:0.1484.:0.1309 and:0.1001 the:0.0242)
		( and:0.1670,:0.0317 a:0.0776 very:0.0245.:0.1494.:0.1387 and:0.0908 the:0.0398)
		( and:0.1680,:0.0320 a:0.0767 very:0.0254.:0.1680.:0.1602 and:0.0923 the:0.0386)
		( and:0.1621,:0.0304 a:0.0737 very:0.0240.:0.1611.:0.1543 and:0.0903 the:0.0344)
		( and:0.1611,:0.0292 a:0.0747 very:0.0236.:0.1533.:0.1553 and:0.0938 the:0.0309)
		( and:0.1602,:0.0281 a:0.0820 very:0.0233.:0.1504.:0.1445 and:0.0913 the:0.0269)
		( and:0.1670,:0.0299 a:0.0898 very:0.0214.:0.1484.:0.1406 and:0.0933 a:0.0245)
		( and:0.1650,:0.0309 a:0.0938 very:0.0194,:0.1416.:0.1416 and:0.0972 a:0.0261)
		( and:0.1641 a:0.0337 a:0.0942 very:0.0182,:0.1455.:0.1445 and:0.1030 a:0.0273)
		( and:0.1543 a:0.0366 a:0.0908 very:0.0172,:0.1465.:0.1436 and:0.1108 a:0.0277)
		( and:0.1455 a:0.0381 a:0.0898 very:0.0166,:0.1475.:0.1445 and:0.1147 a:0.0286)
		( and:0.1338 a:0.0383 a:0.0845 very:0.0156,:0.1445.:0.1455 and:0.1138 a:0.0287)
		( and:0.1338 a:0.0383 a:0.0845 very:0.0156,:0.1445.:0.1455 and:0.1138 a:0.0287)
 a few of the same time, and the same time
500: sample 0: Hello, I'm a language model,
------
		(,:0.0679 and:0.1465 can:0.0564 not:0.0591 more:0.0121,:0.1758 of:0.1289 and:0.0776)
		(,:0.0605 and:0.1177 do:0.0640 not:0.0645 very:0.0133,:0.1157 of:0.1572 I:0.1621)
		(,:0.0422 and:0.1250 do:0.0923 not:0.0713 very:0.0157.:0.1709.:0.1465 and:0.0815)
		(,:0.0284 and:0.1113 do:0.1123 not:0.0684 very:0.0179.:0.2168.:0.1924 and:0.1055)
		(,:0.0262 and:0.0981 do:0.1250 not:0.0586 very:0.0190.:0.2031.:0.1777 and:0.1001)
		(,:0.0249 and:0.0972 do:0.1377 not:0.0513 very:0.0186.:0.1885.:0.1533 and:0.0972)
		(,:0.0298 and:0.1045 do:0.1514 the:0.0574 very:0.0184.:0.1709.:0.1436 and:0.0972)
		(,:0.0417 and:0.1089 do:0.1553 the:0.0659 very:0.0184,:0.1660.:0.1338 and:0.1001)
		(,:0.0515 and:0.1069 do:0.1494 the:0.0693 very:0.0178,:0.1807,:0.1328 and:0.0947)
		(,:0.0571 and:0.1030 do:0.1416 the:0.0737 very:0.0177,:0.1904,:0.1416 and:0.0913)
		(,:0.0603 and:0.0977 do:0.1348 the:0.0786 very:0.0171,:0.1963,:0.1465 and:0.0894)
		(,:0.0635 and:0.0898 do:0.1240 the:0.0796 very:0.0167,:0.2021,:0.1445 and:0.0874)
		(,:0.0635 and:0.0898 do:0.1240 the:0.0796 very:0.0167,:0.2021,:0.1445 and:0.0874)
 and
------
		( and:0.1465 can:0.0564 not:0.0591 more:0.0121,:0.1758 of:0.1289 and:0.0776 the:0.0320)
		( and:0.1177 do:0.0640 not:0.0645 very:0.0133,:0.1157 of:0.1572 I:0.1621 I:0.0613)
		( and:0.1250 do:0.0923 not:0.0713 very:0.0157.:0.1709.:0.1465 and:0.0815 the:0.0422)
		( and:0.1113 do:0.1123 not:0.0684 very:0.0179.:0.2168.:0.1924 and:0.1055 the:0.0432)
		( and:0.0981 do:0.1250 not:0.0586 very:0.0190.:0.2031.:0.1777 and:0.1001 the:0.0449)
		( and:0.0972 do:0.1377 not:0.0513 very:0.0186.:0.1885.:0.1533 and:0.0972 the:0.0464)
		( and:0.1045 do:0.1514 the:0.0574 very:0.0184.:0.1709.:0.1436 and:0.0972 the:0.0459)
		( and:0.1089 do:0.1553 the:0.0659 very:0.0184,:0.1660.:0.1338 and:0.1001 the:0.0452)
		( and:0.1069 do:0.1494 the:0.0693 very:0.0178,:0.1807,:0.1328 and:0.0947 the:0.0447)
		( and:0.1030 do:0.1416 the:0.0737 very:0.0177,:0.1904,:0.1416 and:0.0913 the:0.0437)
		( and:0.0977 do:0.1348 the:0.0786 very:0.0171,:0.1963,:0.1465 and:0.0894 the:0.0430)
		( and:0.0898 do:0.1240 the:0.0796 very:0.0167,:0.2021,:0.1445 and:0.0874 the:0.0413)
		( and:0.0898 do:0.1240 the:0.0796 very:0.0167,:0.2021,:0.1445 and:0.0874 the:0.0413)
 the same time, and the same time, and the
650: sample 0: Hello, I'm a language model,
------
		(,:0.0737 and:0.1230 would:0.0376.:0.0515 more:0.0092,:0.1475.:0.1348 and:0.0864)
		(,:0.0977 and:0.0869 did:0.0718 a:0.0508 very:0.0124 of:0.1357 of:0.1377 and:0.1089)
		(,:0.0518 and:0.0879 did:0.0923 a:0.0417 very:0.0153.:0.1650.:0.1699 and:0.1416)
		(,:0.0240 and:0.0884 did:0.0850 a:0.0388 few:0.0166.:0.1406.:0.1270 and:0.1582)
		( of:0.0151 and:0.1006 was:0.0762 a:0.0325 few:0.0212.:0.1445?:0.1777 and:0.1689)
		(,:0.0330 and:0.1172 was:0.0781 a:0.0325 lot:0.0261.:0.1367?:0.2021 and:0.1699)
		(,:0.0427 and:0.1279 was:0.0781 a:0.0315 lot:0.0286.:0.1279?:0.2041 and:0.1680)
		(,:0.0459 and:0.1309 was:0.0796 a:0.0309 lot:0.0300.:0.1245?:0.2090 and:0.1670)
		(,:0.0471 and:0.1309 was:0.0820 going:0.0312 lot:0.0315.:0.1206?:0.2031 and:0.1602)
		(,:0.0471 and:0.1318 was:0.0801 going:0.0317 lot:0.0312 of:0.1245?:0.2002 and:0.1631)
		(,:0.0474 and:0.1328 was:0.0830 going:0.0322 lot:0.0322 of:0.1221?:0.1973 and:0.1572)
		(.:0.0476 and:0.1338 was:0.0815 going:0.0327 lot:0.0322 of:0.1191?:0.1826 and:0.1523)
		(.:0.0476 and:0.1338 was:0.0815 going:0.0327 lot:0.0322 of:0.1191?:0.1826 and:0.1523)
 and
------
		( and:0.1230 would:0.0376.:0.0515 more:0.0092,:0.1475.:0.1348 and:0.0864 the:0.0544)
		( and:0.0869 did:0.0718 a:0.0508 very:0.0124 of:0.1357 of:0.1377 and:0.1089 the:0.0525)
		( and:0.0879 did:0.0923 a:0.0417 very:0.0153.:0.1650.:0.1699 and:0.1416 the:0.0574)
		( and:0.0884 did:0.0850 a:0.0388 few:0.0166.:0.1406.:0.1270 and:0.1582 the:0.0635)
		( and:0.1006 was:0.0762 a:0.0325 few:0.0212.:0.1445?:0.1777 and:0.1689 I:0.0771)
		( and:0.1172 was:0.0781 a:0.0325 lot:0.0261.:0.1367?:0.2021 and:0.1699 I:0.0762)
		( and:0.1279 was:0.0781 a:0.0315 lot:0.0286.:0.1279?:0.2041 and:0.1680 I:0.0786)
		( and:0.1309 was:0.0796 a:0.0309 lot:0.0300.:0.1245?:0.2090 and:0.1670 I:0.0786)
		( and:0.1309 was:0.0820 going:0.0312 lot:0.0315.:0.1206?:0.2031 and:0.1602 I:0.0747)
		( and:0.1318 was:0.0801 going:0.0317 lot:0.0312 of:0.1245?:0.2002 and:0.1631 the:0.0713)
		( and:0.1328 was:0.0830 going:0.0322 lot:0.0322 of:0.1221?:0.1973 and:0.1572 the:0.0728)
		( and:0.1338 was:0.0815 going:0.0327 lot:0.0322 of:0.1191?:0.1826 and:0.1523 the:0.0698)
		( and:0.1338 was:0.0815 going:0.0327 lot:0.0322 of:0.1191?:0.1826 and:0.1523 the:0.0698)
 the same way to be the same way of the same
800: sample 0: Hello, I'm a language model,
------
		(,:0.0752 and:0.0679 can:0.0466 not:0.0479 very:0.0093,:0.1357.:0.1338 and:0.0659)
		(,:0.0674 and:0.0610 was:0.0583 not:0.0781 very:0.0168.:0.1099.:0.1221 and:0.0786)
		(,:0.0383 and:0.0593 was:0.0698 not:0.0801 very:0.0216.:0.1553.:0.1729 but:0.1055)
		(,:0.0216 and:0.0649 was:0.0703 not:0.0674 good:0.0179.:0.1641.:0.1777 and:0.1157)
		( of:0.0168 and:0.0615 am:0.0864 not:0.0537 lot:0.0292.:0.1738.:0.1797 but:0.1309)
		(,:0.0356 and:0.0669 am:0.0820 not:0.0508 lot:0.0352.:0.1660.:0.1709 but:0.1416)
		(,:0.0398 and:0.0688 am:0.0771 not:0.0505 lot:0.0383.:0.1699.:0.1768 but:0.1426)
		(,:0.0400 and:0.0693 was:0.0693 not:0.0491 lot:0.0396.:0.1758.:0.1826 but:0.1436)
		(,:0.0403 and:0.0688 was:0.0718 not:0.0476 lot:0.0400.:0.1846.:0.1914 but:0.1455)
		(,:0.0381 and:0.0698 was:0.0737 not:0.0471 lot:0.0415.:0.1924.:0.1992 but:0.1475)
		(,:0.0361 and:0.0693 was:0.0718 not:0.0469 lot:0.0417.:0.2109.:0.1992 but:0.1416)
		(,:0.0342 and:0.0688 was:0.0747 not:0.0452 lot:0.0420.:0.2207.:0.2090 but:0.1436)
		(,:0.0342 and:0.0688 was:0.0747 not:0.0452 lot:0.0420.:0.2207.:0.2090 but:0.1436)
 but
------
		( and:0.0679 can:0.0466 not:0.0479 very:0.0093,:0.1357.:0.1338 and:0.0659 it:0.0664)
		( and:0.0610 was:0.0583 not:0.0781 very:0.0168.:0.1099.:0.1221 and:0.0786 I:0.0972)
		( and:0.0593 was:0.0698 not:0.0801 very:0.0216.:0.1553.:0.1729 but:0.1055 I:0.1523)
		( and:0.0649 was:0.0703 not:0.0674 good:0.0179.:0.1641.:0.1777 and:0.1157 I:0.2090)
		( and:0.0615 am:0.0864 not:0.0537 lot:0.0292.:0.1738.:0.1797 but:0.1309 I:0.2217)
		( and:0.0669 am:0.0820 not:0.0508 lot:0.0352.:0.1660.:0.1709 but:0.1416 I:0.2227)
		( and:0.0688 am:0.0771 not:0.0505 lot:0.0383.:0.1699.:0.1768 but:0.1426 I:0.2334)
		( and:0.0693 was:0.0693 not:0.0491 lot:0.0396.:0.1758.:0.1826 but:0.1436 I:0.2266)
		( and:0.0688 was:0.0718 not:0.0476 lot:0.0400.:0.1846.:0.1914 but:0.1455 I:0.2285)
		( and:0.0698 was:0.0737 not:0.0471 lot:0.0415.:0.1924.:0.1992 but:0.1475 I:0.2236)
		( and:0.0693 was:0.0718 not:0.0469 lot:0.0417.:0.2109.:0.1992 but:0.1416 I:0.2148)
		( and:0.0688 was:0.0747 not:0.0452 lot:0.0420.:0.2207.:0.2090 but:0.1436 I:0.2109)
		( and:0.0688 was:0.0747 not:0.0452 lot:0.0420.:0.2207.:0.2090 but:0.1436 I:0.2109)
 I have a lot of the same time.
I
950: sample 0: Hello, I'm a language model,
------
		(,:0.0630 and:0.0635 was:0.0420 not:0.0347 very:0.0125,:0.1670,:0.1562 and:0.0918)
		(,:0.0654 and:0.0771 was:0.0491 not:0.0811 very:0.0184,:0.1689,:0.1553 and:0.2021)
		(,:0.0442 and:0.1011 was:0.0728 not:0.1187 very:0.0171.:0.1406.:0.1758 and:0.1982)
		( of:0.0286 and:0.1318 was:0.0869 not:0.1582 great:0.0220.:0.1289.:0.1738 and:0.1914)
		( of:0.0215 and:0.1338 was:0.0791 not:0.1719 great:0.0244.:0.1494.:0.2031 and:0.1611)
		(,:0.0344 and:0.1436 was:0.0791 not:0.1729 lot:0.0258.:0.1475.:0.2061 and:0.1602)
		(,:0.0405 and:0.1455 was:0.0752 not:0.1836 lot:0.0281.:0.1455.:0.1973 and:0.1611)
		(,:0.0432 and:0.1455 was:0.0771 not:0.1855 lot:0.0300.:0.1523.:0.2012 and:0.1543)
		(,:0.0447 and:0.1465 was:0.0737 not:0.1875 lot:0.0322.:0.1523.:0.1963 and:0.1553)
		(,:0.0437 and:0.1475 was:0.0713 not:0.1885 lot:0.0334.:0.1611.:0.1904 and:0.1572)
		(,:0.0439 and:0.1445 was:0.0693 not:0.1807 lot:0.0359.:0.1621.:0.1953 and:0.1592)
		(,:0.0427 and:0.1426 was:0.0669 not:0.1826 lot:0.0374.:0.1621.:0.1826 and:0.1533)
		(,:0.0427 and:0.1426 was:0.0669 not:0.1826 lot:0.0374.:0.1621.:0.1826 and:0.1533)
 and
------
		( and:0.0635 was:0.0420 not:0.0347 very:0.0125,:0.1670,:0.1562 and:0.0918 the:0.0442)
		( and:0.0771 was:0.0491 not:0.0811 very:0.0184,:0.1689,:0.1553 and:0.2021 the:0.0347)
		( and:0.1011 was:0.0728 not:0.1187 very:0.0171.:0.1406.:0.1758 and:0.1982 I:0.0410)
		( and:0.1318 was:0.0869 not:0.1582 great:0.0220.:0.1289.:0.1738 and:0.1914 I:0.0603)
		( and:0.1338 was:0.0791 not:0.1719 great:0.0244.:0.1494.:0.2031 and:0.1611 I:0.0552)
		( and:0.1436 was:0.0791 not:0.1729 lot:0.0258.:0.1475.:0.2061 and:0.1602 I:0.0564)
		( and:0.1455 was:0.0752 not:0.1836 lot:0.0281.:0.1455.:0.1973 and:0.1611 I:0.0603)
		( and:0.1455 was:0.0771 not:0.1855 lot:0.0300.:0.1523.:0.2012 and:0.1543 I:0.0615)
		( and:0.1465 was:0.0737 not:0.1875 lot:0.0322.:0.1523.:0.1963 and:0.1553 I:0.0630)
		( and:0.1475 was:0.0713 not:0.1885 lot:0.0334.:0.1611.:0.1904 and:0.1572 I:0.0645)
		( and:0.1445 was:0.0693 not:0.1807 lot:0.0359.:0.1621.:0.1953 and:0.1592 I:0.0659)
		( and:0.1426 was:0.0669 not:0.1826 lot:0.0374.:0.1621.:0.1826 and:0.1533 I:0.0659)
		( and:0.1426 was:0.0669 not:0.1826 lot:0.0374.:0.1621.:0.1826 and:0.1533 I:0.0659)
 I think of the language.
I think of the
1100: sample 0: Hello, I'm a language model,
------
		(,:0.0427 and:0.0322 was:0.0527 not:0.0461 way:0.0086,:0.1074,:0.1484 and:0.0679)
		(,:0.0535 and:0.0369 was:0.0471 not:0.0898 lot:0.0119,:0.1191,:0.1543 and:0.0811)
		(,:0.0571 and:0.0635 was:0.0801 not:0.0972 great:0.0090 of:0.1260 of:0.1494 but:0.0806)
		( of:0.0273 the:0.0623 was:0.0854 not:0.1650 lot:0.0151 that:0.1069 of:0.1045 but:0.1436)
		( of:0.0240 the:0.0767 was:0.0942 not:0.1729 lot:0.0245 that:0.0967.:0.1011 but:0.1514)
		(,:0.0417 the:0.0801 was:0.0928 not:0.1699 lot:0.0249 that:0.0957.:0.0991 but:0.1572)
		(,:0.0437 the:0.0806 was:0.0933 not:0.1709 lot:0.0249 that:0.0933 for:0.1016 but:0.1572)
		(,:0.0420 the:0.0796 was:0.0942 not:0.1729 lot:0.0250 that:0.0918 for:0.1069 but:0.1582)
		(,:0.0381 the:0.0791 was:0.0903 not:0.1748 lot:0.0253 that:0.0898 for:0.1128 but:0.1592)
		(,:0.0352 the:0.0781 was:0.0913 not:0.1758 lot:0.0248 that:0.0894 for:0.1152 but:0.1523)
		(,:0.0315 the:0.0771 was:0.0874 not:0.1777 lot:0.0242 that:0.0879 for:0.1182 but:0.1543)
		(,:0.0289 the:0.0791 was:0.0889 not:0.1709 lot:0.0237 that:0.0874 for:0.1211 but:0.1562)
		(,:0.0289 the:0.0791 was:0.0889 not:0.1709 lot:0.0237 that:0.0874 for:0.1211 but:0.1562)
 but
------
		( and:0.0322 was:0.0527 not:0.0461 way:0.0086,:0.1074,:0.1484 and:0.0679 the:0.0452)
		( and:0.0369 was:0.0471 not:0.0898 lot:0.0119,:0.1191,:0.1543 and:0.0811 it:0.0625)
		( and:0.0635 was:0.0801 not:0.0972 great:0.0090 of:0.1260 of:0.1494 but:0.0806 I:0.2256)
		( the:0.0623 was:0.0854 not:0.1650 lot:0.0151 that:0.1069 of:0.1045 but:0.1436 I:0.3262)
		( the:0.0767 was:0.0942 not:0.1729 lot:0.0245 that:0.0967.:0.1011 but:0.1514 I:0.3125)
		( the:0.0801 was:0.0928 not:0.1699 lot:0.0249 that:0.0957.:0.0991 but:0.1572 I:0.3125)
		( the:0.0806 was:0.0933 not:0.1709 lot:0.0249 that:0.0933 for:0.1016 but:0.1572 I:0.3105)
		( the:0.0796 was:0.0942 not:0.1729 lot:0.0250 that:0.0918 for:0.1069 but:0.1582 I:0.3105)
		( the:0.0791 was:0.0903 not:0.1748 lot:0.0253 that:0.0898 for:0.1128 but:0.1592 I:0.2988)
		( the:0.0781 was:0.0913 not:0.1758 lot:0.0248 that:0.0894 for:0.1152 but:0.1523 I:0.2852)
		( the:0.0771 was:0.0874 not:0.1777 lot:0.0242 that:0.0879 for:0.1182 but:0.1543 I:0.2891)
		( the:0.0791 was:0.0889 not:0.1709 lot:0.0237 that:0.0874 for:0.1211 but:0.1562 I:0.2754)
		( the:0.0791 was:0.0889 not:0.1709 lot:0.0237 that:0.0874 for:0.1211 but:0.1562 I:0.2754)
 I am a lot of my own. I am a
