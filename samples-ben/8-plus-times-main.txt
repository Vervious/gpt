1: sample 0: Hello, I'm a language model,
------
		( mu:0.0001static:0.0001initial:0.0001 afar:0.0001activity:0.0001tion:0.0001 boosting:0.0001 wrench:0.0001)
		( invaded:0.0001Sum:0.0001 Dest:0.0001 disposal:0.0001 synaptic:0.0001money:0.0001 opacity:0.0001 opacity:0.0001)
		(iggs:0.0001 inverted:0.0001 inverted:0.0001rium:0.0001rium:0.0001rium:0.0001rium:0.0001rium:0.0001)
		( lobster:0.0001 lobster:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001)
		(rophe:0.0001rophe:0.0001rophe:0.0001rophe:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001)
		( Timberwolves:0.0001 bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 UNDER:0.0001 UNDER:0.0001)
		( Mas:0.0001 marketers:0.0001 marketers:0.0001 marketers:0.0001 BO:0.0001 BO:0.0001 BO:0.0001 BO:0.0001)
		( clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001)
		( Lana:0.0001 Lana:0.0001 Lana:0.0001 Lana:0.0001 Lana:0.0001 Thu:0.0001 Thu:0.0001 Thu:0.0001)
		( constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001)
		( leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001)
		( compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001)
		( compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001)
 compulsion
------
		(static:0.0001initial:0.0001 afar:0.0001activity:0.0001tion:0.0001 boosting:0.0001 wrench:0.0001 Rend:0.0001)
		(Sum:0.0001 Dest:0.0001 disposal:0.0001 synaptic:0.0001money:0.0001 opacity:0.0001 opacity:0.0001 Brees:0.0001)
		( inverted:0.0001 inverted:0.0001rium:0.0001rium:0.0001rium:0.0001rium:0.0001rium:0.0001 Benz:0.0001)
		( lobster:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001 vision:0.0001)
		(rophe:0.0001rophe:0.0001rophe:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001 vulnerabilities:0.0001)
		( bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 bipolar:0.0001 UNDER:0.0001 UNDER:0.0001 advoc:0.0001)
		( marketers:0.0001 marketers:0.0001 marketers:0.0001 BO:0.0001 BO:0.0001 BO:0.0001 BO:0.0001 BO:0.0001)
		( clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001 clar:0.0001)
		( Lana:0.0001 Lana:0.0001 Lana:0.0001 Lana:0.0001 Thu:0.0001 Thu:0.0001 Thu:0.0001 Thu:0.0001)
		( constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001 constitu:0.0001)
		( leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001 leth:0.0001)
		( compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001)
		( compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001 compulsion:0.0001)
 compulsion compulsion compulsion compulsion Darkness Darkness Darkness Darkness Darkness Darkness Darkness
50: sample 0: Hello, I'm a language model,
------
		(.:0.0253.:0.0410.:0.0386.:0.0398.:0.0388.:0.0388.:0.0388.:0.0403)
		(.:0.0427
:0.0061.:0.0479.:0.0481.:0.0493.:0.0493.:0.0496.:0.0481)
		(.:0.0520.:0.0317.:0.0444.:0.0513.:0.0510.:0.0496.:0.0508.:0.0464)
		(.:0.0457.:0.0359.:0.0518.:0.0486.:0.0498.:0.0525.:0.0503.:0.0537)
		(.:0.0535.:0.0270.:0.0454.:0.0491.:0.0496.:0.0508.:0.0483.:0.0449)
		(.:0.0449.:0.0210.:0.0547.:0.0466.:0.0488.:0.0520.:0.0466.:0.0537)
		(.:0.0530.:0.0145.:0.0432.:0.0415.:0.0476.:0.0520.:0.0420.:0.0422)
		(.:0.0408 the:0.0059.:0.0510.:0.0376.:0.0437.:0.0510.:0.0378.:0.0459)
		(.:0.0486 to:0.0047.:0.0359.:0.0211.:0.0359.:0.0522.:0.0234.:0.0325)
		(.:0.0344 the:0.0014.:0.0422.:0.0225.:0.0310.:0.0486.:0.0212.:0.0236)
		(.:0.0366 to:0.0011.:0.0190 the:0.0051 the:0.0135.:0.0527 the:0.0061.:0.0154)
		(.:0.0248 the:0.0005 the:0.0203 to:0.0061.:0.0107.:0.0454 to:0.0049 the:0.0036)
		(.:0.0248 the:0.0005 the:0.0203 to:0.0061.:0.0107.:0.0454 to:0.0049 the:0.0036)
 the
------
		(.:0.0410.:0.0386.:0.0398.:0.0388.:0.0388.:0.0388.:0.0403.:0.0398)
		(
:0.0061.:0.0479.:0.0481.:0.0493.:0.0493.:0.0496.:0.0481.:0.0432)
		(.:0.0317.:0.0444.:0.0513.:0.0510.:0.0496.:0.0508.:0.0464.:0.0320)
		(.:0.0359.:0.0518.:0.0486.:0.0498.:0.0525.:0.0503.:0.0537.:0.0391)
		(.:0.0270.:0.0454.:0.0491.:0.0496.:0.0508.:0.0483.:0.0449.:0.0199)
		(.:0.0210.:0.0547.:0.0466.:0.0488.:0.0520.:0.0466.:0.0537.:0.0294)
		(.:0.0145.:0.0432.:0.0415.:0.0476.:0.0520.:0.0420.:0.0422 the:0.0096)
		( the:0.0059.:0.0510.:0.0376.:0.0437.:0.0510.:0.0378.:0.0459.:0.0150)
		( to:0.0047.:0.0359.:0.0211.:0.0359.:0.0522.:0.0234.:0.0325 the:0.0028)
		( the:0.0014.:0.0422.:0.0225.:0.0310.:0.0486.:0.0212.:0.0236 to:0.0051)
		( to:0.0011.:0.0190 the:0.0051 the:0.0135.:0.0527 the:0.0061.:0.0154 the:0.0008)
		( the:0.0005 the:0.0203 to:0.0061.:0.0107.:0.0454 to:0.0049 the:0.0036 to:0.0013)
		( the:0.0005 the:0.0203 to:0.0061.:0.0107.:0.0454 to:0.0049 the:0.0036 to:0.0013)
 to the to the to.- to the to the
200: sample 0: Hello, I'm a language model,
------
		(
:0.0430 the:0.0564 the:0.0674 the:0.1543 the:0.0189,:0.0669,:0.0747 and:0.0654)
		(
:0.1357 the:0.0522 the:0.0684 the:0.1338 the:0.0183,:0.0864,:0.0986 the:0.0811)
		(
:0.1108 the:0.0579 and:0.0635 the:0.1016 the:0.0117,:0.1001,:0.1138 the:0.0771)
		(,:0.0986,:0.0698,:0.0718 the:0.0859 the:0.0118.:0.1162.:0.1260 the:0.0781)
		(.:0.1040,:0.0723,:0.0737 the:0.0728 the:0.0082.:0.1260.:0.1387 the:0.0645)
		(.:0.1113,:0.0752,:0.0903 the:0.0732 the:0.0088.:0.1406.:0.1475 the:0.0645)
		(.:0.1079,:0.0776,:0.0908 the:0.0713 other:0.0068.:0.1484.:0.1562 the:0.0544)
		(.:0.1187,:0.0781,:0.1021 the:0.0679 other:0.0074.:0.1484.:0.1592 the:0.0505)
		(.:0.1108,:0.0767,:0.1040 the:0.0659 other:0.0063.:0.1504.:0.1582
:0.0571)
		(.:0.1240,:0.0771,:0.1143 the:0.0623 other:0.0072.:0.1543.:0.1641
:0.0645)
		(.:0.1143,:0.0767.:0.1147 the:0.0583 other:0.0064.:0.1553.:0.1543
:0.0830)
		(.:0.1245,:0.0771.:0.1226 the:0.0530 other:0.0070.:0.1602.:0.1611
:0.0894)
		(.:0.1245,:0.0771.:0.1226 the:0.0530 other:0.0070.:0.1602.:0.1611
:0.0894)


------
		( the:0.0564 the:0.0674 the:0.1543 the:0.0189,:0.0669,:0.0747 and:0.0654-:0.1387)
		( the:0.0522 the:0.0684 the:0.1338 the:0.0183,:0.0864,:0.0986 the:0.0811-:0.2363)
		( the:0.0579 and:0.0635 the:0.1016 the:0.0117,:0.1001,:0.1138 the:0.0771-:0.1934)
		(,:0.0698,:0.0718 the:0.0859 the:0.0118.:0.1162.:0.1260 the:0.0781-:0.1914)
		(,:0.0723,:0.0737 the:0.0728 the:0.0082.:0.1260.:0.1387 the:0.0645-:0.1650)
		(,:0.0752,:0.0903 the:0.0732 the:0.0088.:0.1406.:0.1475 the:0.0645-:0.1533)
		(,:0.0776,:0.0908 the:0.0713 other:0.0068.:0.1484.:0.1562 the:0.0544-:0.1484)
		(,:0.0781,:0.1021 the:0.0679 other:0.0074.:0.1484.:0.1592 the:0.0505-:0.1426)
		(,:0.0767,:0.1040 the:0.0659 other:0.0063.:0.1504.:0.1582
:0.0571-:0.1299)
		(,:0.0771,:0.1143 the:0.0623 other:0.0072.:0.1543.:0.1641
:0.0645-:0.1260)
		(,:0.0767.:0.1147 the:0.0583 other:0.0064.:0.1553.:0.1543
:0.0830-:0.1152)
		(,:0.0771.:0.1226 the:0.0530 other:0.0070.:0.1602.:0.1611
:0.0894-:0.1064)
		(,:0.0771.:0.1226 the:0.0530 other:0.0070.:0.1602.:0.1611
:0.0894-:0.1064)
-The other, and the other, and the the
350: sample 0: Hello, I'm a language model,
------
		(.:0.0464,:0.0703 as:0.0913 the:0.0508 lot:0.0111.:0.1523.:0.1279 the:0.0767)
		(,:0.0581 the:0.1094 to:0.0581 a:0.0767 few:0.0124.:0.1553,:0.1494 and:0.0762)
		(.:0.0820 the:0.0967 to:0.0447 a:0.0498 lot:0.0143,:0.1406,:0.1240 and:0.0679)
		(,:0.0806 and:0.0952 to:0.0449 not:0.0481 few:0.0154,:0.1562,:0.1504 and:0.0757)
		(.:0.1045 and:0.0752 to:0.0386 not:0.0427 few:0.0167,:0.1426,:0.1299 and:0.0752)
		(,:0.1143 and:0.0649 to:0.0334 not:0.0479 few:0.0167,:0.1611,:0.1436 and:0.0815)
		(,:0.1270 the:0.0605 to:0.0273 not:0.0420 few:0.0171,:0.1475,:0.1289 and:0.0781)
		(,:0.1367 the:0.0554,:0.0270 not:0.0457 few:0.0170,:0.1670,:0.1377 and:0.0835)
		(,:0.1377 the:0.0522,:0.0260 not:0.0396 few:0.0168,:0.1523,:0.1318 and:0.0791)
		(,:0.1387 the:0.0471,:0.0276 not:0.0398 few:0.0166,:0.1719,:0.1367 and:0.0864)
		(,:0.1289 the:0.0435,:0.0262 a:0.0398 few:0.0160,:0.1582,:0.1367 and:0.0801)
		(,:0.1196 the:0.0386 used:0.0284 a:0.0337 good:0.0157,:0.1758,:0.1348 and:0.0850)
		(,:0.1196 the:0.0386 used:0.0284 a:0.0337 good:0.0157,:0.1758,:0.1348 and:0.0850)
 and
------
		(,:0.0703 as:0.0913 the:0.0508 lot:0.0111.:0.1523.:0.1279 the:0.0767 the:0.0640)
		( the:0.1094 to:0.0581 a:0.0767 few:0.0124.:0.1553,:0.1494 and:0.0762 the:0.0635)
		( the:0.0967 to:0.0447 a:0.0498 lot:0.0143,:0.1406,:0.1240 and:0.0679 the:0.0469)
		( and:0.0952 to:0.0449 not:0.0481 few:0.0154,:0.1562,:0.1504 and:0.0757 the:0.0479)
		( and:0.0752 to:0.0386 not:0.0427 few:0.0167,:0.1426,:0.1299 and:0.0752 the:0.0388)
		( and:0.0649 to:0.0334 not:0.0479 few:0.0167,:0.1611,:0.1436 and:0.0815 the:0.0466)
		( the:0.0605 to:0.0273 not:0.0420 few:0.0171,:0.1475,:0.1289 and:0.0781 the:0.0354)
		( the:0.0554,:0.0270 not:0.0457 few:0.0170,:0.1670,:0.1377 and:0.0835 the:0.0435)
		( the:0.0522,:0.0260 not:0.0396 few:0.0168,:0.1523,:0.1318 and:0.0791 the:0.0354)
		( the:0.0471,:0.0276 not:0.0398 few:0.0166,:0.1719,:0.1367 and:0.0864 the:0.0422)
		( the:0.0435,:0.0262 a:0.0398 few:0.0160,:0.1582,:0.1367 and:0.0801 the:0.0347)
		( the:0.0386 used:0.0284 a:0.0337 good:0.0157,:0.1758,:0.1348 and:0.0850 the:0.0391)
		( the:0.0386 used:0.0284 a:0.0337 good:0.0157,:0.1758,:0.1348 and:0.0850 the:0.0391)
 the same, and the same, and the same,
500: sample 0: Hello, I'm a language model,
------
		(.:0.1553,:0.2031,:0.0737,:0.0157 new:0.0145.:0.2119.:0.1768 and:0.1021)
		(.:0.0801,:0.1113,:0.0481,:0.0386 few:0.0260.:0.1436.:0.1357 the:0.1133)
		(.:0.0811,:0.0933 also:0.0449.:0.0227 few:0.0248 of:0.1050.:0.1040 the:0.0986)
		(.:0.0640,:0.0679 also:0.0461.:0.0248 few:0.0242 of:0.0913.:0.0977 the:0.0835)
		(.:0.0718,:0.0527 also:0.0437.:0.0238 few:0.0194 of:0.0874.:0.0977 the:0.0869)
		(.:0.0618,:0.0439 also:0.0410.:0.0262 few:0.0193 of:0.0889.:0.1035 the:0.0708)
		(.:0.0630
:0.0403 also:0.0347.:0.0254 new:0.0183.:0.0884.:0.1094 the:0.0723)
		(,:0.0481
:0.0354 said:0.0334 not:0.0256 new:0.0184.:0.0942.:0.1094 and:0.0688)
		(,:0.0427
:0.0386 said:0.0349 not:0.0270 new:0.0181.:0.0947.:0.1133 and:0.0693)
		(,:0.0325
:0.0322 was:0.0405 not:0.0286 new:0.0179.:0.0991.:0.1108 and:0.0698)
		(,:0.0231
:0.0330 was:0.0476 not:0.0286 new:0.0177.:0.1025.:0.1138 and:0.0742)
		(,:0.0164
:0.0267 was:0.0562 not:0.0286 �:0.0172.:0.1025.:0.1079 and:0.0742)
		(,:0.0164
:0.0267 was:0.0562 not:0.0286 �:0.0172.:0.1025.:0.1079 and:0.0742)
 and
------
		(,:0.2031,:0.0737,:0.0157 new:0.0145.:0.2119.:0.1768 and:0.1021 the:0.0586)
		(,:0.1113,:0.0481,:0.0386 few:0.0260.:0.1436.:0.1357 the:0.1133 the:0.0791)
		(,:0.0933 also:0.0449.:0.0227 few:0.0248 of:0.1050.:0.1040 the:0.0986 the:0.0742)
		(,:0.0679 also:0.0461.:0.0248 few:0.0242 of:0.0913.:0.0977 the:0.0835 the:0.0664)
		(,:0.0527 also:0.0437.:0.0238 few:0.0194 of:0.0874.:0.0977 the:0.0869 the:0.0708)
		(,:0.0439 also:0.0410.:0.0262 few:0.0193 of:0.0889.:0.1035 the:0.0708 the:0.0625)
		(
:0.0403 also:0.0347.:0.0254 new:0.0183.:0.0884.:0.1094 the:0.0723 the:0.0664)
		(
:0.0354 said:0.0334 not:0.0256 new:0.0184.:0.0942.:0.1094 and:0.0688 the:0.0571)
		(
:0.0386 said:0.0349 not:0.0270 new:0.0181.:0.0947.:0.1133 and:0.0693 the:0.0625)
		(
:0.0322 was:0.0405 not:0.0286 new:0.0179.:0.0991.:0.1108 and:0.0698 the:0.0544)
		(
:0.0330 was:0.0476 not:0.0286 new:0.0177.:0.1025.:0.1138 and:0.0742 the:0.0569)
		(
:0.0267 was:0.0562 not:0.0286 �:0.0172.:0.1025.:0.1079 and:0.0742 the:0.0505)
		(
:0.0267 was:0.0562 not:0.0286 �:0.0172.:0.1025.:0.1079 and:0.0742 the:0.0505)
 the first, and the first, and the first,
650: sample 0: Hello, I'm a language model,
------
		(,:0.0771,:0.0903,:0.0503.:0.0081 �:0.0184,:0.1562.:0.1396 and:0.0767)
		(,:0.0962,:0.0540,:0.0306.:0.0145 few:0.0205,:0.1396.:0.1357 and:0.0962)
		(,:0.1128 the:0.0508 was:0.0420.:0.0125 few:0.0339.:0.1113.:0.1396 and:0.1064)
		(,:0.1367 the:0.0757 was:0.0564.:0.0115 few:0.0376.:0.1104.:0.1377 and:0.0957)
		(,:0.1426 the:0.0806 was:0.0679.:0.0114 few:0.0393.:0.1104.:0.1436 and:0.0962)
		(,:0.1562 the:0.1221 was:0.0806.:0.0125 few:0.0391.:0.1147.:0.1445 and:0.0908)
		(,:0.1592 the:0.1079 was:0.0908.:0.0134 few:0.0383.:0.1147.:0.1494 and:0.0928)
		(,:0.1650 the:0.1309 was:0.0981.:0.0143 few:0.0381.:0.1211.:0.1553 and:0.0898)
		(,:0.1621 the:0.1030 was:0.0991.:0.0152 few:0.0359.:0.1201.:0.1553 and:0.0903)
		(,:0.1621 the:0.1094 was:0.0898.:0.0161 few:0.0347.:0.1191.:0.1582 and:0.0879)
		(,:0.1562 the:0.0840 was:0.0718.:0.0165 few:0.0337.:0.1201.:0.1602 and:0.0859)
		(,:0.1523 the:0.0820 was:0.0540.:0.0164 few:0.0325.:0.1177.:0.1562 and:0.0840)
		(,:0.1523 the:0.0820 was:0.0540.:0.0164 few:0.0325.:0.1177.:0.1562 and:0.0840)
 and
------
		(,:0.0903,:0.0503.:0.0081 �:0.0184,:0.1562.:0.1396 and:0.0767 the:0.0669)
		(,:0.0540,:0.0306.:0.0145 few:0.0205,:0.1396.:0.1357 and:0.0962 the:0.0713)
		( the:0.0508 was:0.0420.:0.0125 few:0.0339.:0.1113.:0.1396 and:0.1064 the:0.0623)
		( the:0.0757 was:0.0564.:0.0115 few:0.0376.:0.1104.:0.1377 and:0.0957 the:0.0698)
		( the:0.0806 was:0.0679.:0.0114 few:0.0393.:0.1104.:0.1436 and:0.0962 the:0.0640)
		( the:0.1221 was:0.0806.:0.0125 few:0.0391.:0.1147.:0.1445 and:0.0908 the:0.0737)
		( the:0.1079 was:0.0908.:0.0134 few:0.0383.:0.1147.:0.1494 and:0.0928 the:0.0708)
		( the:0.1309 was:0.0981.:0.0143 few:0.0381.:0.1211.:0.1553 and:0.0898 the:0.0747)
		( the:0.1030 was:0.0991.:0.0152 few:0.0359.:0.1201.:0.1553 and:0.0903 the:0.0703)
		( the:0.1094 was:0.0898.:0.0161 few:0.0347.:0.1191.:0.1582 and:0.0879 the:0.0703)
		( the:0.0840 was:0.0718.:0.0165 few:0.0337.:0.1201.:0.1602 and:0.0859 the:0.0674)
		( the:0.0820 was:0.0540.:0.0164 few:0.0325.:0.1177.:0.1562 and:0.0840 the:0.0654)
		( the:0.0820 was:0.0540.:0.0164 few:0.0325.:0.1177.:0.1562 and:0.0840 the:0.0654)
 the first-to-to-to-to-
800: sample 0: Hello, I'm a language model,
------
		(,:0.0742,:0.0830 have:0.0255,:0.0152 few:0.0134 of:0.0884 of:0.1338 and:0.0737)
		(,:0.0942
:0.0305 was:0.0459,:0.0247 few:0.0189 of:0.1235 of:0.1543 and:0.0620)
		(,:0.1030
:0.0347 was:0.0742 not:0.0280 few:0.0272 of:0.1245 of:0.1299 and:0.0669)
		(,:0.1484 the:0.0391 was:0.0869 not:0.0459 lot:0.0315 of:0.1279 of:0.1167 and:0.0603)
		(,:0.1533 the:0.0361 was:0.0908 not:0.0693 lot:0.0361 of:0.1260 of:0.1104 and:0.0659)
		(,:0.1729 the:0.0520 was:0.0894 not:0.0903 lot:0.0398 of:0.1299.:0.1040 and:0.0693)
		(,:0.1758 the:0.0486 was:0.0830 not:0.1045 lot:0.0400 of:0.1309.:0.1050 and:0.0752)
		(,:0.1855 the:0.0581 was:0.0811 not:0.1138 lot:0.0396 of:0.1367.:0.1035 and:0.0825)
		(,:0.1836 the:0.0588 was:0.0737 not:0.1201 lot:0.0396 of:0.1455.:0.1011 and:0.0889)
		(,:0.1777 the:0.0625 was:0.0703 not:0.1226 lot:0.0386 of:0.1523 of:0.1021 and:0.0981)
		(,:0.1719 the:0.0649 was:0.0654 not:0.1206 lot:0.0383 of:0.1592 of:0.1040 and:0.1055)
		(,:0.1660 the:0.0645�:0.0645 not:0.1157 lot:0.0396 of:0.1689 of:0.1064 and:0.1099)
		(,:0.1660 the:0.0645�:0.0645 not:0.1157 lot:0.0396 of:0.1689 of:0.1064 and:0.1099)
 and
------
		(,:0.0830 have:0.0255,:0.0152 few:0.0134 of:0.0884 of:0.1338 and:0.0737 the:0.0540)
		(
:0.0305 was:0.0459,:0.0247 few:0.0189 of:0.1235 of:0.1543 and:0.0620 the:0.0884)
		(
:0.0347 was:0.0742 not:0.0280 few:0.0272 of:0.1245 of:0.1299 and:0.0669 the:0.0923)
		( the:0.0391 was:0.0869 not:0.0459 lot:0.0315 of:0.1279 of:0.1167 and:0.0603 the:0.0815)
		( the:0.0361 was:0.0908 not:0.0693 lot:0.0361 of:0.1260 of:0.1104 and:0.0659 the:0.0791)
		( the:0.0520 was:0.0894 not:0.0903 lot:0.0398 of:0.1299.:0.1040 and:0.0693 the:0.0728)
		( the:0.0486 was:0.0830 not:0.1045 lot:0.0400 of:0.1309.:0.1050 and:0.0752 I:0.0786)
		( the:0.0581 was:0.0811 not:0.1138 lot:0.0396 of:0.1367.:0.1035 and:0.0825 I:0.0825)
		( the:0.0588 was:0.0737 not:0.1201 lot:0.0396 of:0.1455.:0.1011 and:0.0889 I:0.0879)
		( the:0.0625 was:0.0703 not:0.1226 lot:0.0386 of:0.1523 of:0.1021 and:0.0981 I:0.0874)
		( the:0.0649 was:0.0654 not:0.1206 lot:0.0383 of:0.1592 of:0.1040 and:0.1055 I:0.0889)
		( the:0.0645�:0.0645 not:0.1157 lot:0.0396 of:0.1689 of:0.1064 and:0.1099 I:0.0850)
		( the:0.0645�:0.0645 not:0.1157 lot:0.0396 of:0.1689 of:0.1064 and:0.1099 I:0.0850)
 I have a few of the first time.
I
950: sample 0: Hello, I'm a language model,
------
		(,:0.0762,:0.1035 have:0.0354 not:0.0232 good:0.0170.:0.0801 of:0.1094 and:0.1504)
		(,:0.0962
:0.0645 have:0.0669 not:0.0620 good:0.0231.:0.1152.:0.1377 and:0.1357)
		(,:0.0815
:0.0781 have:0.0718 not:0.1182 good:0.0253.:0.0972.:0.1338 and:0.0986)
		(,:0.1094
:0.0791 have:0.0776 not:0.1641 great:0.0295 of:0.1035.:0.1270 and:0.1084)
		(,:0.0903
:0.0554 have:0.0728 not:0.1826 great:0.0359 of:0.1226.:0.1289 but:0.1357)
		(,:0.0918 and:0.0591 have:0.0698 not:0.1699 great:0.0398 of:0.1387.:0.1250 but:0.1562)
		(.:0.0903 and:0.0664 have:0.0693 not:0.1738 great:0.0439 of:0.1553.:0.1260 but:0.1650)
		(.:0.0879 and:0.0752 have:0.0688 not:0.1650 great:0.0449 of:0.1748 of:0.1338 but:0.1660)
		(.:0.0884 and:0.0806 have:0.0664 not:0.1543 great:0.0454 of:0.1875 of:0.1338 but:0.1582)
		(.:0.0864 and:0.0869 have:0.0664 not:0.1426 great:0.0449 of:0.2012 of:0.1387 and:0.1602)
		(.:0.0845 and:0.0889 have:0.0645 not:0.1328 great:0.0430 of:0.2080 of:0.1387 and:0.1621)
		(.:0.0806 and:0.0908 have:0.0630 not:0.1206 great:0.0400 of:0.2148 of:0.1348 and:0.1611)
		(.:0.0806 and:0.0908 have:0.0630 not:0.1206 great:0.0400 of:0.2148 of:0.1348 and:0.1611)
 and
------
		(,:0.1035 have:0.0354 not:0.0232 good:0.0170.:0.0801 of:0.1094 and:0.1504 the:0.0386)
		(
:0.0645 have:0.0669 not:0.0620 good:0.0231.:0.1152.:0.1377 and:0.1357 the:0.0762)
		(
:0.0781 have:0.0718 not:0.1182 good:0.0253.:0.0972.:0.1338 and:0.0986 the:0.0801)
		(
:0.0791 have:0.0776 not:0.1641 great:0.0295 of:0.1035.:0.1270 and:0.1084 the:0.0708)
		(
:0.0554 have:0.0728 not:0.1826 great:0.0359 of:0.1226.:0.1289 but:0.1357 I:0.0786)
		( and:0.0591 have:0.0698 not:0.1699 great:0.0398 of:0.1387.:0.1250 but:0.1562 I:0.0962)
		( and:0.0664 have:0.0693 not:0.1738 great:0.0439 of:0.1553.:0.1260 but:0.1650 I:0.1084)
		( and:0.0752 have:0.0688 not:0.1650 great:0.0449 of:0.1748 of:0.1338 but:0.1660 I:0.1201)
		( and:0.0806 have:0.0664 not:0.1543 great:0.0454 of:0.1875 of:0.1338 but:0.1582 I:0.1289)
		( and:0.0869 have:0.0664 not:0.1426 great:0.0449 of:0.2012 of:0.1387 and:0.1602 I:0.1318)
		( and:0.0889 have:0.0645 not:0.1328 great:0.0430 of:0.2080 of:0.1387 and:0.1621 I:0.1318)
		( and:0.0908 have:0.0630 not:0.1206 great:0.0400 of:0.2148 of:0.1348 and:0.1611 I:0.1260)
		( and:0.0908 have:0.0630 not:0.1206 great:0.0400 of:0.2148 of:0.1348 and:0.1611 I:0.1260)
 I am not a very different language.
I am
1100: sample 0: Hello, I'm a language model,
------
		(,:0.0879 and:0.0825 have:0.0391 I:0.0308 few:0.0278,:0.1270.:0.1494 and:0.1279)
		(,:0.1196 you:0.0923�:0.0879 I:0.0679 few:0.0332,:0.1152.:0.1377 and:0.0815)
		(,:0.0884 you:0.0957�:0.0918 not:0.0742 few:0.0486,:0.1060,:0.1406 and:0.0728)
		(,:0.1152 you:0.0791�:0.0781 not:0.0933 lot:0.0408,:0.1006.:0.1504 but:0.0825)
		(,:0.0928 you:0.0679 have:0.0791 not:0.0972 lot:0.0396 of:0.1157.:0.1553 but:0.0942)
		(,:0.0957 and:0.0659 have:0.0845 not:0.0952 lot:0.0398 of:0.1260.:0.1553 but:0.1011)
		(,:0.0913 and:0.0713 have:0.0869 not:0.0908 lot:0.0417 of:0.1338.:0.1553 but:0.1016)
		(,:0.0923 and:0.0757 have:0.0908 not:0.0874 lot:0.0427 of:0.1455.:0.1562 but:0.1045)
		(,:0.0869 and:0.0732 have:0.0903 not:0.0854 lot:0.0449 of:0.1504.:0.1562 but:0.1030)
		(,:0.0869 and:0.0747 have:0.0884 not:0.0806 lot:0.0457 of:0.1562.:0.1514 but:0.1045)
		(,:0.0762 and:0.0708 have:0.0864 not:0.0767 lot:0.0461 of:0.1602.:0.1514 but:0.1035)
		(,:0.0703 and:0.0664 have:0.0835 not:0.0723 lot:0.0469 of:0.1621.:0.1484 but:0.1025)
		(,:0.0703 and:0.0664 have:0.0835 not:0.0723 lot:0.0469 of:0.1621.:0.1484 but:0.1025)
 but
------
		( and:0.0825 have:0.0391 I:0.0308 few:0.0278,:0.1270.:0.1494 and:0.1279 it:0.0728)
		( you:0.0923�:0.0879 I:0.0679 few:0.0332,:0.1152.:0.1377 and:0.0815 it:0.1289)
		( you:0.0957�:0.0918 not:0.0742 few:0.0486,:0.1060,:0.1406 and:0.0728 it:0.1680)
		( you:0.0791�:0.0781 not:0.0933 lot:0.0408,:0.1006.:0.1504 but:0.0825 it:0.1680)
		( you:0.0679 have:0.0791 not:0.0972 lot:0.0396 of:0.1157.:0.1553 but:0.0942 it:0.1631)
		( and:0.0659 have:0.0845 not:0.0952 lot:0.0398 of:0.1260.:0.1553 but:0.1011 it:0.1523)
		( and:0.0713 have:0.0869 not:0.0908 lot:0.0417 of:0.1338.:0.1553 but:0.1016 it:0.1523)
		( and:0.0757 have:0.0908 not:0.0874 lot:0.0427 of:0.1455.:0.1562 but:0.1045 it:0.1504)
		( and:0.0732 have:0.0903 not:0.0854 lot:0.0449 of:0.1504.:0.1562 but:0.1030 it:0.1504)
		( and:0.0747 have:0.0884 not:0.0806 lot:0.0457 of:0.1562.:0.1514 but:0.1045 it:0.1475)
		( and:0.0708 have:0.0864 not:0.0767 lot:0.0461 of:0.1602.:0.1514 but:0.1035 it:0.1455)
		( and:0.0664 have:0.0835 not:0.0723 lot:0.0469 of:0.1621.:0.1484 but:0.1025 it:0.1416)
		( and:0.0664 have:0.0835 not:0.0723 lot:0.0469 of:0.1621.:0.1484 but:0.1025 it:0.1416)
 it is a way to make a lot of people.
1250: sample 0: Hello, I'm a language model,
------
		(,:0.0583 and:0.0830 was:0.0430 I:0.0167 little:0.0216.:0.1196.:0.1729 and:0.1357)
		(,:0.0894 you:0.0483�:0.0928 I:0.0493 few:0.0282.:0.1094.:0.1914 and:0.0825)
		(.:0.0840 and:0.0713�:0.1055 not:0.0479 few:0.0344.:0.1108.:0.2041 but:0.1143)
		(,:0.1060 and:0.1064�:0.0859 not:0.0535 bit:0.0267.:0.1318.:0.2197 but:0.1455)
		(.:0.1118 and:0.1113�:0.0835 not:0.0576 bit:0.0276.:0.1416.:0.2188 but:0.1572)
		(,:0.1128 and:0.1235�:0.0825 not:0.0613 bit:0.0280.:0.1504.:0.2178 but:0.1611)
		(.:0.1177 and:0.1289�:0.0859 not:0.0635 bit:0.0291.:0.1572.:0.2158 but:0.1631)
		(,:0.1191 and:0.1367�:0.0889 not:0.0645 bit:0.0306.:0.1553.:0.2090 but:0.1680)
		(.:0.1196 and:0.1377�:0.0933 not:0.0659 bit:0.0322.:0.1592.:0.2051 but:0.1699)
		(.:0.1191 and:0.1396�:0.1006 not:0.0659 bit:0.0330.:0.1572.:0.2002 but:0.1719)
		(.:0.1187 and:0.1357�:0.1064 not:0.0654 bit:0.0337.:0.1533.:0.1973 but:0.1680)
		(.:0.1201 and:0.1328�:0.1138 not:0.0649 few:0.0344.:0.1562.:0.1943 but:0.1689)
		(.:0.1201 and:0.1328�:0.1138 not:0.0649 few:0.0344.:0.1562.:0.1943 but:0.1689)
 but
------
		( and:0.0830 was:0.0430 I:0.0167 little:0.0216.:0.1196.:0.1729 and:0.1357 the:0.0669)
		( you:0.0483�:0.0928 I:0.0493 few:0.0282.:0.1094.:0.1914 and:0.0825 it:0.1011)
		( and:0.0713�:0.1055 not:0.0479 few:0.0344.:0.1108.:0.2041 but:0.1143 I:0.1118)
		( and:0.1064�:0.0859 not:0.0535 bit:0.0267.:0.1318.:0.2197 but:0.1455 I:0.1729)
		( and:0.1113�:0.0835 not:0.0576 bit:0.0276.:0.1416.:0.2188 but:0.1572 I:0.1992)
		( and:0.1235�:0.0825 not:0.0613 bit:0.0280.:0.1504.:0.2178 but:0.1611 I:0.2139)
		( and:0.1289�:0.0859 not:0.0635 bit:0.0291.:0.1572.:0.2158 but:0.1631 I:0.2139)
		( and:0.1367�:0.0889 not:0.0645 bit:0.0306.:0.1553.:0.2090 but:0.1680 I:0.2129)
		( and:0.1377�:0.0933 not:0.0659 bit:0.0322.:0.1592.:0.2051 but:0.1699 I:0.2051)
		( and:0.1396�:0.1006 not:0.0659 bit:0.0330.:0.1572.:0.2002 but:0.1719 I:0.2021)
		( and:0.1357�:0.1064 not:0.0654 bit:0.0337.:0.1533.:0.1973 but:0.1680 I:0.1943)
		( and:0.1328�:0.1138 not:0.0649 few:0.0344.:0.1562.:0.1943 but:0.1689 I:0.1904)
		( and:0.1328�:0.1138 not:0.0649 few:0.0344.:0.1562.:0.1943 but:0.1689 I:0.1904)
 I have a lot of things.
I have a
1400: sample 0: Hello, I'm a language model,
------
		(,:0.0786 the:0.0635 was:0.0383 going:0.0251 good:0.0294,:0.1079,:0.1309 and:0.1289)
		(,:0.0996 you:0.0811�:0.0850 going:0.0400 good:0.0271,:0.1108,:0.1396 I:0.0894)
		(,:0.0991 you:0.0962�:0.1260 a:0.0520 bit:0.0342 that:0.1187 that:0.1494 I:0.0947)
		(,:0.1201 you:0.0850�:0.1240 going:0.0615 bit:0.0294 that:0.1465 that:0.1748 I:0.1157)
		(,:0.1221 you:0.0767�:0.1206 going:0.0728 bit:0.0322 that:0.1553 that:0.1836 I:0.1143)
		(,:0.1299 you:0.0684�:0.1216 going:0.0835 bit:0.0332 that:0.1572 that:0.1865 I:0.1064)
		(,:0.1318 you:0.0659�:0.1240 going:0.0879 bit:0.0359 that:0.1592 that:0.1885 I:0.0947)
		(,:0.1377 the:0.0669�:0.1245 going:0.0928 bit:0.0376 that:0.1611 that:0.1885 I:0.0864)
		(,:0.1357 the:0.0649�:0.1270 going:0.0952 bit:0.0378 that:0.1660 that:0.1914 and:0.0854)
		(,:0.1328 the:0.0649�:0.1260 going:0.0938 bit:0.0383 that:0.1641 that:0.1846 and:0.0845)
		(,:0.1230 the:0.0603�:0.1270 going:0.0908 bit:0.0391 that:0.1641 that:0.1826 and:0.0850)
		(,:0.1177 the:0.0596�:0.1240 going:0.0874 bit:0.0400 that:0.1611 that:0.1797 and:0.0835)
		(,:0.1177 the:0.0596�:0.1240 going:0.0874 bit:0.0400 that:0.1611 that:0.1797 and:0.0835)
 and
------
		( the:0.0635 was:0.0383 going:0.0251 good:0.0294,:0.1079,:0.1309 and:0.1289 the:0.0530)
		( you:0.0811�:0.0850 going:0.0400 good:0.0271,:0.1108,:0.1396 I:0.0894 I:0.1055)
		( you:0.0962�:0.1260 a:0.0520 bit:0.0342 that:0.1187 that:0.1494 I:0.0947 I:0.0972)
		( you:0.0850�:0.1240 going:0.0615 bit:0.0294 that:0.1465 that:0.1748 I:0.1157 I:0.1289)
		( you:0.0767�:0.1206 going:0.0728 bit:0.0322 that:0.1553 that:0.1836 I:0.1143 I:0.1299)
		( you:0.0684�:0.1216 going:0.0835 bit:0.0332 that:0.1572 that:0.1865 I:0.1064 I:0.1289)
		( you:0.0659�:0.1240 going:0.0879 bit:0.0359 that:0.1592 that:0.1885 I:0.0947 I:0.1245)
		( the:0.0669�:0.1245 going:0.0928 bit:0.0376 that:0.1611 that:0.1885 I:0.0864 I:0.1177)
		( the:0.0649�:0.1270 going:0.0952 bit:0.0378 that:0.1660 that:0.1914 and:0.0854 I:0.1128)
		( the:0.0649�:0.1260 going:0.0938 bit:0.0383 that:0.1641 that:0.1846 and:0.0845 I:0.1094)
		( the:0.0603�:0.1270 going:0.0908 bit:0.0391 that:0.1641 that:0.1826 and:0.0850 I:0.1069)
		( the:0.0596�:0.1240 going:0.0874 bit:0.0400 that:0.1611 that:0.1797 and:0.0835 I:0.1025)
		( the:0.0596�:0.1240 going:0.0874 bit:0.0400 that:0.1611 that:0.1797 and:0.0835 I:0.1025)
 I can see the most common language.
The word
