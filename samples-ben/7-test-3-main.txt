1: sample 0: Hello, I'm a language model,
------
		(Hello:0.4902,:0.6562 I:0.4590'm:0.5977 a:0.1953 language:0.4277 model:0.4434,:0.5234)
		(Hello:0.0008,:0.0021 I:0.0005'm:0.0009 a:0.0004 language:0.0008 model:0.0008,:0.0017)
		(Bron:0.0002,:0.0006 I:0.0002'm:0.0002 Murray:0.0002 splendid:0.0001883:0.0002,:0.0008)
		(Bron:0.0002,:0.0003,:0.0002,:0.0002 Murray:0.0003pps:0.0002).:0.0002,:0.0006)
		( Murray:0.0002,:0.0003,:0.0002,:0.0002 Murray:0.0003,:0.0002).:0.0003,:0.0005)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0002,:0.0002,:0.0003).:0.0003,:0.0004)
		( Murray:0.0002,:0.0002,:0.0002,:0.0003,:0.0002,:0.0003).:0.0003,:0.0003)
,
------
		(,:0.6562 I:0.4590'm:0.5977 a:0.1953 language:0.4277 model:0.4434,:0.5234,:0.5820)
		(,:0.0021 I:0.0005'm:0.0009 a:0.0004 language:0.0008 model:0.0008,:0.0017,:0.0022)
		(,:0.0006 I:0.0002'm:0.0002 Murray:0.0002 splendid:0.0001883:0.0002,:0.0008,:0.0008)
		(,:0.0003,:0.0002,:0.0002 Murray:0.0003pps:0.0002).:0.0002,:0.0006,:0.0005)
		(,:0.0003,:0.0002,:0.0002 Murray:0.0003,:0.0002).:0.0003,:0.0005,:0.0004)
		(,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004,:0.0004)
		(,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004,:0.0004)
		(,:0.0002,:0.0002,:0.0002 Murray:0.0002,:0.0002).:0.0003,:0.0004,:0.0004)
		(,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004,:0.0003)
		(,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004,:0.0003)
		(,:0.0002,:0.0002,:0.0002,:0.0002,:0.0002).:0.0003,:0.0004,:0.0003)
		(,:0.0002,:0.0002,:0.0002,:0.0002,:0.0003).:0.0003,:0.0004,:0.0003)
		(,:0.0002,:0.0002,:0.0003,:0.0002,:0.0003).:0.0003,:0.0003,:0.0003)
,,,,,,,,,,,
50: sample 0: Hello, I'm a language model,
------
		(Hello:0.4590,:0.9844 I:0.7266'm:0.5039 a:0.6797 language:0.5586 model:0.4805,:0.9727)
		(,:0.0267,:0.0366,:0.0977,:0.1582 the:0.1348,:0.1426,:0.1523 the:0.0913)
		(.:0.0413,:0.0410.:0.0786,:0.1113 the:0.1011,:0.1006,:0.0996 the:0.0544)
		(.:0.0483.:0.0474.:0.0771.:0.1021 the:0.0767.:0.0908.:0.0859 the:0.0420)
		(.:0.0525.:0.0518.:0.0786.:0.0952 the:0.0630.:0.0864.:0.0840 the:0.0356)
		(.:0.0552.:0.0532.:0.0771.:0.0923 the:0.0537.:0.0854.:0.0835 the:0.0320)
		(.:0.0569.:0.0547.:0.0776.:0.0938 the:0.0486.:0.0840.:0.0820.:0.0309)
		(.:0.0583.:0.0564.:0.0757.:0.0894 the:0.0439.:0.0825.:0.0806.:0.0337)
		(.:0.0601.:0.0579.:0.0762.:0.0903 the:0.0410.:0.0830.:0.0811.:0.0356)
		(.:0.0615.:0.0596.:0.0762.:0.0884 the:0.0383.:0.0811.:0.0791.:0.0378)
		(.:0.0615.:0.0596.:0.0767.:0.0889 the:0.0359.:0.0815.:0.0796.:0.0388)
		(.:0.0615.:0.0596.:0.0767.:0.0869 the:0.0334.:0.0796.:0.0801.:0.0398)
		(.:0.0630.:0.0613.:0.0767.:0.0874 the:0.0322.:0.0796.:0.0801.:0.0410)
.
------
		(,:0.9844 I:0.7266'm:0.5039 a:0.6797 language:0.5586 model:0.4805,:0.9727.:0.9648)
		(,:0.0366,:0.0977,:0.1582 the:0.1348,:0.1426,:0.1523 the:0.0913 Jed:0.0004)
		(,:0.0410.:0.0786,:0.1113 the:0.1011,:0.1006,:0.0996 the:0.0544 However:0.0021)
		(.:0.0474.:0.0771.:0.1021 the:0.0767.:0.0908.:0.0859 the:0.0420 However:0.0052)
		(.:0.0518.:0.0786.:0.0952 the:0.0630.:0.0864.:0.0840 the:0.0356
:0.0151)
		(.:0.0532.:0.0771.:0.0923 the:0.0537.:0.0854.:0.0835 the:0.0320
:0.0332)
		(.:0.0547.:0.0776.:0.0938 the:0.0486.:0.0840.:0.0820.:0.0309
:0.0571)
		(.:0.0564.:0.0757.:0.0894 the:0.0439.:0.0825.:0.0806.:0.0337
:0.0850)
		(.:0.0579.:0.0762.:0.0903 the:0.0410.:0.0830.:0.0811.:0.0356
:0.1099)
		(.:0.0596.:0.0762.:0.0884 the:0.0383.:0.0811.:0.0791.:0.0378
:0.1406)
		(.:0.0596.:0.0767.:0.0889 the:0.0359.:0.0815.:0.0796.:0.0388
:0.1602)
		(.:0.0596.:0.0767.:0.0869 the:0.0334.:0.0796.:0.0801.:0.0398
:0.1816)
		(.:0.0613.:0.0767.:0.0874 the:0.0322.:0.0796.:0.0801.:0.0410
:0.2051)

, the the the the the the the the the
200: sample 0: Hello, I'm a language model,
------
		(Hello:0.2773,:0.2080 I:0.4258'm:0.6406 a:0.9219 language:0.2490 model:0.5352,:0.9727)
		( the:0.0977 the:0.0270 to:0.0732 the:0.0320 be:0.0796 and:0.1260 of:0.3574 the:0.1602)
		( and:0.1118 the:0.0251 to:0.1328 the:0.0840 be:0.0742 of:0.1484 of:0.4355 the:0.1299)
		( and:0.1099,:0.0259 to:0.1465 the:0.0928 be:0.0586 of:0.1445 of:0.4160 the:0.1050)
		(,:0.1025,:0.0291 to:0.1533 the:0.0957 be:0.0466 of:0.1245 of:0.3496 the:0.0850)
		(,:0.1123 to:0.0325 to:0.1484 the:0.0879 be:0.0386.:0.1147 of:0.2832 the:0.0713)
		(,:0.1147 to:0.0388 to:0.1445 the:0.0850 be:0.0325.:0.1250 of:0.2285 the:0.0605)
		(,:0.1162 to:0.0447 to:0.1426 the:0.0815 the:0.0287.:0.1357 of:0.1826 the:0.0522)
		(,:0.1167 to:0.0500 to:0.1406 the:0.0737 the:0.0366.:0.1377.:0.1611 and:0.0593)
		(,:0.1167 to:0.0542 to:0.1387 the:0.0708 the:0.0439.:0.1406.:0.1680 and:0.0649)
		(,:0.1172 to:0.0588 to:0.1309 the:0.0659 the:0.0537.:0.1406.:0.1729 and:0.0684)
		(,:0.1162 to:0.0620 to:0.1299 the:0.0625 the:0.0620.:0.1416.:0.1846 and:0.0723)
		(,:0.1157 to:0.0654 to:0.1299 the:0.0596 the:0.0713.:0.1338.:0.1875 and:0.0757)
 and
------
		(,:0.2080 I:0.4258'm:0.6406 a:0.9219 language:0.2490 model:0.5352,:0.9727 and:0.9922)
		( the:0.0270 to:0.0732 the:0.0320 be:0.0796 and:0.1260 of:0.3574 the:0.1602 the:0.1006)
		( the:0.0251 to:0.1328 the:0.0840 be:0.0742 of:0.1484 of:0.4355 the:0.1299 the:0.0889)
		(,:0.0259 to:0.1465 the:0.0928 be:0.0586 of:0.1445 of:0.4160 the:0.1050 the:0.0781)
		(,:0.0291 to:0.1533 the:0.0957 be:0.0466 of:0.1245 of:0.3496 the:0.0850 the:0.0718)
		( to:0.0325 to:0.1484 the:0.0879 be:0.0386.:0.1147 of:0.2832 the:0.0713 the:0.0625)
		( to:0.0388 to:0.1445 the:0.0850 be:0.0325.:0.1250 of:0.2285 the:0.0605 the:0.0574)
		( to:0.0447 to:0.1426 the:0.0815 the:0.0287.:0.1357 of:0.1826 the:0.0522 the:0.0525)
		( to:0.0500 to:0.1406 the:0.0737 the:0.0366.:0.1377.:0.1611 and:0.0593 the:0.0488)
		( to:0.0542 to:0.1387 the:0.0708 the:0.0439.:0.1406.:0.1680 and:0.0649 the:0.0454)
		( to:0.0588 to:0.1309 the:0.0659 the:0.0537.:0.1406.:0.1729 and:0.0684 the:0.0420)
		( to:0.0620 to:0.1299 the:0.0625 the:0.0620.:0.1416.:0.1846 and:0.0723 the:0.0398)
		( to:0.0654 to:0.1299 the:0.0596 the:0.0713.:0.1338.:0.1875 and:0.0757 the:0.0376)
 the new the new the new the new the most the
350: sample 0: Hello, I'm a language model,
------
		(Hello:0.0574,:0.0177 I:0.2334'm:0.5430 a:0.9766 language:0.3262 model:0.4785,:0.9922)
		(,:0.1582 a:0.1865 is:0.0505 a:0.2734 lot:0.0129 and:0.2432 of:0.4336 a:0.2930)
		(,:0.1768 a:0.1699 can:0.0659 a:0.2656 help:0.0165 and:0.2139 of:0.4395 a:0.2158)
		(,:0.1680 a:0.1377 can:0.0718 a:0.2275 help:0.0149 and:0.1768 of:0.4121 a:0.1338)
		(,:0.1494 a:0.1074 can:0.0752 a:0.1865 lot:0.0148.:0.1543 of:0.3438 the:0.0869)
		(,:0.1348 the:0.0830 can:0.0776 a:0.1592 lot:0.0159.:0.1602 of:0.2812 and:0.0757)
		(,:0.1201 the:0.0728 can:0.0796 a:0.1270 lot:0.0166.:0.1631 of:0.2363 and:0.0796)
		(,:0.1069 the:0.0623 can:0.0771 a:0.1113 lot:0.0171.:0.1602 of:0.1904 and:0.0786)
		(,:0.0991 and:0.0635 can:0.0786 a:0.0918 lot:0.0170.:0.1562 of:0.1494 and:0.0767)
		(,:0.0903 and:0.0664 can:0.0757 a:0.0791 lot:0.0164.:0.1504.:0.1367 and:0.0752)
		(,:0.0840 and:0.0693 can:0.0728 a:0.0679 child:0.0172.:0.1504.:0.1396 and:0.0728)
		(,:0.0781 and:0.0723 can:0.0742 a:0.0581 child:0.0188.:0.1426.:0.1406 and:0.0703)
		(,:0.0742 and:0.0732 can:0.0708 a:0.0525 child:0.0206.:0.1396.:0.1387 and:0.0674)
 and
------
		(,:0.0177 I:0.2334'm:0.5430 a:0.9766 language:0.3262 model:0.4785,:0.9922 and:0.9961)
		( a:0.1865 is:0.0505 a:0.2734 lot:0.0129 and:0.2432 of:0.4336 a:0.2930 a:0.2715)
		( a:0.1699 can:0.0659 a:0.2656 help:0.0165 and:0.2139 of:0.4395 a:0.2158 a:0.2324)
		( a:0.1377 can:0.0718 a:0.2275 help:0.0149 and:0.1768 of:0.4121 a:0.1338 a:0.1533)
		( a:0.1074 can:0.0752 a:0.1865 lot:0.0148.:0.1543 of:0.3438 the:0.0869 a:0.1016)
		( the:0.0830 can:0.0776 a:0.1592 lot:0.0159.:0.1602 of:0.2812 and:0.0757 a:0.0635)
		( the:0.0728 can:0.0796 a:0.1270 lot:0.0166.:0.1631 of:0.2363 and:0.0796 a:0.0432)
		( the:0.0623 can:0.0771 a:0.1113 lot:0.0171.:0.1602 of:0.1904 and:0.0786 you:0.0325)
		( and:0.0635 can:0.0786 a:0.0918 lot:0.0170.:0.1562 of:0.1494 and:0.0767 you:0.0339)
		( and:0.0664 can:0.0757 a:0.0791 lot:0.0164.:0.1504.:0.1367 and:0.0752 you:0.0342)
		( and:0.0693 can:0.0728 a:0.0679 child:0.0172.:0.1504.:0.1396 and:0.0728 you:0.0332)
		( and:0.0723 can:0.0742 a:0.0581 child:0.0188.:0.1426.:0.1406 and:0.0703 you:0.0322)
		( and:0.0732 can:0.0708 a:0.0525 child:0.0206.:0.1396.:0.1387 and:0.0674 you:0.0311)
 you can be to be to be to be to be
500: sample 0: Hello, I'm a language model,
------
		( the:0.0737,:0.0009 I:0.0317'm:0.5234 a:0.9883 language:0.3496 model:0.7305,:0.9961)
		(.:0.3281 a:0.2041 to:0.0869 a:0.1641 use:0.0127 and:0.2363 of:0.1992 a:0.1826)
		(.:0.3125 a:0.1719,:0.0952 a:0.1562 result:0.0189 and:0.2090 of:0.2373 the:0.1748)
		(.:0.2559 a:0.1206,:0.0840 a:0.1377 result:0.0204 and:0.1797 of:0.2734 the:0.1445)
		(.:0.1963 the:0.1060,:0.0718 a:0.1172 result:0.0197.:0.1475 of:0.2891 the:0.1113)
		(.:0.1533 the:0.0854,:0.0579 a:0.0952 result:0.0176.:0.1465 of:0.2832 and:0.0933)
		(.:0.1235 the:0.0649 be:0.0591 a:0.0742 lot:0.0183.:0.1475 of:0.2637 and:0.0840)
		(.:0.1016 and:0.0593 be:0.0571 a:0.0610 lot:0.0204.:0.1445 of:0.2461 and:0.0811)
		(.:0.0864 and:0.0583 be:0.0537 a:0.0493 lot:0.0214.:0.1436 of:0.2158 and:0.0757)
		(.:0.0762 and:0.0586 be:0.0505 a:0.0432 lot:0.0220.:0.1387 of:0.1904 and:0.0747)
		(.:0.0669 and:0.0603 be:0.0461 a:0.0374 lot:0.0208.:0.1318 of:0.1650 and:0.0732)
		(.:0.0601 and:0.0601 been:0.0444 the:0.0349 few:0.0217.:0.1270 of:0.1406 and:0.0698)
		(.:0.0537 and:0.0615 been:0.0430 the:0.0347 few:0.0228.:0.1206.:0.1309 and:0.0684)
 and
------
		(,:0.0009 I:0.0317'm:0.5234 a:0.9883 language:0.3496 model:0.7305,:0.9961 and:0.9961)
		( a:0.2041 to:0.0869 a:0.1641 use:0.0127 and:0.2363 of:0.1992 a:0.1826 a:0.3438)
		( a:0.1719,:0.0952 a:0.1562 result:0.0189 and:0.2090 of:0.2373 the:0.1748 a:0.2812)
		( a:0.1206,:0.0840 a:0.1377 result:0.0204 and:0.1797 of:0.2734 the:0.1445 a:0.1982)
		( the:0.1060,:0.0718 a:0.1172 result:0.0197.:0.1475 of:0.2891 the:0.1113 the:0.1465)
		( the:0.0854,:0.0579 a:0.0952 result:0.0176.:0.1465 of:0.2832 and:0.0933 the:0.1060)
		( the:0.0649 be:0.0591 a:0.0742 lot:0.0183.:0.1475 of:0.2637 and:0.0840 the:0.0786)
		( and:0.0593 be:0.0571 a:0.0610 lot:0.0204.:0.1445 of:0.2461 and:0.0811 the:0.0618)
		( and:0.0583 be:0.0537 a:0.0493 lot:0.0214.:0.1436 of:0.2158 and:0.0757 the:0.0518)
		( and:0.0586 be:0.0505 a:0.0432 lot:0.0220.:0.1387 of:0.1904 and:0.0747 the:0.0457)
		( and:0.0603 be:0.0461 a:0.0374 lot:0.0208.:0.1318 of:0.1650 and:0.0732 the:0.0422)
		( and:0.0601 been:0.0444 the:0.0349 few:0.0217.:0.1270 of:0.1406 and:0.0698 the:0.0388)
		( and:0.0615 been:0.0430 the:0.0347 few:0.0228.:0.1206.:0.1309 and:0.0684 the:0.0376)
 the same time.
The first time.
The
650: sample 0: Hello, I'm a language model,
------
		( the:0.2617 Melania:0.0015 I:0.0065'm:0.5547 a:0.9922 language:0.4512 model:0.6328,:0.9961)
		(.:0.1914 a:0.1514,:0.1543 a:0.1533 use:0.0190 and:0.3457 and:0.2715 a:0.1816)
		(.:0.2256 and:0.1777,:0.1230 a:0.1050 result:0.0211 and:0.3359 and:0.2656 and:0.1660)
		(.:0.1787 and:0.1738,:0.1006 a:0.0718 result:0.0249 and:0.2988 and:0.2393 and:0.1650)
		(.:0.1406 and:0.1533,:0.0703 the:0.0552 result:0.0259 and:0.2480 and:0.2129 and:0.1416)
		(.:0.1094 and:0.1279,:0.0513 the:0.0522 result:0.0242 and:0.2041 and:0.1787 and:0.1216)
		(.:0.0869 and:0.1128,:0.0420 the:0.0513 new:0.0247 and:0.1562 of:0.1572 and:0.1108)
		(.:0.0718 and:0.1050 have:0.0417 the:0.0508 new:0.0260 and:0.1226 of:0.1768 and:0.1055)
		(,:0.0625 and:0.1055 have:0.0383 the:0.0493 new:0.0265 and:0.0972 of:0.1914 and:0.1016)
		(,:0.0557 and:0.1094 have:0.0334 the:0.0488 new:0.0259.:0.0864 of:0.1943 and:0.1030)
		(,:0.0503 and:0.1104�:0.0403 the:0.0466 new:0.0243,:0.0962 of:0.1875 and:0.1021)
		(,:0.0464 and:0.1143�:0.0474 the:0.0447 new:0.0228,:0.1074 of:0.1670 and:0.1011)
		(,:0.0422 and:0.1187�:0.0547 the:0.0427 new:0.0204,:0.1147 of:0.1553 and:0.1030)
 and
------
		( Melania:0.0015 I:0.0065'm:0.5547 a:0.9922 language:0.4512 model:0.6328,:0.9961 and:0.9961)
		( a:0.1514,:0.1543 a:0.1533 use:0.0190 and:0.3457 and:0.2715 a:0.1816 a:0.2393)
		( and:0.1777,:0.1230 a:0.1050 result:0.0211 and:0.3359 and:0.2656 and:0.1660 a:0.1631)
		( and:0.1738,:0.1006 a:0.0718 result:0.0249 and:0.2988 and:0.2393 and:0.1650 the:0.1240)
		( and:0.1533,:0.0703 the:0.0552 result:0.0259 and:0.2480 and:0.2129 and:0.1416 the:0.0854)
		( and:0.1279,:0.0513 the:0.0522 result:0.0242 and:0.2041 and:0.1787 and:0.1216 the:0.0635)
		( and:0.1128,:0.0420 the:0.0513 new:0.0247 and:0.1562 of:0.1572 and:0.1108 the:0.0510)
		( and:0.1050 have:0.0417 the:0.0508 new:0.0260 and:0.1226 of:0.1768 and:0.1055 the:0.0466)
		( and:0.1055 have:0.0383 the:0.0493 new:0.0265 and:0.0972 of:0.1914 and:0.1016 the:0.0452)
		( and:0.1094 have:0.0334 the:0.0488 new:0.0259.:0.0864 of:0.1943 and:0.1030 the:0.0447)
		( and:0.1104�:0.0403 the:0.0466 new:0.0243,:0.0962 of:0.1875 and:0.1021 the:0.0439)
		( and:0.1143�:0.0474 the:0.0447 new:0.0228,:0.1074 of:0.1670 and:0.1011 the:0.0439)
		( and:0.1187�:0.0547 the:0.0427 new:0.0204,:0.1147 of:0.1553 and:0.1030 the:0.0449)
 the world.
The first time, the world,
800: sample 0: Hello, I'm a language model,
------
		( the:0.4219 Melania:0.0027��:0.0025'm:0.6641 a:0.9922 language:0.6406 model:0.6484,:0.9961)
		(,:0.2412 a:0.0767,:0.0815 important:0.0474 cause:0.0175 and:0.2207 and:0.1836 a:0.1279)
		(,:0.1680 a:0.0374,:0.0566 important:0.0471 result:0.0140 and:0.1758 and:0.1533 the:0.0571)
		(,:0.1226 and:0.0302.:0.0522,:0.0356 result:0.0132 and:0.1021 and:0.1001 and:0.0381)
		(.:0.0938 and:0.0330.:0.0603,:0.0317 result:0.0129 and:0.0674.:0.0898 and:0.0364)
		(.:0.0933 and:0.0396.:0.0742,:0.0286 result:0.0124,:0.0544.:0.0884 and:0.0391)
		(.:0.0977 and:0.0569.:0.0962 not:0.0288 new:0.0132.:0.0645 of:0.1064 and:0.0520)
		(.:0.0952 and:0.0767.:0.1138 not:0.0383 new:0.0152.:0.0840 of:0.1260 and:0.0640)
		(.:0.0879 and:0.0879.:0.1172 not:0.0503 new:0.0146,:0.0967.:0.1357 and:0.0688)
		(.:0.0762 and:0.0923.:0.1079 not:0.0574 new:0.0131,:0.1089.:0.1406 and:0.0698)
		(.:0.0649 and:0.0869.:0.0903 not:0.0598 very:0.0117,:0.1138.:0.1318 and:0.0684)
		(.:0.0537 and:0.0820.:0.0742 not:0.0591 very:0.0118,:0.1196,:0.1226 I:0.1006)
		(.:0.0454 and:0.0747,:0.0669 not:0.0581 man:0.0125,:0.1235,:0.1318 I:0.1328)
 I
------
		( Melania:0.0027��:0.0025'm:0.6641 a:0.9922 language:0.6406 model:0.6484,:0.9961 I:1.0000)
		( a:0.0767,:0.0815 important:0.0474 cause:0.0175 and:0.2207 and:0.1836 a:0.1279,:0.0889)
		( a:0.0374,:0.0566 important:0.0471 result:0.0140 and:0.1758 and:0.1533 the:0.0571,:0.0649)
		( and:0.0302.:0.0522,:0.0356 result:0.0132 and:0.1021 and:0.1001 and:0.0381,:0.0532)
		( and:0.0330.:0.0603,:0.0317 result:0.0129 and:0.0674.:0.0898 and:0.0364.:0.0491)
		( and:0.0396.:0.0742,:0.0286 result:0.0124,:0.0544.:0.0884 and:0.0391.:0.0459)
		( and:0.0569.:0.0962 not:0.0288 new:0.0132.:0.0645 of:0.1064 and:0.0520.:0.0439)
		( and:0.0767.:0.1138 not:0.0383 new:0.0152.:0.0840 of:0.1260 and:0.0640 was:0.0547)
		( and:0.0879.:0.1172 not:0.0503 new:0.0146,:0.0967.:0.1357 and:0.0688 was:0.0649)
		( and:0.0923.:0.1079 not:0.0574 new:0.0131,:0.1089.:0.1406 and:0.0698 was:0.0640)
		( and:0.0869.:0.0903 not:0.0598 very:0.0117,:0.1138.:0.1318 and:0.0684 was:0.0571)
		( and:0.0820.:0.0742 not:0.0591 very:0.0118,:0.1196,:0.1226 I:0.1006 was:0.0515)
		( and:0.0747,:0.0669 not:0.0581 man:0.0125,:0.1235,:0.1318 I:0.1328'm:0.0540)
'm not a very of the word, I'm not
950: sample 0: Hello, I'm a language model,
------
		( the:0.5312embedreportprint:0.0034��:0.0045'm:0.7891 a:0.9883 language:0.8281 model:0.6562,:1.0000)
		(,:0.1504 a:0.0510,:0.0461 important:0.0216,:0.0188,:0.2051 of:0.1816 the:0.0747)
		(,:0.1318 and:0.0320,:0.0400,:0.0312,:0.0270,:0.1602 in:0.1113 and:0.0549)
		(,:0.0630 and:0.0253,:0.0283,:0.0294,:0.0198,:0.1128 of:0.0864 and:0.0425)
		(,:0.0393 and:0.0251.:0.0238 not:0.0256,:0.0120,:0.0776 of:0.0854 and:0.0359)
		(,:0.0332 and:0.0330.:0.0254 not:0.0295,:0.0089,:0.0625 of:0.1045 and:0.0466)
		(,:0.0356 and:0.0532.:0.0327 not:0.0393 very:0.0135.:0.0732 of:0.1504 and:0.0854)
		(,:0.0393 and:0.0835.:0.0435 not:0.0583 very:0.0194.:0.0913 of:0.2285 and:0.1475)
		(,:0.0422 and:0.1123,:0.0569 not:0.0801 very:0.0239.:0.1006 of:0.3438 and:0.2002)
		(,:0.0432 and:0.1216,:0.0579 not:0.1030 very:0.0229 of:0.1001 of:0.3984 and:0.2275)
		(,:0.0417 and:0.1309,:0.0474 not:0.1147 very:0.0222 of:0.0981 of:0.3555 and:0.2344)
		(,:0.0403 and:0.1338,:0.0378 not:0.1206 very:0.0217,:0.1104 of:0.2832 and:0.2305)
		(,:0.0388 and:0.1230 am:0.0366 not:0.1299 very:0.0223,:0.1318 of:0.2227 and:0.2295)
 and
------
		(embedreportprint:0.0034��:0.0045'm:0.7891 a:0.9883 language:0.8281 model:0.6562,:1.0000 and:1.0000)
		( a:0.0510,:0.0461 important:0.0216,:0.0188,:0.2051 of:0.1816 the:0.0747 the:0.0869)
		( and:0.0320,:0.0400,:0.0312,:0.0270,:0.1602 in:0.1113 and:0.0549 the:0.0376)
		( and:0.0253,:0.0283,:0.0294,:0.0198,:0.1128 of:0.0864 and:0.0425 the:0.0251)
		( and:0.0251.:0.0238 not:0.0256,:0.0120,:0.0776 of:0.0854 and:0.0359 the:0.0182)
		( and:0.0330.:0.0254 not:0.0295,:0.0089,:0.0625 of:0.1045 and:0.0466 the:0.0179)
		( and:0.0532.:0.0327 not:0.0393 very:0.0135.:0.0732 of:0.1504 and:0.0854 the:0.0238)
		( and:0.0835.:0.0435 not:0.0583 very:0.0194.:0.0913 of:0.2285 and:0.1475 the:0.0332)
		( and:0.1123,:0.0569 not:0.0801 very:0.0239.:0.1006 of:0.3438 and:0.2002 I:0.0498)
		( and:0.1216,:0.0579 not:0.1030 very:0.0229 of:0.1001 of:0.3984 and:0.2275 I:0.0732)
		( and:0.1309,:0.0474 not:0.1147 very:0.0222 of:0.0981 of:0.3555 and:0.2344 I:0.0938)
		( and:0.1338,:0.0378 not:0.1206 very:0.0217,:0.1104 of:0.2832 and:0.2305 I:0.1221)
		( and:0.1230 am:0.0366 not:0.1299 very:0.0223,:0.1318 of:0.2227 and:0.2295 I:0.1406)
 I can be able to be able to be able to
1100: sample 0: Hello, I'm a language model,
------
		(.:0.4395embedreportprint:0.0038��:0.0064'm:0.9258 a:0.9883 language:0.9219 model:0.6875,:1.0000)
		(,:0.1748 a:0.0393 and:0.0491 important:0.0164 �:0.0198 and:0.1670 of:0.1533 the:0.0557)
		(,:0.1904 and:0.0364,:0.0476,:0.0549,:0.0282,:0.1377 in:0.1064 and:0.0522)
		(,:0.0889 in:0.0248,:0.0309,:0.0376,:0.0150,:0.0840 in:0.0713 and:0.0284)
		(,:0.0552 and:0.0236,:0.0254,:0.0258,:0.0087,:0.0540 in:0.0491 and:0.0208)
		(,:0.0479 and:0.0277,:0.0276 also:0.0214 very:0.0081,:0.0410 in:0.0461 and:0.0259)
		(,:0.0525 and:0.0393,:0.0369 also:0.0239 very:0.0126.:0.0449 of:0.0610 and:0.0403)
		(,:0.0579 and:0.0537,:0.0486 not:0.0332 very:0.0183.:0.0630 of:0.0908 and:0.0576)
		(,:0.0591 and:0.0630,:0.0505 not:0.0444 very:0.0215.:0.0811 of:0.1221 and:0.0679)
		(,:0.0596 and:0.0684 was:0.0522 not:0.0544 very:0.0195.:0.0874 of:0.1377 and:0.0737)
		(,:0.0588 and:0.0732 was:0.0500 not:0.0574 very:0.0168,:0.1079 of:0.1216 I:0.0820)
		(,:0.0549 and:0.0703 am:0.0510 not:0.0588 friend:0.0190,:0.1279,:0.1167 I:0.1021)
		(,:0.0515 and:0.0608 am:0.0537 not:0.0615 friend:0.0214,:0.1387,:0.1309 I:0.1167)
 I
------
		(embedreportprint:0.0038��:0.0064'm:0.9258 a:0.9883 language:0.9219 model:0.6875,:1.0000 I:1.0000)
		( a:0.0393 and:0.0491 important:0.0164 �:0.0198 and:0.1670 of:0.1533 the:0.0557,:0.0698)
		( and:0.0364,:0.0476,:0.0549,:0.0282,:0.1377 in:0.1064 and:0.0522,:0.0679)
		( in:0.0248,:0.0309,:0.0376,:0.0150,:0.0840 in:0.0713 and:0.0284,:0.0405)
		( and:0.0236,:0.0254,:0.0258,:0.0087,:0.0540 in:0.0491 and:0.0208,:0.0302)
		( and:0.0277,:0.0276 also:0.0214 very:0.0081,:0.0410 in:0.0461 and:0.0259,:0.0322)
		( and:0.0393,:0.0369 also:0.0239 very:0.0126.:0.0449 of:0.0610 and:0.0403,:0.0449)
		( and:0.0537,:0.0486 not:0.0332 very:0.0183.:0.0630 of:0.0908 and:0.0576,:0.0593)
		( and:0.0630,:0.0505 not:0.0444 very:0.0215.:0.0811 of:0.1221 and:0.0679,:0.0605)
		( and:0.0684 was:0.0522 not:0.0544 very:0.0195.:0.0874 of:0.1377 and:0.0737 can:0.0635)
		( and:0.0732 was:0.0500 not:0.0574 very:0.0168,:0.1079 of:0.1216 I:0.0820 can:0.0674)
		( and:0.0703 am:0.0510 not:0.0588 friend:0.0190,:0.1279,:0.1167 I:0.1021'm:0.0776)
		( and:0.0608 am:0.0537 not:0.0615 friend:0.0214,:0.1387,:0.1309 I:0.1167'm:0.0889)
'm a student, I'm a student, a student
1250: sample 0: Hello, I'm a language model,
------
		(.:0.4824�:0.0043��:0.0073'm:0.9297 a:0.9883 language:0.9648 model:0.8203,:1.0000)
		(,:0.1475 in:0.0184 and:0.0537 new:0.0091,:0.0251,:0.2148 of:0.3379 the:0.0183)
		(,:0.1699 in:0.0233 and:0.0403,:0.0503,:0.0383,:0.1729 of:0.1328 in:0.0276)
		(,:0.0762 in:0.0139,:0.0217,:0.0289,:0.0149,:0.0879 of:0.1138 the:0.0140)
		(,:0.0464 the:0.0127,:0.0186,:0.0186,:0.0073,:0.0425 of:0.1001 the:0.0115)
		(,:0.0432 the:0.0172,:0.0247,:0.0166.:0.0069,:0.0291 of:0.1138 the:0.0145)
		(,:0.0493 the:0.0265,:0.0398,:0.0176 new:0.0093.:0.0330 of:0.1504 and:0.0309)
		(,:0.0547 the:0.0403,:0.0586,:0.0183 new:0.0129.:0.0513 of:0.2031 and:0.0581)
		(,:0.0596 the:0.0522,:0.0747,:0.0183 new:0.0162.:0.0728 of:0.2402 and:0.0938)
		(,:0.0615 the:0.0603,:0.0869,:0.0189 new:0.0176.:0.0952 of:0.2246 and:0.1221)
		(,:0.0601 the:0.0625,:0.0908,:0.0193 new:0.0170.:0.1104 of:0.1846 and:0.1289)
		(,:0.0571 the:0.0640,:0.0791,:0.0170 very:0.0166,:0.1089 of:0.1572 and:0.1216)
		(,:0.0540 the:0.0659 was:0.0791,:0.0152 very:0.0173,:0.1035 of:0.1387 and:0.1118)
 and
------
		(�:0.0043��:0.0073'm:0.9297 a:0.9883 language:0.9648 model:0.8203,:1.0000 and:1.0000)
		( in:0.0184 and:0.0537 new:0.0091,:0.0251,:0.2148 of:0.3379 the:0.0183 the:0.0850)
		( in:0.0233 and:0.0403,:0.0503,:0.0383,:0.1729 of:0.1328 in:0.0276 the:0.0410)
		( in:0.0139,:0.0217,:0.0289,:0.0149,:0.0879 of:0.1138 the:0.0140 the:0.0222)
		( the:0.0127,:0.0186,:0.0186,:0.0073,:0.0425 of:0.1001 the:0.0115 the:0.0154)
		( the:0.0172,:0.0247,:0.0166.:0.0069,:0.0291 of:0.1138 the:0.0145 the:0.0187)
		( the:0.0265,:0.0398,:0.0176 new:0.0093.:0.0330 of:0.1504 and:0.0309 the:0.0293)
		( the:0.0403,:0.0586,:0.0183 new:0.0129.:0.0513 of:0.2031 and:0.0581 the:0.0459)
		( the:0.0522,:0.0747,:0.0183 new:0.0162.:0.0728 of:0.2402 and:0.0938 the:0.0623)
		( the:0.0603,:0.0869,:0.0189 new:0.0176.:0.0952 of:0.2246 and:0.1221 a:0.0767)
		( the:0.0625,:0.0908,:0.0193 new:0.0170.:0.1104 of:0.1846 and:0.1289 a:0.0879)
		( the:0.0640,:0.0791,:0.0170 very:0.0166,:0.1089 of:0.1572 and:0.1216 a:0.0845)
		( the:0.0659 was:0.0791,:0.0152 very:0.0173,:0.1035 of:0.1387 and:0.1118 I:0.0869)
 I have a great deal with a lot of the more
1400: sample 0: Hello, I'm a language model,
------
		(.:0.6094�:0.0048��:0.0085'm:0.9648 a:0.9883 language:0.9805 model:0.8438,:1.0000)
		(,:0.1699 and:0.0154 and:0.0840 new:0.0144,:0.0396,:0.2422 of:0.2637 the:0.0128)
		(,:0.1738 and:0.0265 and:0.0608,:0.0535,:0.0535,:0.2109 in:0.1387 and:0.0237)
		(,:0.0796 and:0.0170 and:0.0344,:0.0364,:0.0281,:0.1206 of:0.0938 the:0.0128)
		(,:0.0466 and:0.0161,:0.0261,:0.0249,:0.0146,:0.0571 of:0.0747 the:0.0092)
		(,:0.0430 and:0.0211,:0.0322,:0.0227.:0.0110,:0.0386 of:0.0713 and:0.0140)
		(,:0.0498 and:0.0310,:0.0469,:0.0226.:0.0109,:0.0422 of:0.0840 and:0.0322)
		(,:0.0591 and:0.0422,:0.0645 not:0.0260 few:0.0170,:0.0640 of:0.1045 and:0.0688)
		(,:0.0703 and:0.0486,:0.0791 not:0.0403 few:0.0308.:0.1084.:0.1396 and:0.1074)
		(,:0.0786 and:0.0542,:0.0938 not:0.0566 few:0.0403.:0.1455.:0.1699 and:0.1279)
		(,:0.0859 the:0.0630,:0.1060 not:0.0718 few:0.0405.:0.1572.:0.1680 and:0.1147)
		(,:0.0894 the:0.0669,:0.0903 not:0.0815 few:0.0378,:0.1436.:0.1436 and:0.0903)
		(,:0.0913 the:0.0708�:0.0957 not:0.0894 few:0.0356 that:0.1348 that:0.1797 and:0.0713)
 and
------
		(�:0.0048��:0.0085'm:0.9648 a:0.9883 language:0.9805 model:0.8438,:1.0000 and:1.0000)
		( and:0.0154 and:0.0840 new:0.0144,:0.0396,:0.2422 of:0.2637 the:0.0128 the:0.0591)
		( and:0.0265 and:0.0608,:0.0535,:0.0535,:0.2109 in:0.1387 and:0.0237 the:0.0393)
		( and:0.0170 and:0.0344,:0.0364,:0.0281,:0.1206 of:0.0938 the:0.0128 the:0.0237)
		( and:0.0161,:0.0261,:0.0249,:0.0146,:0.0571 of:0.0747 the:0.0092 the:0.0167)
		( and:0.0211,:0.0322,:0.0227.:0.0110,:0.0386 of:0.0713 and:0.0140 the:0.0197)
		( and:0.0310,:0.0469,:0.0226.:0.0109,:0.0422 of:0.0840 and:0.0322 the:0.0315)
		( and:0.0422,:0.0645 not:0.0260 few:0.0170,:0.0640 of:0.1045 and:0.0688 the:0.0513)
		( and:0.0486,:0.0791 not:0.0403 few:0.0308.:0.1084.:0.1396 and:0.1074 the:0.0752)
		( and:0.0542,:0.0938 not:0.0566 few:0.0403.:0.1455.:0.1699 and:0.1279 the:0.0874)
		( the:0.0630,:0.1060 not:0.0718 few:0.0405.:0.1572.:0.1680 and:0.1147 a:0.0967)
		( the:0.0669,:0.0903 not:0.0815 few:0.0378,:0.1436.:0.1436 and:0.0903 a:0.0962)
		( the:0.0708�:0.0957 not:0.0894 few:0.0356 that:0.1348 that:0.1797 and:0.0713 I:0.0947)
 I'm not only a few years ago.
The
1550: sample 0: Hello, I'm a language model,
------
		(.:0.6875�:0.0049��:0.0085'm:0.9609 a:0.9883 language:0.9922 model:0.8750,:1.0000)
		(,:0.1514 and:0.0150 and:0.1040 new:0.0178 �:0.0334,:0.2598 of:0.2490 in:0.0106)
		(,:0.1631 and:0.0371 and:0.0732,:0.0635,:0.0405,:0.2422 of:0.1270 and:0.0297)
		(,:0.0854 and:0.0317 and:0.0491,:0.0457,:0.0233,:0.1270 of:0.0981 and:0.0161)
		(,:0.0510 and:0.0339 and:0.0371,:0.0304,:0.0114,:0.0559 of:0.0874 and:0.0167)
		(,:0.0396 and:0.0457 and:0.0383,:0.0255.:0.0070,:0.0449 of:0.0879 and:0.0398)
		(,:0.0396 and:0.0659,:0.0471,:0.0215 very:0.0089,:0.0522 of:0.0928 and:0.0952)
		(,:0.0403 and:0.0933.:0.0649 not:0.0294 bit:0.0171,:0.0674 for:0.1250 and:0.1494)
		(.:0.0620 and:0.1099.:0.0732 not:0.0444 bit:0.0349.:0.0913 for:0.1592 and:0.1631)
		(.:0.0874 and:0.1235.:0.0635 not:0.0544 bit:0.0500 that:0.1162 for:0.1719 and:0.1416)
		(.:0.1050 and:0.1260�:0.0474 sure:0.0820 bit:0.0569 that:0.1523 for:0.1650 and:0.1162)
		(.:0.1089 and:0.1123�:0.0522 sure:0.1001 bit:0.0579 that:0.1797 for:0.1631 but:0.1279)
		(.:0.1050 and:0.1011�:0.0554 sure:0.1104 bit:0.0557 that:0.2021 that:0.1650 but:0.1299)
 but
------
		(�:0.0049��:0.0085'm:0.9609 a:0.9883 language:0.9922 model:0.8750,:1.0000 but:1.0000)
		( and:0.0150 and:0.1040 new:0.0178 �:0.0334,:0.2598 of:0.2490 in:0.0106 the:0.0288)
		( and:0.0371 and:0.0732,:0.0635,:0.0405,:0.2422 of:0.1270 and:0.0297 the:0.0518)
		( and:0.0317 and:0.0491,:0.0457,:0.0233,:0.1270 of:0.0981 and:0.0161 the:0.0391)
		( and:0.0339 and:0.0371,:0.0304,:0.0114,:0.0559 of:0.0874 and:0.0167 the:0.0315)
		( and:0.0457 and:0.0383,:0.0255.:0.0070,:0.0449 of:0.0879 and:0.0398 the:0.0366)
		( and:0.0659,:0.0471,:0.0215 very:0.0089,:0.0522 of:0.0928 and:0.0952 it:0.0574)
		( and:0.0933.:0.0649 not:0.0294 bit:0.0171,:0.0674 for:0.1250 and:0.1494 it:0.1270)
		( and:0.1099.:0.0732 not:0.0444 bit:0.0349.:0.0913 for:0.1592 and:0.1631 it:0.2158)
		( and:0.1235.:0.0635 not:0.0544 bit:0.0500 that:0.1162 for:0.1719 and:0.1416 it:0.2715)
		( and:0.1260�:0.0474 sure:0.0820 bit:0.0569 that:0.1523 for:0.1650 and:0.1162 it:0.2695)
		( and:0.1123�:0.0522 sure:0.1001 bit:0.0579 that:0.1797 for:0.1631 but:0.1279 I:0.2832)
		( and:0.1011�:0.0554 sure:0.1104 bit:0.0557 that:0.2021 that:0.1650 but:0.1299 I:0.2871)
 I've been able to use the word "s"
1700: sample 0: Hello, I'm a language model,
------
		(.:0.7383�:0.0051��:0.0069'm:0.9648 a:0.9844 language:0.9961 model:0.8906,:1.0000)
		(,:0.2031 and:0.0383 and:0.1357 new:0.0267,:0.0325,:0.2891,:0.1973 in:0.0249)
		(,:0.1807 and:0.0623 and:0.1094,:0.0491,:0.0427,:0.2383,:0.1641 and:0.0522)
		(,:0.1060 and:0.0498 and:0.0703,:0.0325,:0.0320,:0.1177,:0.0801 and:0.0278)
		(,:0.0684 and:0.0544 and:0.0557,:0.0233,:0.0195,:0.0515 in:0.0488 and:0.0303)
		(,:0.0557 and:0.0684 and:0.0581 not:0.0256,:0.0113,:0.0422 in:0.0491 and:0.0723)
		(,:0.0532 and:0.0918 and:0.0659 not:0.0562 bit:0.0103,:0.0513 of:0.0620 and:0.1484)
		(,:0.0552 and:0.1152 and:0.0654 not:0.1108 bit:0.0238,:0.0708 for:0.0918 and:0.2021)
		(,:0.0574 and:0.1211�:0.0520 not:0.1689 bit:0.0432,:0.0850 for:0.1240 and:0.2129)
		(,:0.0615 and:0.1221�:0.0679 not:0.1846 bit:0.0576 that:0.1235 for:0.1406 and:0.1855)
		(,:0.0649 and:0.1250�:0.0767 not:0.1660 bit:0.0654 that:0.1660 of:0.1426 and:0.1533)
		(,:0.0688 and:0.1152�:0.0845 not:0.1504 bit:0.0654 that:0.1904 that:0.1523 and:0.1250)
		(,:0.0698 and:0.1011 have:0.0781 not:0.1357 bit:0.0630 that:0.2031 that:0.1738 and:0.1187)
 and
------
		(�:0.0051��:0.0069'm:0.9648 a:0.9844 language:0.9961 model:0.8906,:1.0000 and:1.0000)
		( and:0.0383 and:0.1357 new:0.0267,:0.0325,:0.2891,:0.1973 in:0.0249 the:0.0684)
		( and:0.0623 and:0.1094,:0.0491,:0.0427,:0.2383,:0.1641 and:0.0522 the:0.0679)
		( and:0.0498 and:0.0703,:0.0325,:0.0320,:0.1177,:0.0801 and:0.0278 the:0.0527)
		( and:0.0544 and:0.0557,:0.0233,:0.0195,:0.0515 in:0.0488 and:0.0303 the:0.0481)
		( and:0.0684 and:0.0581 not:0.0256,:0.0113,:0.0422 in:0.0491 and:0.0723 the:0.0554)
		( and:0.0918 and:0.0659 not:0.0562 bit:0.0103,:0.0513 of:0.0620 and:0.1484 the:0.0728)
		( and:0.1152 and:0.0654 not:0.1108 bit:0.0238,:0.0708 for:0.0918 and:0.2021 a:0.0972)
		( and:0.1211�:0.0520 not:0.1689 bit:0.0432,:0.0850 for:0.1240 and:0.2129 a:0.1328)
		( and:0.1221�:0.0679 not:0.1846 bit:0.0576 that:0.1235 for:0.1406 and:0.1855 a:0.1562)
		( and:0.1250�:0.0767 not:0.1660 bit:0.0654 that:0.1660 of:0.1426 and:0.1533 I:0.1982)
		( and:0.1152�:0.0845 not:0.1504 bit:0.0654 that:0.1904 that:0.1523 and:0.1250 I:0.2773)
		( and:0.1011 have:0.0781 not:0.1357 bit:0.0630 that:0.2031 that:0.1738 and:0.1187 I:0.2676)
 I'm a very useful tool for the development of the
1850: sample 0: Hello, I'm a language model,
------
		(.:0.8242�:0.0047groupon:0.0056'm:0.9531 a:0.9883 language:0.9961 model:0.8359,:1.0000)
		(,:0.2275 and:0.0334 and:0.1064 new:0.0248 �:0.0269,:0.2969,:0.2432 and:0.0400)
		(,:0.1865 and:0.0461 and:0.0801 �:0.0145,:0.0271,:0.2256,:0.1699 and:0.0486)
		(,:0.0972 and:0.0349 and:0.0564 ":0.0081,:0.0167,:0.0874,:0.0684 and:0.0293)
		(,:0.0598 and:0.0381 and:0.0464 more:0.0081,:0.0112,:0.0427 in:0.0508 and:0.0442)
		(,:0.0464 and:0.0488 and:0.0481 a:0.0103,:0.0081,:0.0356 in:0.0496 and:0.0923)
		(,:0.0461 and:0.0618 and:0.0469 a:0.0201 little:0.0112 that:0.0483 that:0.0586 and:0.1484)
		(,:0.0476 and:0.0664�:0.0549 a:0.0361 little:0.0214 that:0.0957 that:0.1021 and:0.1670)
		(,:0.0510 and:0.0613�:0.0850 a:0.0522 little:0.0344 that:0.1611 that:0.1543 and:0.1533)
		(,:0.0549 the:0.0669�:0.1060 a:0.0618 little:0.0415 that:0.2285 that:0.2051 which:0.1465)
		(,:0.0593 the:0.0732�:0.1172 a:0.0623 little:0.0393 that:0.2754 that:0.2490 which:0.1299)
		(.:0.0703 the:0.0757�:0.1279 a:0.0569 bit:0.0378 that:0.2871 that:0.2637 and:0.1094)
		(.:0.0791 the:0.0732�:0.1299 a:0.0522 good:0.0393 that:0.3027 that:0.2617 but:0.1113)
 but
------
		(�:0.0047groupon:0.0056'm:0.9531 a:0.9883 language:0.9961 model:0.8359,:1.0000 but:1.0000)
		( and:0.0334 and:0.1064 new:0.0248 �:0.0269,:0.2969,:0.2432 and:0.0400 the:0.0742)
		( and:0.0461 and:0.0801 �:0.0145,:0.0271,:0.2256,:0.1699 and:0.0486 the:0.0737)
		( and:0.0349 and:0.0564 ":0.0081,:0.0167,:0.0874,:0.0684 and:0.0293 the:0.0452)
		( and:0.0381 and:0.0464 more:0.0081,:0.0112,:0.0427 in:0.0508 and:0.0442 the:0.0371)
		( and:0.0488 and:0.0481 a:0.0103,:0.0081,:0.0356 in:0.0496 and:0.0923 the:0.0425)
		( and:0.0618 and:0.0469 a:0.0201 little:0.0112 that:0.0483 that:0.0586 and:0.1484 it:0.0588)
		( and:0.0664�:0.0549 a:0.0361 little:0.0214 that:0.0957 that:0.1021 and:0.1670 it:0.1270)
		( and:0.0613�:0.0850 a:0.0522 little:0.0344 that:0.1611 that:0.1543 and:0.1533 it:0.1846)
		( the:0.0669�:0.1060 a:0.0618 little:0.0415 that:0.2285 that:0.2051 which:0.1465 I:0.2695)
		( the:0.0732�:0.1172 a:0.0623 little:0.0393 that:0.2754 that:0.2490 which:0.1299 I:0.3105)
		( the:0.0757�:0.1279 a:0.0569 bit:0.0378 that:0.2871 that:0.2637 and:0.1094 I:0.3418)
		( the:0.0732�:0.1299 a:0.0522 good:0.0393 that:0.3027 that:0.2617 but:0.1113 I:0.3477)
 I'm a bit more than a few. I'm
2000: sample 0: Hello, I'm a language model,
------
		(.:0.8594�:0.0049groupon:0.0049'm:0.8750 a:0.9883 language:0.9961 model:0.7148,:1.0000)
		(,:0.2402
:0.0297 and:0.0552 �:0.0249 �:0.0298,:0.2246,:0.2090 and:0.0210)
		(,:0.1943 and:0.0464 and:0.0593 �:0.0176,:0.0269,:0.1875,:0.1377 and:0.0452)
		(,:0.1099 and:0.0342 and:0.0483,:0.0096,:0.0188,:0.0840,:0.0598 and:0.0270)
		(,:0.0747 and:0.0352 and:0.0457,:0.0115,:0.0139,:0.0415 in:0.0420 and:0.0378)
		(,:0.0610 and:0.0422 and:0.0496,:0.0148,:0.0107,:0.0320 in:0.0408 and:0.0781)
		(,:0.0576 and:0.0520 and:0.0522 a:0.0264.:0.0079,:0.0354,:0.0454 and:0.1172)
		(,:0.0588 and:0.0569,:0.0469 a:0.0452 little:0.0165,:0.0476,:0.0679 and:0.1377)
		(,:0.0593 and:0.0537�:0.0679 not:0.0757 little:0.0322 that:0.0718,:0.0928 and:0.1455)
		(,:0.0571 the:0.0522�:0.0903 not:0.1040 little:0.0469 that:0.1377.:0.1221 and:0.1416)
		(,:0.0574 the:0.0615�:0.1118 not:0.1152 little:0.0542 that:0.2109 that:0.1553 and:0.1357)
		(,:0.0547 the:0.0723�:0.1226 not:0.1138 little:0.0542 that:0.2598 that:0.1914 and:0.1196)
		(.:0.0574 the:0.0786�:0.1328 not:0.1104 little:0.0513 that:0.2969 that:0.2207 and:0.1113)
 and
------
		(�:0.0049groupon:0.0049'm:0.8750 a:0.9883 language:0.9961 model:0.7148,:1.0000 and:1.0000)
		(
:0.0297 and:0.0552 �:0.0249 �:0.0298,:0.2246,:0.2090 and:0.0210 in:0.0608)
		( and:0.0464 and:0.0593 �:0.0176,:0.0269,:0.1875,:0.1377 and:0.0452 the:0.0461)
		( and:0.0342 and:0.0483,:0.0096,:0.0188,:0.0840,:0.0598 and:0.0270 the:0.0354)
		( and:0.0352 and:0.0457,:0.0115,:0.0139,:0.0415 in:0.0420 and:0.0378 the:0.0344)
		( and:0.0422 and:0.0496,:0.0148,:0.0107,:0.0320 in:0.0408 and:0.0781 the:0.0405)
		( and:0.0520 and:0.0522 a:0.0264.:0.0079,:0.0354,:0.0454 and:0.1172 the:0.0547)
		( and:0.0569,:0.0469 a:0.0452 little:0.0165,:0.0476,:0.0679 and:0.1377 a:0.0771)
		( and:0.0537�:0.0679 not:0.0757 little:0.0322 that:0.0718,:0.0928 and:0.1455 I:0.1035)
		( the:0.0522�:0.0903 not:0.1040 little:0.0469 that:0.1377.:0.1221 and:0.1416 I:0.1943)
		( the:0.0615�:0.1118 not:0.1152 little:0.0542 that:0.2109 that:0.1553 and:0.1357 I:0.2500)
		( the:0.0723�:0.1226 not:0.1138 little:0.0542 that:0.2598 that:0.1914 and:0.1196 I:0.2715)
		( the:0.0786�:0.1328 not:0.1104 little:0.0513 that:0.2969 that:0.2207 and:0.1113 I:0.2715)
 I'm a language that's a language that's a
2150: sample 0: Hello, I'm a language model,
------
		(.:0.8750�:0.0049groupon:0.0049'm:0.6562 a:0.9883 language:0.9961 model:0.3848,:1.0000)
		(,:0.2734
:0.0192 and:0.0620 3:0.0130
:0.0239,:0.3828,:0.2207
:0.0371)
		(,:0.2119 and:0.0583 and:0.0742 ":0.0121,:0.0265,:0.2852,:0.1494 and:0.0713)
		(,:0.1211 and:0.0447 and:0.0698 in:0.0064,:0.0204,:0.1455 in:0.0737 and:0.0422)
		( and:0.0806 and:0.0403 and:0.0645 best:0.0073,:0.0167,:0.0664 of:0.0525 and:0.0369)
		( and:0.0659 and:0.0435 and:0.0693 a:0.0131,:0.0173,:0.0413 of:0.0598 and:0.0608)
		(,:0.0608 and:0.0493 and:0.0752 a:0.0298,:0.0168,:0.0374 of:0.0854 and:0.0962)
		(,:0.0649 and:0.0532 and:0.0762 a:0.0571 little:0.0146,:0.0491 of:0.1348 and:0.1235)
		(,:0.0703 and:0.0549�:0.0649 a:0.0850 little:0.0244 that:0.0938 of:0.1924 and:0.1523)
		(,:0.0747 and:0.0598�:0.0859 a:0.1040 little:0.0325 that:0.1689 of:0.2354 and:0.1748)
		(,:0.0767 and:0.0713�:0.1055 a:0.1050 good:0.0442 that:0.2520 of:0.2559 and:0.1797)
		(,:0.0801 and:0.0830�:0.1201 a:0.0991 good:0.0498 that:0.3066 of:0.2402 and:0.1611)
		(,:0.0825 and:0.0859�:0.1318 a:0.0903 good:0.0510 that:0.3438 that:0.2217 and:0.1475)
 and
------
		(�:0.0049groupon:0.0049'm:0.6562 a:0.9883 language:0.9961 model:0.3848,:1.0000 and:1.0000)
		(
:0.0192 and:0.0620 3:0.0130
:0.0239,:0.3828,:0.2207
:0.0371
:0.0845)
		( and:0.0583 and:0.0742 ":0.0121,:0.0265,:0.2852,:0.1494 and:0.0713 the:0.0459)
		( and:0.0447 and:0.0698 in:0.0064,:0.0204,:0.1455 in:0.0737 and:0.0422 the:0.0361)
		( and:0.0403 and:0.0645 best:0.0073,:0.0167,:0.0664 of:0.0525 and:0.0369 the:0.0376)
		( and:0.0435 and:0.0693 a:0.0131,:0.0173,:0.0413 of:0.0598 and:0.0608 the:0.0447)
		( and:0.0493 and:0.0752 a:0.0298,:0.0168,:0.0374 of:0.0854 and:0.0962 the:0.0591)
		( and:0.0532 and:0.0762 a:0.0571 little:0.0146,:0.0491 of:0.1348 and:0.1235 the:0.0728)
		( and:0.0549�:0.0649 a:0.0850 little:0.0244 that:0.0938 of:0.1924 and:0.1523 a:0.0928)
		( and:0.0598�:0.0859 a:0.1040 little:0.0325 that:0.1689 of:0.2354 and:0.1748 a:0.1128)
		( and:0.0713�:0.1055 a:0.1050 good:0.0442 that:0.2520 of:0.2559 and:0.1797 I:0.1289)
		( and:0.0830�:0.1201 a:0.0991 good:0.0498 that:0.3066 of:0.2402 and:0.1611 I:0.1748)
		( and:0.0859�:0.1318 a:0.0903 good:0.0510 that:0.3438 that:0.2217 and:0.1475 I:0.2178)
 I'm a very useful language. I'm a very
2300: sample 0: Hello, I'm a language model,
------
		(.:0.9062 Kinnikuman:0.0051groupon:0.0054'm:0.4023 a:0.9844 language:0.9922 model:0.4863,:1.0000)
		(,:0.3379
:0.0233
:0.0381
:0.0110
:0.0344,:0.3242,:0.1240
:0.0366)
		(,:0.2539 and:0.0454 and:0.0649 in:0.0171,:0.0255,:0.2451,:0.0864 and:0.0474)
		(,:0.1543 and:0.0386 and:0.0623 in:0.0095,:0.0165,:0.1084 in:0.0515 and:0.0253)
		( and:0.1055 and:0.0386 and:0.0598 in:0.0093,:0.0128,:0.0505 of:0.0391 and:0.0276)
		( and:0.0874 and:0.0469 and:0.0649 a:0.0170 type:0.0115,:0.0309 of:0.0474 and:0.0684)
		( and:0.0732 and:0.0554 and:0.0649 a:0.0258 type:0.0135,:0.0286 of:0.0654 and:0.1006)
		(,:0.0723 and:0.0540 and:0.0554 a:0.0330 very:0.0167 that:0.0403 of:0.0928 and:0.1108)
		(,:0.0718 and:0.0486�:0.0623 a:0.0342 very:0.0210 that:0.0593 of:0.1191 and:0.1289)
		(,:0.0728 and:0.0476�:0.0933 not:0.0371 very:0.0219 that:0.0698 of:0.1309 and:0.1465)
		(,:0.0737 and:0.0532�:0.1216 going:0.0486 bit:0.0278 that:0.0684 of:0.1279 and:0.1504)
		(,:0.0752 the:0.0608�:0.1328 going:0.0593 bit:0.0304 that:0.0654 of:0.1177 and:0.1445)
		(,:0.0767 the:0.0684�:0.1426 sure:0.0747 bit:0.0315 that:0.0603,:0.1230 and:0.1338)
 and
------
		( Kinnikuman:0.0051groupon:0.0054'm:0.4023 a:0.9844 language:0.9922 model:0.4863,:1.0000 and:1.0000)
		(
:0.0233
:0.0381
:0.0110
:0.0344,:0.3242,:0.1240
:0.0366
:0.0884)
		( and:0.0454 and:0.0649 in:0.0171,:0.0255,:0.2451,:0.0864 and:0.0474 the:0.0422)
		( and:0.0386 and:0.0623 in:0.0095,:0.0165,:0.1084 in:0.0515 and:0.0253 the:0.0309)
		( and:0.0386 and:0.0598 in:0.0093,:0.0128,:0.0505 of:0.0391 and:0.0276 the:0.0312)
		( and:0.0469 and:0.0649 a:0.0170 type:0.0115,:0.0309 of:0.0474 and:0.0684 the:0.0442)
		( and:0.0554 and:0.0649 a:0.0258 type:0.0135,:0.0286 of:0.0654 and:0.1006 the:0.0613)
		( and:0.0540 and:0.0554 a:0.0330 very:0.0167 that:0.0403 of:0.0928 and:0.1108 a:0.1094)
		( and:0.0486�:0.0623 a:0.0342 very:0.0210 that:0.0593 of:0.1191 and:0.1289 a:0.1582)
		( and:0.0476�:0.0933 not:0.0371 very:0.0219 that:0.0698 of:0.1309 and:0.1465 a:0.1553)
		( and:0.0532�:0.1216 going:0.0486 bit:0.0278 that:0.0684 of:0.1279 and:0.1504 I:0.1631)
		( the:0.0608�:0.1328 going:0.0593 bit:0.0304 that:0.0654 of:0.1177 and:0.1445 I:0.2070)
		( the:0.0684�:0.1426 sure:0.0747 bit:0.0315 that:0.0603,:0.1230 and:0.1338 I:0.2178)
 I'm a very interesting, but I'm not sure
2450: sample 0: Hello, I'm a language model,
------
		(.:0.9258�:0.0048groupon:0.0054's:0.4355 a:0.9844 language:0.9922 model:0.3594,:1.0000)
		(,:0.3047
:0.0393 and:0.0447 �:0.0117
:0.0417,:0.2441,:0.1260
:0.0464)
		(,:0.2021 and:0.0483 and:0.0557 in:0.0193,:0.0231,:0.1572,:0.0747 and:0.0659)
		( and:0.1289 and:0.0374 and:0.0471 in:0.0120,:0.0148,:0.0596 in:0.0403 and:0.0356)
		( and:0.0972 and:0.0371 and:0.0498 a:0.0179,:0.0126 in:0.0309 in:0.0315 and:0.0444)
		( and:0.0771 and:0.0400 and:0.0571 a:0.0272,:0.0111 in:0.0254 that:0.0447 and:0.0845)
		( the:0.0623 and:0.0364 and:0.0574 a:0.0352 type:0.0104 that:0.0410 that:0.0791 and:0.0864)
		( the:0.0564 the:0.0356 and:0.0493 not:0.0422 good:0.0164 that:0.0747 that:0.1221 and:0.0806)
		(,:0.0513 the:0.0386�:0.0439 not:0.0574 good:0.0242 that:0.1128 that:0.1641 and:0.0933)
		(,:0.0483 the:0.0471�:0.0583 not:0.0679 good:0.0287 that:0.1406 that:0.1963 and:0.1050)
		(.:0.0491 the:0.0593�:0.0688 not:0.0737 good:0.0288 that:0.1533 that:0.2119 and:0.1099)
		(.:0.0505 the:0.0674�:0.0771 not:0.0796 bit:0.0297 that:0.1514 that:0.2080 and:0.1060)
		(.:0.0498 the:0.0728�:0.0840 not:0.0850 bit:0.0295 that:0.1406 that:0.1943 and:0.1001)
 and
------
		(�:0.0048groupon:0.0054's:0.4355 a:0.9844 language:0.9922 model:0.3594,:1.0000 and:1.0000)
		(
:0.0393 and:0.0447 �:0.0117
:0.0417,:0.2441,:0.1260
:0.0464
:0.0771)
		( and:0.0483 and:0.0557 in:0.0193,:0.0231,:0.1572,:0.0747 and:0.0659 the:0.0359)
		( and:0.0374 and:0.0471 in:0.0120,:0.0148,:0.0596 in:0.0403 and:0.0356 the:0.0327)
		( and:0.0371 and:0.0498 a:0.0179,:0.0126 in:0.0309 in:0.0315 and:0.0444 the:0.0452)
		( and:0.0400 and:0.0571 a:0.0272,:0.0111 in:0.0254 that:0.0447 and:0.0845 the:0.0732)
		( and:0.0364 and:0.0574 a:0.0352 type:0.0104 that:0.0410 that:0.0791 and:0.0864 the:0.0996)
		( the:0.0356 and:0.0493 not:0.0422 good:0.0164 that:0.0747 that:0.1221 and:0.0806 the:0.1240)
		( the:0.0386�:0.0439 not:0.0574 good:0.0242 that:0.1128 that:0.1641 and:0.0933 a:0.1494)
		( the:0.0471�:0.0583 not:0.0679 good:0.0287 that:0.1406 that:0.1963 and:0.1050 I:0.1621)
		( the:0.0593�:0.0688 not:0.0737 good:0.0288 that:0.1533 that:0.2119 and:0.1099 I:0.2158)
		( the:0.0674�:0.0771 not:0.0796 bit:0.0297 that:0.1514 that:0.2080 and:0.1060 I:0.2539)
		( the:0.0728�:0.0840 not:0.0850 bit:0.0295 that:0.1406 that:0.1943 and:0.1001 I:0.2812)
 I'm a very different. I'm not a language
2600: sample 0: Hello, I'm a language model,
------
		(.:0.9336魔:0.0062groupon:0.0050 and:0.5117 a:0.9727 language:0.9922 model:0.4160,:1.0000)
		(,:0.3105
:0.0189
:0.0352
:0.0190
:0.0256,:0.2734,:0.1582
:0.0845)
		(,:0.1992 and:0.0474 and:0.0608 �:0.0129.:0.0150,:0.1621,:0.0747 and:0.0603)
		( and:0.1484 and:0.0320 and:0.0513 a:0.0086.:0.0104 and:0.0649 in:0.0361 and:0.0308)
		( and:0.1128 and:0.0299 and:0.0454 a:0.0164.:0.0096 and:0.0408 in:0.0304 and:0.0366)
		( and:0.0903 and:0.0369 and:0.0488 a:0.0306.:0.0093 and:0.0361 that:0.0396 and:0.0728)
		(,:0.0718 and:0.0408 and:0.0535 a:0.0457 very:0.0088 that:0.0408 that:0.0732 and:0.0903)
		(,:0.0728 and:0.0366 and:0.0547 a:0.0540 good:0.0153 that:0.0688 that:0.1201 and:0.0830)
		(,:0.0713 and:0.0332 and:0.0527 a:0.0564 bit:0.0292 that:0.1035 that:0.1611 and:0.0845)
		(,:0.0688 and:0.0344�:0.0742 not:0.0635 bit:0.0498 that:0.1357 that:0.1895 and:0.0913)
		(.:0.0771 and:0.0427�:0.1133 not:0.0684 bit:0.0664 that:0.1553 that:0.1934 and:0.0938)
		(.:0.0894 and:0.0520�:0.1572 not:0.0698 bit:0.0742 that:0.1572 that:0.1865 I:0.1128)
		(.:0.0991 and:0.0574�:0.2051 not:0.0718 bit:0.0767 that:0.1514,:0.1836 I:0.1572)
 I
------
		(魔:0.0062groupon:0.0050 and:0.5117 a:0.9727 language:0.9922 model:0.4160,:1.0000 I:1.0000)
		(
:0.0189
:0.0352
:0.0190
:0.0256,:0.2734,:0.1582
:0.0845
:0.0679)
		( and:0.0474 and:0.0608 �:0.0129.:0.0150,:0.1621,:0.0747 and:0.0603,:0.0344)
		( and:0.0320 and:0.0513 a:0.0086.:0.0104 and:0.0649 in:0.0361 and:0.0308,:0.0226)
		( and:0.0299 and:0.0454 a:0.0164.:0.0096 and:0.0408 in:0.0304 and:0.0366,:0.0247)
		( and:0.0369 and:0.0488 a:0.0306.:0.0093 and:0.0361 that:0.0396 and:0.0728 am:0.0311)
		( and:0.0408 and:0.0535 a:0.0457 very:0.0088 that:0.0408 that:0.0732 and:0.0903 am:0.0588)
		( and:0.0366 and:0.0547 a:0.0540 good:0.0153 that:0.0688 that:0.1201 and:0.0830 am:0.0967)
		( and:0.0332 and:0.0527 a:0.0564 bit:0.0292 that:0.1035 that:0.1611 and:0.0845 am:0.1377)
		( and:0.0344�:0.0742 not:0.0635 bit:0.0498 that:0.1357 that:0.1895 and:0.0913 am:0.1660)
		( and:0.0427�:0.1133 not:0.0684 bit:0.0664 that:0.1553 that:0.1934 and:0.0938 am:0.1719)
		( and:0.0520�:0.1572 not:0.0698 bit:0.0742 that:0.1572 that:0.1865 I:0.1128'm:0.1748)
		( and:0.0574�:0.2051 not:0.0718 bit:0.0767 that:0.1514,:0.1836 I:0.1572'm:0.1924)
'm a language that is used to describe the language of
2750: sample 0: Hello, I'm a language model,
------
		(.:0.9414�:0.0045groupon:0.0050 and:0.6133 a:0.9570 language:0.9844 model:0.2637,:1.0000)
		(,:0.2949
:0.0312 and:0.0435 present:0.0109
:0.0317,:0.2773,:0.1680
:0.0635)
		( and:0.2168 and:0.0554 and:0.0908 in:0.0217,:0.0232,:0.1680,:0.0928 and:0.0815)
		( and:0.1650 and:0.0410 and:0.0664 in:0.0126 range:0.0134,:0.0718 in:0.0552 and:0.0466)
		( and:0.1216 and:0.0405 and:0.0518 in:0.0140 reference:0.0118 in:0.0476 in:0.0500 and:0.0562)
		( and:0.0962 and:0.0427 and:0.0447 a:0.0244 more:0.0089,:0.0383 in:0.0466 and:0.0806)
		( and:0.0737 and:0.0371 and:0.0366 a:0.0371 few:0.0121,:0.0432 that:0.0645 and:0.0889)
		( the:0.0645 and:0.0291�:0.0342 a:0.0459 great:0.0167 that:0.0623 that:0.1064 and:0.0898)
		( the:0.0591 the:0.0293�:0.0537 not:0.0559 little:0.0256 that:0.0835 that:0.1514 and:0.0981)
		( the:0.0569 the:0.0344�:0.0708 not:0.0605 bit:0.0386 that:0.0913 that:0.1826 but:0.1328)
		( the:0.0547 the:0.0408�:0.0854 not:0.0620 bit:0.0444 that:0.0825 that:0.1904 but:0.1592)
		( the:0.0569 the:0.0491�:0.0938 sure:0.0747 bit:0.0444 that:0.0752 that:0.1895 but:0.1670)
		( the:0.0623 the:0.0559�:0.1025 sure:0.0854 bit:0.0403.:0.0713 that:0.1758 but:0.1670)
 but
------
		(�:0.0045groupon:0.0050 and:0.6133 a:0.9570 language:0.9844 model:0.2637,:1.0000 but:1.0000)
		(
:0.0312 and:0.0435 present:0.0109
:0.0317,:0.2773,:0.1680
:0.0635 and:0.0393)
		( and:0.0554 and:0.0908 in:0.0217,:0.0232,:0.1680,:0.0928 and:0.0815 and:0.0332)
		( and:0.0410 and:0.0664 in:0.0126 range:0.0134,:0.0718 in:0.0552 and:0.0466 the:0.0237)
		( and:0.0405 and:0.0518 in:0.0140 reference:0.0118 in:0.0476 in:0.0500 and:0.0562 the:0.0294)
		( and:0.0427 and:0.0447 a:0.0244 more:0.0089,:0.0383 in:0.0466 and:0.0806 the:0.0405)
		( and:0.0371 and:0.0366 a:0.0371 few:0.0121,:0.0432 that:0.0645 and:0.0889 it:0.0669)
		( and:0.0291�:0.0342 a:0.0459 great:0.0167 that:0.0623 that:0.1064 and:0.0898 I:0.1191)
		( the:0.0293�:0.0537 not:0.0559 little:0.0256 that:0.0835 that:0.1514 and:0.0981 I:0.2314)
		( the:0.0344�:0.0708 not:0.0605 bit:0.0386 that:0.0913 that:0.1826 but:0.1328 I:0.3105)
		( the:0.0408�:0.0854 not:0.0620 bit:0.0444 that:0.0825 that:0.1904 but:0.1592 I:0.3789)
		( the:0.0491�:0.0938 sure:0.0747 bit:0.0444 that:0.0752 that:0.1895 but:0.1670 I:0.4512)
		( the:0.0559�:0.1025 sure:0.0854 bit:0.0403.:0.0713 that:0.1758 but:0.1670 I:0.4688)
 I'm not sure I'm sure I'm sure I
2900: sample 0: Hello, I'm a language model,
------
		(.:0.9219 Kinnikuman:0.0047groupon:0.0056 and:0.6914 a:0.9414 language:0.9805 model:0.2480,:1.0000)
		(,:0.3633
:0.0121 ':0.0094 �:0.0078
:0.0540,:0.1719,:0.0981
:0.0427)
		(,:0.2520 and:0.0388 and:0.0530 in:0.0187.:0.0243,:0.1357,:0.0737 and:0.0703)
		( and:0.1680 and:0.0320 and:0.0469 in:0.0112.:0.0165 in:0.0618 in:0.0356 and:0.0400)
		( and:0.1260 and:0.0352 and:0.0432 in:0.0143.:0.0137 in:0.0469 in:0.0349 and:0.0515)
		( and:0.0972 and:0.0393 and:0.0432 not:0.0195.:0.0123 in:0.0371 that:0.0525 and:0.0864)
		( and:0.0742 and:0.0376 and:0.0437 not:0.0381 good:0.0156 that:0.0374 that:0.0952 and:0.0967)
		(,:0.0688 and:0.0311,:0.0444 not:0.0569 good:0.0352 that:0.0608 that:0.1465 and:0.0991)
		(,:0.0640 and:0.0269 will:0.0469 going:0.0693 bit:0.0630 that:0.0854 that:0.1953 and:0.1064)
		(,:0.0613 and:0.0317 will:0.0640 going:0.1011 bit:0.1055 that:0.1011 that:0.2207 but:0.1416)
		(.:0.0620 and:0.0505 will:0.0679 going:0.1260 bit:0.1357 that:0.1060 that:0.2197 but:0.1494)
		(.:0.0664 and:0.0845 will:0.0576 going:0.1416 bit:0.1445 that:0.1040.:0.2051 I:0.1426)
		(.:0.0728 and:0.1064 have:0.0659 going:0.1494 bit:0.1455 that:0.1001.:0.2188 I:0.1895)
 I
------
		( Kinnikuman:0.0047groupon:0.0056 and:0.6914 a:0.9414 language:0.9805 model:0.2480,:1.0000 I:1.0000)
		(
:0.0121 ':0.0094 �:0.0078
:0.0540,:0.1719,:0.0981
:0.0427
:0.0481)
		( and:0.0388 and:0.0530 in:0.0187.:0.0243,:0.1357,:0.0737 and:0.0703,:0.0405)
		( and:0.0320 and:0.0469 in:0.0112.:0.0165 in:0.0618 in:0.0356 and:0.0400,:0.0287)
		( and:0.0352 and:0.0432 in:0.0143.:0.0137 in:0.0469 in:0.0349 and:0.0515,:0.0325)
		( and:0.0393 and:0.0432 not:0.0195.:0.0123 in:0.0371 that:0.0525 and:0.0864,:0.0388)
		( and:0.0376 and:0.0437 not:0.0381 good:0.0156 that:0.0374 that:0.0952 and:0.0967 am:0.0635)
		( and:0.0311,:0.0444 not:0.0569 good:0.0352 that:0.0608 that:0.1465 and:0.0991 am:0.1021)
		( and:0.0269 will:0.0469 going:0.0693 bit:0.0630 that:0.0854 that:0.1953 and:0.1064 am:0.1348)
		( and:0.0317 will:0.0640 going:0.1011 bit:0.1055 that:0.1011 that:0.2207 but:0.1416 am:0.1455)
		( and:0.0505 will:0.0679 going:0.1260 bit:0.1357 that:0.1060 that:0.2197 but:0.1494'm:0.1387)
		( and:0.0845 will:0.0576 going:0.1416 bit:0.1445 that:0.1040.:0.2051 I:0.1426'm:0.1514)
		( and:0.1064 have:0.0659 going:0.1494 bit:0.1455 that:0.1001.:0.2188 I:0.1895'm:0.1494)
'm a computer programmer. I'm a computer programmer.
3050: sample 0: Hello, I'm a language model,
------
		(.:0.9414�:0.0041groupon:0.0050 and:0.7461 a:0.8945 language:0.9883 model:0.1138,:1.0000)
		(,:0.3652
:0.0188
:0.0111
:0.0116
:0.0457,:0.1562,:0.1455
:0.0581)
		(,:0.2490 and:0.0291 and:0.0376 in:0.0099,:0.0142,:0.0884,:0.0884 and:0.0530)
		( and:0.1680 and:0.0210 and:0.0269 in:0.0092 more:0.0106 in:0.0447 in:0.0378 and:0.0344)
		( and:0.1230 and:0.0236 and:0.0259 in:0.0131 more:0.0134 in:0.0344 in:0.0344 and:0.0432)
		( and:0.0962 and:0.0305 and:0.0276 a:0.0145 more:0.0145 in:0.0282 in:0.0386 and:0.0845)
		( and:0.0801 and:0.0344 and:0.0256 not:0.0223 more:0.0143 that:0.0364 of:0.0498 and:0.1001)
		(,:0.0752 and:0.0342 will:0.0249 not:0.0315 bit:0.0162 that:0.0591 for:0.0752 and:0.1230)
		(,:0.0796 and:0.0347 will:0.0349 not:0.0427 bit:0.0381 that:0.0811 for:0.0947 and:0.1318)
		(,:0.0815 and:0.0393 will:0.0437 not:0.0559 bit:0.0669 that:0.0942,:0.1079 and:0.1416)
		(,:0.0850 and:0.0498�:0.0481 not:0.0659 bit:0.0913 that:0.1006,:0.1230 and:0.1484)
		(.:0.0928 and:0.0623 have:0.0547 not:0.0757 bit:0.1055 that:0.0962,:0.1338 but:0.1494)
		(.:0.1069 and:0.0747 have:0.0654 sure:0.0972 bit:0.1113 that:0.0884.:0.1572 but:0.1631)
 but
------
		(�:0.0041groupon:0.0050 and:0.7461 a:0.8945 language:0.9883 model:0.1138,:1.0000 but:0.9961)
		(
:0.0188
:0.0111
:0.0116
:0.0457,:0.1562,:0.1455
:0.0581 and:0.0547)
		( and:0.0291 and:0.0376 in:0.0099,:0.0142,:0.0884,:0.0884 and:0.0530 and:0.0322)
		( and:0.0210 and:0.0269 in:0.0092 more:0.0106 in:0.0447 in:0.0378 and:0.0344 in:0.0209)
		( and:0.0236 and:0.0259 in:0.0131 more:0.0134 in:0.0344 in:0.0344 and:0.0432 in:0.0214)
		( and:0.0305 and:0.0276 a:0.0145 more:0.0145 in:0.0282 in:0.0386 and:0.0845 the:0.0292)
		( and:0.0344 and:0.0256 not:0.0223 more:0.0143 that:0.0364 of:0.0498 and:0.1001 a:0.0425)
		( and:0.0342 will:0.0249 not:0.0315 bit:0.0162 that:0.0591 for:0.0752 and:0.1230 it:0.0747)
		( and:0.0347 will:0.0349 not:0.0427 bit:0.0381 that:0.0811 for:0.0947 and:0.1318 it:0.1089)
		( and:0.0393 will:0.0437 not:0.0559 bit:0.0669 that:0.0942,:0.1079 and:0.1416 it:0.1338)
		( and:0.0498�:0.0481 not:0.0659 bit:0.0913 that:0.1006,:0.1230 and:0.1484 I:0.1709)
		( and:0.0623 have:0.0547 not:0.0757 bit:0.1055 that:0.0962,:0.1338 but:0.1494 I:0.2090)
		( and:0.0747 have:0.0654 sure:0.0972 bit:0.1113 that:0.0884.:0.1572 but:0.1631 I:0.2422)
 I'm not sure I've got a lot of ideas
3200: sample 0: Hello, I'm a language model,
------
		(.:0.9492�:0.0042groupon:0.0047 and:0.7578 a:0.8438 language:0.9844 model:0.0654,:1.0000)
		(,:0.2949
:0.0112ces:0.0176 heart:0.0063
:0.0124,:0.0972,:0.0908
:0.0239)
		( and:0.2363 and:0.0332 and:0.0276 in:0.0135.:0.0130,:0.0757,:0.0598 and:0.0688)
		( and:0.1699 and:0.0281 and:0.0205 in:0.0072 more:0.0112 in:0.0464 in:0.0337 and:0.0461)
		( and:0.1240 and:0.0293 and:0.0189 in:0.0115 more:0.0167 in:0.0398 in:0.0393 and:0.0623)
		( and:0.0996 and:0.0315 and:0.0184 a:0.0198 more:0.0229 in:0.0342 that:0.0554 and:0.0913)
		( and:0.0811 and:0.0304.:0.0203 not:0.0284 more:0.0236 that:0.0359 that:0.0938 and:0.0991)
		( and:0.0635 the:0.0292�:0.0269 not:0.0398 great:0.0225 that:0.0554 that:0.1406 and:0.1138)
		(,:0.0618 the:0.0381 can:0.0422 not:0.0532 great:0.0287 that:0.0688 that:0.1816 and:0.1260)
		(,:0.0620 the:0.0505 can:0.0640 not:0.0654 great:0.0299 that:0.0664 that:0.2021 and:0.1357)
		(,:0.0635 the:0.0669 can:0.0791 not:0.0786 bit:0.0339 that:0.0527 that:0.2031 and:0.1338)
		(,:0.0679 and:0.0840 can:0.0786 not:0.0918 bit:0.0376 professor:0.0659 that:0.1846 and:0.1196)
		(.:0.0791 and:0.1001 can:0.0664 not:0.1060 bit:0.0381 professor:0.0781 that:0.1699 I:0.1455)
 I
------
		(�:0.0042groupon:0.0047 and:0.7578 a:0.8438 language:0.9844 model:0.0654,:1.0000 I:1.0000)
		(
:0.0112ces:0.0176 heart:0.0063
:0.0124,:0.0972,:0.0908
:0.0239
:0.0359)
		( and:0.0332 and:0.0276 in:0.0135.:0.0130,:0.0757,:0.0598 and:0.0688.:0.0219)
		( and:0.0281 and:0.0205 in:0.0072 more:0.0112 in:0.0464 in:0.0337 and:0.0461.:0.0200)
		( and:0.0293 and:0.0189 in:0.0115 more:0.0167 in:0.0398 in:0.0393 and:0.0623.:0.0182)
		( and:0.0315 and:0.0184 a:0.0198 more:0.0229 in:0.0342 that:0.0554 and:0.0913 am:0.0325)
		( and:0.0304.:0.0203 not:0.0284 more:0.0236 that:0.0359 that:0.0938 and:0.0991 am:0.0527)
		( the:0.0292�:0.0269 not:0.0398 great:0.0225 that:0.0554 that:0.1406 and:0.1138 am:0.0791)
		( the:0.0381 can:0.0422 not:0.0532 great:0.0287 that:0.0688 that:0.1816 and:0.1260'm:0.1357)
		( the:0.0505 can:0.0640 not:0.0654 great:0.0299 that:0.0664 that:0.2021 and:0.1357'm:0.2217)
		( the:0.0669 can:0.0791 not:0.0786 bit:0.0339 that:0.0527 that:0.2031 and:0.1338'm:0.2969)
		( and:0.0840 can:0.0786 not:0.0918 bit:0.0376 professor:0.0659 that:0.1846 and:0.1196'm:0.3242)
		( and:0.1001 can:0.0664 not:0.1060 bit:0.0381 professor:0.0781 that:0.1699 I:0.1455'm:0.3223)
'm a language model.
I'm a language model
3350: sample 0: Hello, I'm a language model,
------
		(.:0.9531�:0.0043groupon:0.0052 and:0.8125 a:0.7656 language:0.9805 model:0.0229,:1.0000)
		(,:0.3086 leading:0.0081ces:0.0320 present:0.0101
:0.0271,:0.0767,:0.0718
:0.0659)
		( and:0.2188 and:0.0278 and:0.0234 present:0.0110,:0.0165,:0.0654,:0.0564 and:0.0549)
		( and:0.1592 and:0.0275 in:0.0233 ":0.0070.:0.0126 in:0.0425 in:0.0356 and:0.0391)
		( and:0.1182 and:0.0322 and:0.0282 best:0.0111 more:0.0150 in:0.0364 of:0.0342 and:0.0566)
		( and:0.0942 and:0.0378 and:0.0344 a:0.0148 more:0.0134 that:0.0325 of:0.0471 and:0.1045)
		( and:0.0747 and:0.0369 and:0.0369 a:0.0227 very:0.0104 that:0.0530 that:0.0776 and:0.1377)
		(,:0.0571 and:0.0308 will:0.0454 a:0.0330 very:0.0134 that:0.0908 that:0.1318 and:0.1631)
		(,:0.0542 the:0.0287 will:0.0635 a:0.0481 very:0.0140 that:0.1260 that:0.1816 and:0.1680)
		(,:0.0510 and:0.0356 will:0.0718 a:0.0615 student:0.0193 that:0.1387 that:0.2080 and:0.1689)
		(.:0.0569 and:0.0574 will:0.0684 a:0.0669 student:0.0254 that:0.1328 that:0.2285 and:0.1650)
		(.:0.0640 and:0.0859 would:0.0747 not:0.0752 professor:0.0297 that:0.1167 that:0.2197 I:0.1484)
		(.:0.0718 and:0.0952 have:0.0850 not:0.0884 professor:0.0320 that:0.1064 that:0.2129 I:0.1738)
 I
------
		(�:0.0043groupon:0.0052 and:0.8125 a:0.7656 language:0.9805 model:0.0229,:1.0000 I:1.0000)
		( leading:0.0081ces:0.0320 present:0.0101
:0.0271,:0.0767,:0.0718
:0.0659
:0.0371)
		( and:0.0278 and:0.0234 present:0.0110,:0.0165,:0.0654,:0.0564 and:0.0549,:0.0267)
		( and:0.0275 in:0.0233 ":0.0070.:0.0126 in:0.0425 in:0.0356 and:0.0391,:0.0222)
		( and:0.0322 and:0.0282 best:0.0111 more:0.0150 in:0.0364 of:0.0342 and:0.0566,:0.0273)
		( and:0.0378 and:0.0344 a:0.0148 more:0.0134 that:0.0325 of:0.0471 and:0.1045 have:0.0413)
		( and:0.0369 and:0.0369 a:0.0227 very:0.0104 that:0.0530 that:0.0776 and:0.1377 have:0.0576)
		( and:0.0308 will:0.0454 a:0.0330 very:0.0134 that:0.0908 that:0.1318 and:0.1631 am:0.0806)
		( the:0.0287 will:0.0635 a:0.0481 very:0.0140 that:0.1260 that:0.1816 and:0.1680 am:0.0972)
		( and:0.0356 will:0.0718 a:0.0615 student:0.0193 that:0.1387 that:0.2080 and:0.1689'm:0.1650)
		( and:0.0574 will:0.0684 a:0.0669 student:0.0254 that:0.1328 that:0.2285 and:0.1650'm:0.2383)
		( and:0.0859 would:0.0747 not:0.0752 professor:0.0297 that:0.1167 that:0.2197 I:0.1484'm:0.2695)
		( and:0.0952 have:0.0850 not:0.0884 professor:0.0320 that:0.1064 that:0.2129 I:0.1738'm:0.2656)
'm a language model. I'm a language model.
3500: sample 0: Hello, I'm a language model,
------
		(.:0.9531��:0.0044groupon:0.0049 and:0.8320 a:0.6094 language:0.9727 model:0.0066,:1.0000)
		(,:0.3262 leading:0.0042ces:0.0289ces:0.0061
:0.0120,:0.0415,:0.0938
:0.0322)
		( and:0.2334 and:0.0288 and:0.0211,:0.0119,:0.0150,:0.0469,:0.0591 and:0.0520)
		( and:0.1797 and:0.0287 and:0.0166 3:0.0065.:0.0100 in:0.0427 in:0.0371 and:0.0493)
		( and:0.1416 and:0.0369 and:0.0160 best:0.0088.:0.0103 in:0.0388 of:0.0376 and:0.0684)
		( and:0.1162 and:0.0439 and:0.0162 a:0.0129.:0.0084,:0.0322 of:0.0527 and:0.1211)
		( and:0.0908 and:0.0442.:0.0181 a:0.0165.:0.0076,:0.0408 of:0.0723 and:0.1582)
		( and:0.0718 and:0.0388�:0.0294 a:0.0183 few:0.0098,:0.0520.:0.0928 and:0.1855)
		(,:0.0640 and:0.0378�:0.0581 a:0.0190 few:0.0126,:0.0569.:0.1299 and:0.1768)
		(,:0.0635 and:0.0430�:0.0977 not:0.0226 bit:0.0216.:0.0588.:0.1592 and:0.1650)
		(,:0.0664 and:0.0566�:0.1338 sure:0.0435 bit:0.0332.:0.0640.:0.1836 and:0.1455)
		(,:0.0747 and:0.0742�:0.1533 sure:0.0708 bit:0.0430.:0.0728.:0.2061 and:0.1128)
		(,:0.0845 and:0.0815�:0.1621 sure:0.0962 bit:0.0505.:0.0894.:0.2148 I:0.0981)
 I
------
		(��:0.0044groupon:0.0049 and:0.8320 a:0.6094 language:0.9727 model:0.0066,:1.0000 I:1.0000)
		( leading:0.0042ces:0.0289ces:0.0061
:0.0120,:0.0415,:0.0938
:0.0322
:0.0309)
		( and:0.0288 and:0.0211,:0.0119,:0.0150,:0.0469,:0.0591 and:0.0520.:0.0234)
		( and:0.0287 and:0.0166 3:0.0065.:0.0100 in:0.0427 in:0.0371 and:0.0493.:0.0236)
		( and:0.0369 and:0.0160 best:0.0088.:0.0103 in:0.0388 of:0.0376 and:0.0684 have:0.0256)
		( and:0.0439 and:0.0162 a:0.0129.:0.0084,:0.0322 of:0.0527 and:0.1211 have:0.0347)
		( and:0.0442.:0.0181 a:0.0165.:0.0076,:0.0408 of:0.0723 and:0.1582 have:0.0442)
		( and:0.0388�:0.0294 a:0.0183 few:0.0098,:0.0520.:0.0928 and:0.1855 am:0.0635)
		( and:0.0378�:0.0581 a:0.0190 few:0.0126,:0.0569.:0.1299 and:0.1768'm:0.1113)
		( and:0.0430�:0.0977 not:0.0226 bit:0.0216.:0.0588.:0.1592 and:0.1650'm:0.2041)
		( and:0.0566�:0.1338 sure:0.0435 bit:0.0332.:0.0640.:0.1836 and:0.1455'm:0.2773)
		( and:0.0742�:0.1533 sure:0.0708 bit:0.0430.:0.0728.:0.2061 and:0.1128'm:0.2930)
		( and:0.0815�:0.1621 sure:0.0962 bit:0.0505.:0.0894.:0.2148 I:0.0981'm:0.2930)
'm a language that's not a language.
The
3650: sample 0: Hello, I'm a language model,
------
		(.:0.9609の魔:0.0040groupon:0.0044 and:0.8516 a:0.4922 language:0.9688 model:0.0035,:1.0000)
		(,:0.2559 leading:0.0090ces:0.0265 present:0.0083
:0.0369,:0.1064 of:0.0498 and:0.0569)
		( and:0.2256 and:0.0210 and:0.0214 in:0.0112 and:0.0120,:0.0703 in:0.0398 and:0.0510)
		( and:0.1631 and:0.0212 in:0.0216 in:0.0095 more:0.0127 in:0.0422 in:0.0334 and:0.0442)
		( and:0.1250 and:0.0251 in:0.0229 in:0.0126 more:0.0177 in:0.0366 in:0.0405 and:0.0562)
		( and:0.0977 and:0.0267 and:0.0229 a:0.0152 more:0.0177 that:0.0356 in:0.0491 and:0.0728)
		( and:0.0757 and:0.0260 and:0.0261 a:0.0201 more:0.0143 that:0.0549 that:0.0664 and:0.0923)
		(,:0.0630 and:0.0225 and:0.0277 not:0.0322 very:0.0110 that:0.0815 that:0.0947 and:0.1250)
		(,:0.0588 the:0.0222 will:0.0337 going:0.0466 very:0.0126 that:0.1050 that:0.1147 and:0.1445)
		(,:0.0544 the:0.0287 will:0.0479 going:0.0640 good:0.0136 that:0.1123.:0.1216 and:0.1621)
		(,:0.0549 the:0.0381�:0.0640 going:0.0732 believer:0.0181 that:0.0991.:0.1406 and:0.1660)
		(,:0.0586 and:0.0522�:0.0908 not:0.0801 believer:0.0227 that:0.0825.:0.1533 and:0.1562)
		(,:0.0669 and:0.0684�:0.1084 not:0.0923 believer:0.0236 that:0.0708.:0.1592 and:0.1367)
 and
------
		(の魔:0.0040groupon:0.0044 and:0.8516 a:0.4922 language:0.9688 model:0.0035,:1.0000 and:1.0000)
		( leading:0.0090ces:0.0265 present:0.0083
:0.0369,:0.1064 of:0.0498 and:0.0569 and:0.1768)
		( and:0.0210 and:0.0214 in:0.0112 and:0.0120,:0.0703 in:0.0398 and:0.0510 and:0.0449)
		( and:0.0212 in:0.0216 in:0.0095 more:0.0127 in:0.0422 in:0.0334 and:0.0442 the:0.0304)
		( and:0.0251 in:0.0229 in:0.0126 more:0.0177 in:0.0366 in:0.0405 and:0.0562 the:0.0527)
		( and:0.0267 and:0.0229 a:0.0152 more:0.0177 that:0.0356 in:0.0491 and:0.0728 the:0.0742)
		( and:0.0260 and:0.0261 a:0.0201 more:0.0143 that:0.0549 that:0.0664 and:0.0923 the:0.0923)
		( and:0.0225 and:0.0277 not:0.0322 very:0.0110 that:0.0815 that:0.0947 and:0.1250 the:0.1060)
		( the:0.0222 will:0.0337 going:0.0466 very:0.0126 that:0.1050 that:0.1147 and:0.1445 the:0.0908)
		( the:0.0287 will:0.0479 going:0.0640 good:0.0136 that:0.1123.:0.1216 and:0.1621 I:0.1260)
		( the:0.0381�:0.0640 going:0.0732 believer:0.0181 that:0.0991.:0.1406 and:0.1660 I:0.1768)
		( and:0.0522�:0.0908 not:0.0801 believer:0.0227 that:0.0825.:0.1533 and:0.1562 I:0.2305)
		( and:0.0684�:0.1084 not:0.0923 believer:0.0236 that:0.0708.:0.1592 and:0.1367 I:0.2773)
 I'm a language model.
- I'm a
3800: sample 0: Hello, I'm a language model,
------
		(.:0.9648�:0.0039groupon:0.0047 and:0.8438 of:0.5352 language:0.9609:0.0032,:1.0000)
		(,:0.2852 leading:0.0081ces:0.0374 heart:0.0038
:0.0162,:0.1187,:0.0845 and:0.0457)
		( and:0.2539 and:0.0192 and:0.0237 and:0.0104 and:0.0121 in:0.0747 and:0.0583 and:0.0498)
		( and:0.1875 and:0.0203 and:0.0204 in:0.0052 more:0.0076 in:0.0525 of:0.0425 and:0.0491)
		( and:0.1426 and:0.0256 and:0.0217 a:0.0082 more:0.0117 in:0.0449 of:0.0581 and:0.0747)
		( and:0.1104 and:0.0300 and:0.0236 a:0.0139 more:0.0103 in:0.0371 of:0.0762 and:0.1270)
		( and:0.0835 and:0.0325 and:0.0238 a:0.0172 more:0.0070,:0.0364 of:0.0977 and:0.1777)
		( and:0.0635 and:0.0308 was:0.0405 going:0.0219 very:0.0086,:0.0466 of:0.1050 and:0.2402)
		(,:0.0557 and:0.0325 was:0.0593 going:0.0310 bit:0.0118 that:0.0598,:0.1260 and:0.2451)
		(,:0.0581 and:0.0444 was:0.0664 sure:0.0500 bit:0.0198 that:0.0767,:0.1465 and:0.2373)
		(,:0.0635 and:0.0613 was:0.0640 sure:0.1079 bit:0.0258 that:0.0825.:0.1816 and:0.2285)
		(,:0.0757 and:0.0786�:0.0791 sure:0.1807 bit:0.0284 that:0.0801.:0.2100 and:0.1963)
		(.:0.0928 and:0.0825�:0.1050 sure:0.2295 bit:0.0294 that:0.0776.:0.2266 and:0.1592)
 and
------
		(�:0.0039groupon:0.0047 and:0.8438 of:0.5352 language:0.9609:0.0032,:1.0000 and:1.0000)
		( leading:0.0081ces:0.0374 heart:0.0038
:0.0162,:0.1187,:0.0845 and:0.0457 and:0.1807)
		( and:0.0192 and:0.0237 and:0.0104 and:0.0121 in:0.0747 and:0.0583 and:0.0498 and:0.0471)
		( and:0.0203 and:0.0204 in:0.0052 more:0.0076 in:0.0525 of:0.0425 and:0.0491 the:0.0264)
		( and:0.0256 and:0.0217 a:0.0082 more:0.0117 in:0.0449 of:0.0581 and:0.0747 the:0.0547)
		( and:0.0300 and:0.0236 a:0.0139 more:0.0103 in:0.0371 of:0.0762 and:0.1270 the:0.0757)
		( and:0.0325 and:0.0238 a:0.0172 more:0.0070,:0.0364 of:0.0977 and:0.1777 the:0.0908)
		( and:0.0308 was:0.0405 going:0.0219 very:0.0086,:0.0466 of:0.1050 and:0.2402 the:0.0962)
		( and:0.0325 was:0.0593 going:0.0310 bit:0.0118 that:0.0598,:0.1260 and:0.2451 I:0.1128)
		( and:0.0444 was:0.0664 sure:0.0500 bit:0.0198 that:0.0767,:0.1465 and:0.2373 I:0.1553)
		( and:0.0613 was:0.0640 sure:0.1079 bit:0.0258 that:0.0825.:0.1816 and:0.2285 I:0.2051)
		( and:0.0786�:0.0791 sure:0.1807 bit:0.0284 that:0.0801.:0.2100 and:0.1963 I:0.2695)
		( and:0.0825�:0.1050 sure:0.2295 bit:0.0294 that:0.0776.:0.2266 and:0.1592 I:0.3184)
 I'm a language model.
The first thing I
3950: sample 0: Hello, I'm a language model,
------
		(.:0.9727の魔:0.0040groupon:0.0046 and:0.8320 of:0.5898 language:0.9609wcsstore:0.0033,:1.0000)
		(,:0.3887
:0.0080ces:0.0195 growing:0.0050
:0.0535,:0.1348,:0.1157
:0.0830)
		(,:0.2070 and:0.0153 and:0.0187 and:0.0070 and:0.0128,:0.0579,:0.0688 and:0.0452)
		( and:0.1504 and:0.0171 and:0.0197 in:0.0054 more:0.0118,:0.0327,:0.0425 and:0.0376)
		( and:0.1167 and:0.0229 and:0.0249 more:0.0087 more:0.0160,:0.0337 of:0.0576 and:0.0537)
		( and:0.0884 and:0.0281 and:0.0254 going:0.0208 more:0.0156,:0.0410 of:0.0850 and:0.0903)
		( and:0.0659 and:0.0312�:0.0293 going:0.0569 good:0.0148,:0.0540 of:0.1055 and:0.1182)
		(,:0.0508 and:0.0317�:0.0439 going:0.1006 good:0.0282,:0.0591 that:0.1289 and:0.1377)
		(,:0.0464 and:0.0359�:0.0608 going:0.1279 lot:0.0420 that:0.0588 that:0.1689 and:0.1562)
		(,:0.0459 and:0.0427�:0.0737 going:0.1318 lot:0.0518 that:0.0608 that:0.1875 and:0.1602)
		(.:0.0540 and:0.0547�:0.0742 going:0.1138 lot:0.0486-:0.0559 that:0.1885 and:0.1504)
		(.:0.0693 and:0.0708�:0.0728 not:0.0884 bit:0.0435-:0.0496.:0.1807 but:0.1387)
		(.:0.0879 and:0.0767�:0.0698 sure:0.0977 bit:0.0447.:0.0464.:0.1895 but:0.1514)
 but
------
		(の魔:0.0040groupon:0.0046 and:0.8320 of:0.5898 language:0.9609wcsstore:0.0033,:1.0000 but:0.6172)
		(
:0.0080ces:0.0195 growing:0.0050
:0.0535,:0.1348,:0.1157
:0.0830 and:0.0255)
		( and:0.0153 and:0.0187 and:0.0070 and:0.0128,:0.0579,:0.0688 and:0.0452 and:0.0183)
		( and:0.0171 and:0.0197 in:0.0054 more:0.0118,:0.0327,:0.0425 and:0.0376 the:0.0197)
		( and:0.0229 and:0.0249 more:0.0087 more:0.0160,:0.0337 of:0.0576 and:0.0537 the:0.0330)
		( and:0.0281 and:0.0254 going:0.0208 more:0.0156,:0.0410 of:0.0850 and:0.0903 it:0.0486)
		( and:0.0312�:0.0293 going:0.0569 good:0.0148,:0.0540 of:0.1055 and:0.1182 it:0.1030)
		( and:0.0317�:0.0439 going:0.1006 good:0.0282,:0.0591 that:0.1289 and:0.1377 it:0.1689)
		( and:0.0359�:0.0608 going:0.1279 lot:0.0420 that:0.0588 that:0.1689 and:0.1562 it:0.2061)
		( and:0.0427�:0.0737 going:0.1318 lot:0.0518 that:0.0608 that:0.1875 and:0.1602 it:0.2080)
		( and:0.0547�:0.0742 going:0.1138 lot:0.0486-:0.0559 that:0.1885 and:0.1504 I:0.2266)
		( and:0.0708�:0.0728 not:0.0884 bit:0.0435-:0.0496.:0.1807 but:0.1387 I:0.2949)
		( and:0.0767�:0.0698 sure:0.0977 bit:0.0447.:0.0464.:0.1895 but:0.1514 I:0.3223)
 I'm not sure what you're doing.
I
4100: sample 0: Hello, I'm a language model,
------
		(.:0.9805�:0.0036groupon:0.0041 and:0.8281 of:0.6992 language:0.9375 Ivanka:0.0036,:1.0000)
		(,:0.3535 leading:0.0046ces:0.0342ces:0.0071
:0.0376,:0.1021,:0.0762 and:0.0417)
		( and:0.2178 and:0.0177 and:0.0162 in:0.0079 and:0.0123,:0.0505,:0.0554 and:0.0444)
		( and:0.1758 and:0.0226 and:0.0148 in:0.0067 and:0.0094,:0.0297,:0.0405 and:0.0488)
		( and:0.1328 and:0.0281 and:0.0188,:0.0108 I:0.0122,:0.0298,:0.0449 and:0.0742)
		( and:0.0967 and:0.0317 and:0.0228,:0.0144 I:0.0107,:0.0334,:0.0664 and:0.1187)
		(,:0.0728 and:0.0349 and:0.0245 not:0.0209 great:0.0092,:0.0391,:0.1084 and:0.1553)
		(,:0.0635 and:0.0386 would:0.0278 not:0.0388 good:0.0182,:0.0417,:0.1494 and:0.1992)
		(,:0.0562 and:0.0466 would:0.0461 not:0.0630 good:0.0293 that:0.0535,:0.1689 and:0.2080)
		(,:0.0552 and:0.0588 would:0.0635 not:0.0830 good:0.0354 that:0.0576,:0.1660 and:0.2158)
		(,:0.0566 and:0.0688 would:0.0718 not:0.0977 good:0.0361 that:0.0503.:0.1582 and:0.2051)
		(.:0.0703 and:0.0762 would:0.0688 sure:0.1279 good:0.0371 that:0.0400.:0.1826 and:0.1777)
		(.:0.0889 and:0.0757 think:0.0703 sure:0.1602 good:0.0386 interpreter:0.0391.:0.1992 and:0.1514)
 and
------
		(�:0.0036groupon:0.0041 and:0.8281 of:0.6992 language:0.9375 Ivanka:0.0036,:1.0000 and:1.0000)
		( leading:0.0046ces:0.0342ces:0.0071
:0.0376,:0.1021,:0.0762 and:0.0417 and:0.1279)
		( and:0.0177 and:0.0162 in:0.0079 and:0.0123,:0.0505,:0.0554 and:0.0444 and:0.0369)
		( and:0.0226 and:0.0148 in:0.0067 and:0.0094,:0.0297,:0.0405 and:0.0488 the:0.0277)
		( and:0.0281 and:0.0188,:0.0108 I:0.0122,:0.0298,:0.0449 and:0.0742 the:0.0530)
		( and:0.0317 and:0.0228,:0.0144 I:0.0107,:0.0334,:0.0664 and:0.1187 the:0.0654)
		( and:0.0349 and:0.0245 not:0.0209 great:0.0092,:0.0391,:0.1084 and:0.1553 I:0.1069)
		( and:0.0386 would:0.0278 not:0.0388 good:0.0182,:0.0417,:0.1494 and:0.1992 I:0.1865)
		( and:0.0466 would:0.0461 not:0.0630 good:0.0293 that:0.0535,:0.1689 and:0.2080 I:0.2598)
		( and:0.0588 would:0.0635 not:0.0830 good:0.0354 that:0.0576,:0.1660 and:0.2158 I:0.3125)
		( and:0.0688 would:0.0718 not:0.0977 good:0.0361 that:0.0503.:0.1582 and:0.2051 I:0.3613)
		( and:0.0762 would:0.0688 sure:0.1279 good:0.0371 that:0.0400.:0.1826 and:0.1777 I:0.3965)
		( and:0.0757 think:0.0703 sure:0.1602 good:0.0386 interpreter:0.0391.:0.1992 and:0.1514 I:0.4043)
 I'm a language model.
The first thing to
4250: sample 0: Hello, I'm a language model,
------
		(.:0.9805�:0.0034groupon:0.0051 and:0.8242 of:0.7500 language:0.9219 Ivanka:0.0044,:1.0000)
		(,:0.3145
:0.0051ces:0.0170ces:0.0036
:0.0374,:0.0420,:0.0728
:0.0845)
		(,:0.1953 and:0.0201 in:0.0125 ":0.0081 and:0.0100,:0.0295,:0.0474 and:0.0383)
		( and:0.1406 and:0.0286 in:0.0154 in:0.0054.:0.0086,:0.0209 of:0.0317 and:0.0325)
		( and:0.1021 and:0.0289 in:0.0164 more:0.0098 more:0.0122,:0.0256 of:0.0518 and:0.0400)
		( and:0.0723 and:0.0251cing:0.0198 not:0.0220 few:0.0146,:0.0342 of:0.0884 and:0.0510)
		(,:0.0515 which:0.0229 can:0.0212 not:0.0386 few:0.0151,:0.0356 of:0.1318 and:0.0742)
		(,:0.0396 the:0.0265 will:0.0310 not:0.0522 very:0.0175 that:0.0403 of:0.1592 and:0.0913)
		(,:0.0308 the:0.0305 would:0.0483 not:0.0630 bit:0.0344-:0.0532 of:0.1699 and:0.0923)
		(.:0.0305 the:0.0359 would:0.0625 not:0.0757 bit:0.0613-:0.0540 of:0.1621 I:0.1074)
		(.:0.0337 the:0.0457�:0.0796 not:0.0850 bit:0.0801 therapist:0.0547 of:0.1436 I:0.1396)
		(.:0.0400 the:0.0564�:0.0947 not:0.0923 bit:0.0845 lear:0.0640,:0.1357 I:0.1494)
		(.:0.0466 the:0.0645�:0.1099 not:0.0913 bit:0.0811 lear:0.0786.:0.1465 I:0.1328)
 I
------
		(�:0.0034groupon:0.0051 and:0.8242 of:0.7500 language:0.9219 Ivanka:0.0044,:1.0000 I:0.9883)
		(
:0.0051ces:0.0170ces:0.0036
:0.0374,:0.0420,:0.0728
:0.0845
:0.0491)
		( and:0.0201 in:0.0125 ":0.0081 and:0.0100,:0.0295,:0.0474 and:0.0383.:0.0186)
		( and:0.0286 in:0.0154 in:0.0054.:0.0086,:0.0209 of:0.0317 and:0.0325.:0.0278)
		( and:0.0289 in:0.0164 more:0.0098 more:0.0122,:0.0256 of:0.0518 and:0.0400.:0.0325)
		( and:0.0251cing:0.0198 not:0.0220 few:0.0146,:0.0342 of:0.0884 and:0.0510,:0.0234)
		( which:0.0229 can:0.0212 not:0.0386 few:0.0151,:0.0356 of:0.1318 and:0.0742'm:0.0486)
		( the:0.0265 will:0.0310 not:0.0522 very:0.0175 that:0.0403 of:0.1592 and:0.0913'm:0.1299)
		( the:0.0305 would:0.0483 not:0.0630 bit:0.0344-:0.0532 of:0.1699 and:0.0923'm:0.2500)
		( the:0.0359 would:0.0625 not:0.0757 bit:0.0613-:0.0540 of:0.1621 I:0.1074'm:0.3750)
		( the:0.0457�:0.0796 not:0.0850 bit:0.0801 therapist:0.0547 of:0.1436 I:0.1396'm:0.4434)
		( the:0.0564�:0.0947 not:0.0923 bit:0.0845 lear:0.0640,:0.1357 I:0.1494'm:0.4570)
		( the:0.0645�:0.1099 not:0.0913 bit:0.0811 lear:0.0786.:0.1465 I:0.1328'm:0.4336)
'm a language model, I'm a language model,
4400: sample 0: Hello, I'm a language model,
------
		(.:0.9805 Sakuya:0.0035groupon:0.0040 and:0.7930 of:0.7734 language:0.9414:0.0032,:1.0000)
		(,:0.3066 most:0.0024cing:0.0305mon:0.0039
:0.0157,:0.0625,:0.0566
:0.0825)
		( and:0.1992 and:0.0144 and:0.0161 in:0.0061 and:0.0091 and:0.0369 and:0.0386 and:0.0576)
		( and:0.1426 and:0.0186 and:0.0152 in:0.0056.:0.0095 in:0.0295 of:0.0282 and:0.0503)
		( and:0.1050 and:0.0192 and:0.0145 more:0.0101 more:0.0130,:0.0282 of:0.0386 and:0.0491)
		( and:0.0737 and:0.0160 can:0.0208 not:0.0173 more:0.0098 that:0.0275 of:0.0625 and:0.0586)
		(,:0.0508 in:0.0164 can:0.0260 going:0.0308 great:0.0122 that:0.0391 of:0.0923 and:0.0830)
		(,:0.0417 the:0.0184 was:0.0337 going:0.0466 great:0.0150 that:0.0564,:0.1133 and:0.1094)
		(,:0.0386 the:0.0247 was:0.0488 going:0.0571 member:0.0182 that:0.0659,:0.1475 and:0.1201)
		(.:0.0427 the:0.0305�:0.0654 going:0.0630 member:0.0242 that:0.0549,:0.1689 and:0.1367)
		(.:0.0562 and:0.0410�:0.0859 sure:0.0835 member:0.0272 programmer:0.0449,:0.1855 and:0.1543)
		(.:0.0752 and:0.0540�:0.1001 sure:0.1089 member:0.0280 programmer:0.0576.:0.1855 and:0.1514)
		(.:0.0986 and:0.0601�:0.1157 sure:0.1211 member:0.0270 programmer:0.0659.:0.2002 and:0.1406)
 and
------
		( Sakuya:0.0035groupon:0.0040 and:0.7930 of:0.7734 language:0.9414:0.0032,:1.0000 and:1.0000)
		( most:0.0024cing:0.0305mon:0.0039
:0.0157,:0.0625,:0.0566
:0.0825 and:0.2012)
		( and:0.0144 and:0.0161 in:0.0061 and:0.0091 and:0.0369 and:0.0386 and:0.0576 and:0.0664)
		( and:0.0186 and:0.0152 in:0.0056.:0.0095 in:0.0295 of:0.0282 and:0.0503 the:0.0292)
		( and:0.0192 and:0.0145 more:0.0101 more:0.0130,:0.0282 of:0.0386 and:0.0491 the:0.0508)
		( and:0.0160 can:0.0208 not:0.0173 more:0.0098 that:0.0275 of:0.0625 and:0.0586 the:0.0588)
		( in:0.0164 can:0.0260 going:0.0308 great:0.0122 that:0.0391 of:0.0923 and:0.0830 I:0.0732)
		( the:0.0184 was:0.0337 going:0.0466 great:0.0150 that:0.0564,:0.1133 and:0.1094 I:0.1592)
		( the:0.0247 was:0.0488 going:0.0571 member:0.0182 that:0.0659,:0.1475 and:0.1201 I:0.2188)
		( the:0.0305�:0.0654 going:0.0630 member:0.0242 that:0.0549,:0.1689 and:0.1367 I:0.2461)
		( and:0.0410�:0.0859 sure:0.0835 member:0.0272 programmer:0.0449,:0.1855 and:0.1543 I:0.2715)
		( and:0.0540�:0.1001 sure:0.1089 member:0.0280 programmer:0.0576.:0.1855 and:0.1514 I:0.2910)
		( and:0.0601�:0.1157 sure:0.1211 member:0.0270 programmer:0.0659.:0.2002 and:0.1406 I:0.2969)
 I'm a language model.
I'm a language
4550: sample 0: Hello, I'm a language model,
------
		(.:0.9805ottenham:0.0036groupon:0.0037 and:0.7539 of:0.8164 language:0.9062 Ivanka:0.0036,:1.0000)
		(,:0.3242
:0.0063ces:0.0140cing:0.0035
:0.0142,:0.0593,:0.0391 and:0.0493)
		( and:0.2119 and:0.0182 and:0.0151 in:0.0075 and:0.0079,:0.0391,:0.0304 and:0.0415)
		( and:0.1592 and:0.0258 in:0.0164 in:0.0051 more:0.0106,:0.0310,:0.0228 and:0.0420)
		( and:0.1187 and:0.0312 and:0.0161 more:0.0098 more:0.0160,:0.0306 of:0.0327 and:0.0591)
		( and:0.0894 and:0.0289 and:0.0156 going:0.0173 more:0.0140 arts:0.0393 of:0.0508 or:0.0981)
		( and:0.0613 and:0.0254 and:0.0154 going:0.0444 great:0.0181 arts:0.0430,:0.0767 or:0.1240)
		(,:0.0452 and:0.0250 am:0.0211 going:0.0737 great:0.0300,:0.0457,:0.1069 and:0.1377)
		(,:0.0371 and:0.0327�:0.0334 going:0.0898 bit:0.0403 that:0.0583,:0.1299 and:0.1338)
		(,:0.0337 and:0.0479�:0.0581 going:0.0884 bit:0.0688 that:0.0583,:0.1406 and:0.1348)
		(,:0.0354 and:0.0757�:0.0825 going:0.0757 bit:0.0889-:0.0583,:0.1465 and:0.1367)
		(,:0.0415 and:0.1050�:0.1074 not:0.0679 bit:0.0908-:0.0525,:0.1504 and:0.1270)
		(.:0.0559 and:0.1128�:0.1318 not:0.0742 bit:0.0854-:0.0449,:0.1494 I:0.1196)
 I
------
		(ottenham:0.0036groupon:0.0037 and:0.7539 of:0.8164 language:0.9062 Ivanka:0.0036,:1.0000 I:0.9766)
		(
:0.0063ces:0.0140cing:0.0035
:0.0142,:0.0593,:0.0391 and:0.0493
:0.0276)
		( and:0.0182 and:0.0151 in:0.0075 and:0.0079,:0.0391,:0.0304 and:0.0415 in:0.0204)
		( and:0.0258 in:0.0164 in:0.0051 more:0.0106,:0.0310,:0.0228 and:0.0420,:0.0198)
		( and:0.0312 and:0.0161 more:0.0098 more:0.0160,:0.0306 of:0.0327 and:0.0591,:0.0281)
		( and:0.0289 and:0.0156 going:0.0173 more:0.0140 arts:0.0393 of:0.0508 or:0.0981 was:0.0317)
		( and:0.0254 and:0.0154 going:0.0444 great:0.0181 arts:0.0430,:0.0767 or:0.1240 am:0.0728)
		( and:0.0250 am:0.0211 going:0.0737 great:0.0300,:0.0457,:0.1069 and:0.1377 am:0.1289)
		( and:0.0327�:0.0334 going:0.0898 bit:0.0403 that:0.0583,:0.1299 and:0.1338 am:0.1582)
		( and:0.0479�:0.0581 going:0.0884 bit:0.0688 that:0.0583,:0.1406 and:0.1348'm:0.2031)
		( and:0.0757�:0.0825 going:0.0757 bit:0.0889-:0.0583,:0.1465 and:0.1367'm:0.2148)
		( and:0.1050�:0.1074 not:0.0679 bit:0.0908-:0.0525,:0.1504 and:0.1270'm:0.1943)
		( and:0.1128�:0.1318 not:0.0742 bit:0.0854-:0.0449,:0.1494 I:0.1196'm:0.1631)
'm a language model, I'm a language model,
4700: sample 0: Hello, I'm a language model,
------
		(.:0.9844ottenham:0.0040groupon:0.0038 and:0.7734 of:0.8086 language:0.8867:0.0033,:1.0000)
		(,:0.3086 making:0.0034ces:0.0190cing:0.0032
:0.0140 in:0.0206 of:0.0281 and:0.0664)
		( and:0.1924 and:0.0113 in:0.0127 ":0.0050 and:0.0068 in:0.0251 and:0.0254 and:0.0415)
		( and:0.1533 and:0.0183 in:0.0154 more:0.0060 second:0.0079 in:0.0221 of:0.0210 and:0.0488)
		( and:0.1167 and:0.0253 and:0.0175 more:0.0112 more:0.0086 arts:0.0195 of:0.0288 and:0.0615)
		( and:0.0874 and:0.0277 and:0.0190 not:0.0161 small:0.0077 arts:0.0366 of:0.0437 and:0.0864)
		( and:0.0640 and:0.0282 can:0.0256 going:0.0398 great:0.0139 arts:0.0393,:0.0698 and:0.1328)
		(,:0.0476 and:0.0317 can:0.0332 going:0.0781 great:0.0212 that:0.0486,:0.0962 and:0.1846)
		(,:0.0435 and:0.0413 am:0.0466 going:0.1143 great:0.0222 that:0.0544,:0.1162 and:0.2061)
		(,:0.0422 and:0.0544�:0.0613 going:0.1338 bit:0.0237 that:0.0515,:0.1289 and:0.2090)
		(.:0.0513 and:0.0674�:0.0786 going:0.1299 bit:0.0334-:0.0437,:0.1338 and:0.2080)
		(.:0.0645 and:0.0791�:0.0947 sure:0.1118 bit:0.0398 teacher:0.0403,:0.1309 and:0.1816)
		(.:0.0815 and:0.0859�:0.1030 sure:0.1426 bit:0.0425 teacher:0.0552.:0.1270 and:0.1543)
 and
------
		(ottenham:0.0040groupon:0.0038 and:0.7734 of:0.8086 language:0.8867:0.0033,:1.0000 and:1.0000)
		( making:0.0034ces:0.0190cing:0.0032
:0.0140 in:0.0206 of:0.0281 and:0.0664 and:0.1387)
		( and:0.0113 in:0.0127 ":0.0050 and:0.0068 in:0.0251 and:0.0254 and:0.0415 and:0.0420)
		( and:0.0183 in:0.0154 more:0.0060 second:0.0079 in:0.0221 of:0.0210 and:0.0488 the:0.0266)
		( and:0.0253 and:0.0175 more:0.0112 more:0.0086 arts:0.0195 of:0.0288 and:0.0615 the:0.0454)
		( and:0.0277 and:0.0190 not:0.0161 small:0.0077 arts:0.0366 of:0.0437 and:0.0864 the:0.0520)
		( and:0.0282 can:0.0256 going:0.0398 great:0.0139 arts:0.0393,:0.0698 and:0.1328 I:0.0854)
		( and:0.0317 can:0.0332 going:0.0781 great:0.0212 that:0.0486,:0.0962 and:0.1846 I:0.1895)
		( and:0.0413 am:0.0466 going:0.1143 great:0.0222 that:0.0544,:0.1162 and:0.2061 I:0.2793)
		( and:0.0544�:0.0613 going:0.1338 bit:0.0237 that:0.0515,:0.1289 and:0.2090 I:0.3535)
		( and:0.0674�:0.0786 going:0.1299 bit:0.0334-:0.0437,:0.1338 and:0.2080 I:0.3867)
		( and:0.0791�:0.0947 sure:0.1118 bit:0.0398 teacher:0.0403,:0.1309 and:0.1816 I:0.4004)
		( and:0.0859�:0.1030 sure:0.1426 bit:0.0425 teacher:0.0552.:0.1270 and:0.1543 I:0.3984)
 I'm a language model. I'm a language model
4850: sample 0: Hello, I'm a language model,
------
		(.:0.9844ottenham:0.0045groupon:0.0037 and:0.7344 of:0.7930 language:0.9062:0.0034,:1.0000)
		(,:0.2578
:0.0034cing:0.0334cing:0.0031ces:0.0079,:0.0146,:0.0339
:0.0432)
		( and:0.2178 and:0.0124 and:0.0110 and:0.0047 second:0.0058 in:0.0137,:0.0261 and:0.0378)
		( and:0.1729 and:0.0229 and:0.0125,:0.0045 second:0.0091,:0.0113,:0.0259 and:0.0437)
		( and:0.1348 and:0.0306 and:0.0140 a:0.0089 second:0.0108 arts:0.0223,:0.0347 and:0.0542)
		( and:0.0981 and:0.0322cing:0.0222 going:0.0243 second:0.0075 arts:0.0391,:0.0542 and:0.0737)
		( and:0.0640 and:0.0325cing:0.0221 going:0.0547 long:0.0103 that:0.0474,:0.0757 and:0.0962)
		( and:0.0432 and:0.0361 am:0.0284 going:0.0771 great:0.0136 that:0.0708 that:0.1147 and:0.1226)
		(,:0.0339 and:0.0496 am:0.0464 going:0.0908 great:0.0146 that:0.0913 that:0.1504 and:0.1289)
		(,:0.0291 and:0.0630 am:0.0679 going:0.0894 bit:0.0232 that:0.1011 that:0.1650 and:0.1328)
		(.:0.0293 and:0.0767 am:0.0854 not:0.1011 bit:0.0334 that:0.0967 that:0.1621 and:0.1348)
		(.:0.0371 and:0.0967 am:0.0972 not:0.1157 bit:0.0415 that:0.0879.:0.1504 and:0.1338)
		(.:0.0520 and:0.1064 am:0.1035 not:0.1216 bit:0.0459 that:0.0791.:0.1611 and:0.1221)
 and
------
		(ottenham:0.0045groupon:0.0037 and:0.7344 of:0.7930 language:0.9062:0.0034,:1.0000 and:1.0000)
		(
:0.0034cing:0.0334cing:0.0031ces:0.0079,:0.0146,:0.0339
:0.0432
:0.1445)
		( and:0.0124 and:0.0110 and:0.0047 second:0.0058 in:0.0137,:0.0261 and:0.0378 and:0.0510)
		( and:0.0229 and:0.0125,:0.0045 second:0.0091,:0.0113,:0.0259 and:0.0437 and:0.0287)
		( and:0.0306 and:0.0140 a:0.0089 second:0.0108 arts:0.0223,:0.0347 and:0.0542 the:0.0388)
		( and:0.0322cing:0.0222 going:0.0243 second:0.0075 arts:0.0391,:0.0542 and:0.0737 the:0.0449)
		( and:0.0325cing:0.0221 going:0.0547 long:0.0103 that:0.0474,:0.0757 and:0.0962 I:0.0806)
		( and:0.0361 am:0.0284 going:0.0771 great:0.0136 that:0.0708 that:0.1147 and:0.1226 I:0.1973)
		( and:0.0496 am:0.0464 going:0.0908 great:0.0146 that:0.0913 that:0.1504 and:0.1289 I:0.2734)
		( and:0.0630 am:0.0679 going:0.0894 bit:0.0232 that:0.1011 that:0.1650 and:0.1328 I:0.3164)
		( and:0.0767 am:0.0854 not:0.1011 bit:0.0334 that:0.0967 that:0.1621 and:0.1348 I:0.3398)
		( and:0.0967 am:0.0972 not:0.1157 bit:0.0415 that:0.0879.:0.1504 and:0.1338 I:0.3438)
		( and:0.1064 am:0.1035 not:0.1216 bit:0.0459 that:0.0791.:0.1611 and:0.1221 I:0.3320)
 I'm a computer model.
I'm a computer
5000: sample 0: Hello, I'm a language model,
------
		(.:0.9883ottenham:0.0057groupon:0.0032 and:0.6875 of:0.7695 language:0.9023:0.0034,:1.0000)
		(,:0.2852
:0.0022ces:0.0247ces:0.0035
:0.0133 in:0.0088,:0.0311
:0.0442)
		( and:0.2061 and:0.0067 and:0.0075 present:0.0059 and:0.0055 and:0.0128,:0.0206 and:0.0312)
		( and:0.1572 and:0.0125 and:0.0123 more:0.0044 second:0.0090 and:0.0175 of:0.0280 and:0.0481)
		( and:0.1182 and:0.0151 and:0.0179 not:0.0132 second:0.0109 and:0.0258 of:0.0515 and:0.0659)
		( and:0.0791 and:0.0157cing:0.0245 not:0.0311 great:0.0094 that:0.0337 of:0.0737 and:0.0815)
		( and:0.0537 and:0.0177 will:0.0245 not:0.0535 great:0.0182 that:0.0452 that:0.1172 and:0.1196)
		(,:0.0386 and:0.0217 will:0.0303 not:0.0723 great:0.0254-:0.0601 that:0.1621 and:0.1494)
		(,:0.0352 and:0.0288�:0.0408 not:0.0928 great:0.0277-:0.0854 that:0.1934 and:0.1758)
		(,:0.0361 and:0.0383�:0.0549 not:0.1104 bit:0.0337-:0.0869 that:0.2002 and:0.1934)
		(,:0.0425 and:0.0525�:0.0635 not:0.1270 bit:0.0393 teacher:0.0801 that:0.1963 and:0.1865)
		(.:0.0586 and:0.0703�:0.0723 not:0.1348 bit:0.0410 teacher:0.1196 that:0.1729 and:0.1670)
		(.:0.0806 and:0.0835�:0.0786 not:0.1338 bit:0.0393 teacher:0.1455 that:0.1641 and:0.1514)
 and
------
		(ottenham:0.0057groupon:0.0032 and:0.6875 of:0.7695 language:0.9023:0.0034,:1.0000 and:1.0000)
		(
:0.0022ces:0.0247ces:0.0035
:0.0133 in:0.0088,:0.0311
:0.0442 and:0.1182)
		( and:0.0067 and:0.0075 present:0.0059 and:0.0055 and:0.0128,:0.0206 and:0.0312 and:0.0306)
		( and:0.0125 and:0.0123 more:0.0044 second:0.0090 and:0.0175 of:0.0280 and:0.0481 the:0.0265)
		( and:0.0151 and:0.0179 not:0.0132 second:0.0109 and:0.0258 of:0.0515 and:0.0659 the:0.0515)
		( and:0.0157cing:0.0245 not:0.0311 great:0.0094 that:0.0337 of:0.0737 and:0.0815 the:0.0742)
		( and:0.0177 will:0.0245 not:0.0535 great:0.0182 that:0.0452 that:0.1172 and:0.1196 I:0.1621)
		( and:0.0217 will:0.0303 not:0.0723 great:0.0254-:0.0601 that:0.1621 and:0.1494 I:0.3223)
		( and:0.0288�:0.0408 not:0.0928 great:0.0277-:0.0854 that:0.1934 and:0.1758 I:0.4473)
		( and:0.0383�:0.0549 not:0.1104 bit:0.0337-:0.0869 that:0.2002 and:0.1934 I:0.5234)
		( and:0.0525�:0.0635 not:0.1270 bit:0.0393 teacher:0.0801 that:0.1963 and:0.1865 I:0.5547)
		( and:0.0703�:0.0723 not:0.1348 bit:0.0410 teacher:0.1196 that:0.1729 and:0.1670 I:0.5508)
		( and:0.0835�:0.0786 not:0.1338 bit:0.0393 teacher:0.1455 that:0.1641 and:0.1514 I:0.5156)
 I'm a linguist. I'm a linguist
5150: sample 0: Hello, I'm a language model,
------
		(.:0.9844ottenham:0.0048groupon:0.0032 and:0.5977 of:0.7461 language:0.8633:0.0033,:1.0000)
		(,:0.2637
:0.0352cing:0.0168mon:0.0040
:0.0581,:0.0293 space:0.0219
:0.0549)
		( and:0.2002 and:0.0145 in:0.0116 present:0.0041 and:0.0108 in:0.0249 and:0.0188 and:0.0320)
		( and:0.1523 and:0.0173 in:0.0120 designed:0.0052
:0.0112 in:0.0165 and:0.0154 and:0.0359)
		( and:0.1104 and:0.0170 and:0.0128 most:0.0106 second:0.0078 arts:0.0276 of:0.0277 and:0.0530)
		( and:0.0791 and:0.0151cing:0.0123 not:0.0157 great:0.0061 arts:0.0598 of:0.0483 and:0.0776)
		( and:0.0530 which:0.0149 was:0.0170 not:0.0244 great:0.0123 arts:0.0728 of:0.0713 and:0.1211)
		(,:0.0410 which:0.0208'm:0.0303 not:0.0342 great:0.0211 arts:0.0630 that:0.1104 and:0.1689)
		(,:0.0366 which:0.0253�:0.0481 going:0.0493 great:0.0287 that:0.0835 that:0.1719 and:0.1836)
		(,:0.0374 but:0.0325�:0.0693 not:0.0669 great:0.0317 that:0.1152 that:0.2158 and:0.1875)
		(.:0.0422 the:0.0420�:0.0811 not:0.0840 great:0.0306 that:0.1338 that:0.2246 and:0.1758)
		(.:0.0569 the:0.0574�:0.0869 not:0.1035 great:0.0271 that:0.1328 that:0.2246 and:0.1553)
		(.:0.0791 the:0.0737�:0.0898 not:0.1138 great:0.0251 that:0.1245 that:0.2041 and:0.1289)
 and
------
		(ottenham:0.0048groupon:0.0032 and:0.5977 of:0.7461 language:0.8633:0.0033,:1.0000 and:1.0000)
		(
:0.0352cing:0.0168mon:0.0040
:0.0581,:0.0293 space:0.0219
:0.0549 and:0.1147)
		( and:0.0145 in:0.0116 present:0.0041 and:0.0108 in:0.0249 and:0.0188 and:0.0320 and:0.0297)
		( and:0.0173 in:0.0120 designed:0.0052
:0.0112 in:0.0165 and:0.0154 and:0.0359 is:0.0221)
		( and:0.0170 and:0.0128 most:0.0106 second:0.0078 arts:0.0276 of:0.0277 and:0.0530 the:0.0408)
		( and:0.0151cing:0.0123 not:0.0157 great:0.0061 arts:0.0598 of:0.0483 and:0.0776 the:0.0530)
		( which:0.0149 was:0.0170 not:0.0244 great:0.0123 arts:0.0728 of:0.0713 and:0.1211 the:0.0613)
		( which:0.0208'm:0.0303 not:0.0342 great:0.0211 arts:0.0630 that:0.1104 and:0.1689 I:0.1309)
		( which:0.0253�:0.0481 going:0.0493 great:0.0287 that:0.0835 that:0.1719 and:0.1836 I:0.2275)
		( but:0.0325�:0.0693 not:0.0669 great:0.0317 that:0.1152 that:0.2158 and:0.1875 I:0.3164)
		( the:0.0420�:0.0811 not:0.0840 great:0.0306 that:0.1338 that:0.2246 and:0.1758 I:0.3613)
		( the:0.0574�:0.0869 not:0.1035 great:0.0271 that:0.1328 that:0.2246 and:0.1553 I:0.3652)
		( the:0.0737�:0.0898 not:0.1138 great:0.0251 that:0.1245 that:0.2041 and:0.1289 I:0.3555)
 I'm a language model.
The first thing is
5300: sample 0: Hello, I'm a language model,
------
		(.:0.9844!!":0.0053groupon:0.0028 and:0.5430 of:0.7109 language:0.8867:0.0030,:1.0000)
		(,:0.2451
:0.0126cing:0.0140ney:0.0042 second:0.0108,:0.0339,:0.0381
:0.0415)
		( and:0.1943 and:0.0135 in:0.0120 present:0.0043 second:0.0088,:0.0242,:0.0320 and:0.0396)
		( and:0.1504 and:0.0253 in:0.0125 most:0.0055 second:0.0162,:0.0190,:0.0305 and:0.0488)
		( and:0.1157 and:0.0315 in:0.0131 most:0.0123 second:0.0183 arts:0.0483,:0.0391 and:0.0713)
		( and:0.0820 and:0.0308 is:0.0119 not:0.0237 second:0.0122 arts:0.0859,:0.0625 and:0.1069)
		(,:0.0623 which:0.0452 was:0.0205 not:0.0425 member:0.0107 arts:0.0762,:0.0923 and:0.1543)
		(,:0.0503 which:0.0496 was:0.0330 not:0.0669 member:0.0219 arts:0.0496,:0.1143 and:0.1826)
		(,:0.0437 which:0.0518 will:0.0442 not:0.0962 member:0.0332 that:0.0557,:0.1245 and:0.1885)
		(.:0.0457 and:0.0522 will:0.0537 not:0.1206 member:0.0398 that:0.0535,:0.1270 and:0.1768)
		(.:0.0623 and:0.0603 have:0.0603 not:0.1357 member:0.0413 lear:0.0623.:0.1592 and:0.1650)
		(.:0.0884 and:0.0664 have:0.0723 not:0.1387 member:0.0381 lear:0.0898.:0.1660 and:0.1475)
		(.:0.1211 the:0.0698 have:0.0815 not:0.1245 writer:0.0405 lear:0.1064.:0.1660 and:0.1250)
 and
------
		(!!":0.0053groupon:0.0028 and:0.5430 of:0.7109 language:0.8867:0.0030,:1.0000 and:1.0000)
		(
:0.0126cing:0.0140ney:0.0042 second:0.0108,:0.0339,:0.0381
:0.0415
:0.1045)
		( and:0.0135 in:0.0120 present:0.0043 second:0.0088,:0.0242,:0.0320 and:0.0396 and:0.0308)
		( and:0.0253 in:0.0125 most:0.0055 second:0.0162,:0.0190,:0.0305 and:0.0488 the:0.0332)
		( and:0.0315 in:0.0131 most:0.0123 second:0.0183 arts:0.0483,:0.0391 and:0.0713 the:0.0535)
		( and:0.0308 is:0.0119 not:0.0237 second:0.0122 arts:0.0859,:0.0625 and:0.1069 the:0.0554)
		( which:0.0452 was:0.0205 not:0.0425 member:0.0107 arts:0.0762,:0.0923 and:0.1543 I:0.0669)
		( which:0.0496 was:0.0330 not:0.0669 member:0.0219 arts:0.0496,:0.1143 and:0.1826 I:0.2148)
		( which:0.0518 will:0.0442 not:0.0962 member:0.0332 that:0.0557,:0.1245 and:0.1885 I:0.3906)
		( and:0.0522 will:0.0537 not:0.1206 member:0.0398 that:0.0535,:0.1270 and:0.1768 I:0.4453)
		( and:0.0603 have:0.0603 not:0.1357 member:0.0413 lear:0.0623.:0.1592 and:0.1650 I:0.4219)
		( and:0.0664 have:0.0723 not:0.1387 member:0.0381 lear:0.0898.:0.1660 and:0.1475 I:0.3750)
		( the:0.0698 have:0.0815 not:0.1245 writer:0.0405 lear:0.1064.:0.1660 and:0.1250 I:0.3125)
 I'm a language model. I'm a language model
5450: sample 0: Hello, I'm a language model,
------
		(.:0.9805!!":0.0069anyahu:0.0026 and:0.4160 of:0.6211 language:0.8750 Ivanka:0.0031,:1.0000)
		(,:0.2520
:0.0045cing:0.0288ney:0.0033 second:0.0109,:0.0167 space:0.0131
:0.0219)
		( and:0.1826 and:0.0063 in:0.0055mon:0.0042 second:0.0110,:0.0204,:0.0170 and:0.0234)
		( and:0.1543 and:0.0129 in:0.0064,:0.0040 second:0.0195,:0.0237,:0.0233 and:0.0425)
		( and:0.1201 and:0.0190cing:0.0095,:0.0092 second:0.0251,:0.0342,:0.0422 and:0.0591)
		( and:0.0874 and:0.0255cing:0.0132 not:0.0171 second:0.0179,:0.0505,:0.0752 and:0.0806)
		(,:0.0630 and:0.0317'm:0.0240 not:0.0322 great:0.0157,:0.0620,:0.1084 and:0.1279)
		(,:0.0537 and:0.0386'm:0.0449 not:0.0562 great:0.0211 that:0.0840 that:0.1572 and:0.1680)
		(,:0.0476 and:0.0486'm:0.0649 not:0.0859 great:0.0236 that:0.1162 that:0.2080 and:0.1855)
		(,:0.0479 and:0.0608'm:0.0796 not:0.1162 great:0.0232 that:0.1357 that:0.2295 and:0.1943)
		(,:0.0557 and:0.0796 am:0.0864 not:0.1426 great:0.0215 that:0.1357 that:0.2188 and:0.2021)
		(,:0.0752 and:0.1025 am:0.0859 not:0.1582 student:0.0233 that:0.1270 that:0.2031 and:0.1924)
		(,:0.0996 and:0.1113 am:0.0845 not:0.1611 student:0.0267 that:0.1147,:0.1875 and:0.1709)
 and
------
		(!!":0.0069anyahu:0.0026 and:0.4160 of:0.6211 language:0.8750 Ivanka:0.0031,:1.0000 and:1.0000)
		(
:0.0045cing:0.0288ney:0.0033 second:0.0109,:0.0167 space:0.0131
:0.0219 and:0.0496)
		( and:0.0063 in:0.0055mon:0.0042 second:0.0110,:0.0204,:0.0170 and:0.0234 and:0.0190)
		( and:0.0129 in:0.0064,:0.0040 second:0.0195,:0.0237,:0.0233 and:0.0425 the:0.0272)
		( and:0.0190cing:0.0095,:0.0092 second:0.0251,:0.0342,:0.0422 and:0.0591 other:0.0598)
		( and:0.0255cing:0.0132 not:0.0171 second:0.0179,:0.0505,:0.0752 and:0.0806 the:0.0669)
		( and:0.0317'm:0.0240 not:0.0322 great:0.0157,:0.0620,:0.1084 and:0.1279 I:0.0796)
		( and:0.0386'm:0.0449 not:0.0562 great:0.0211 that:0.0840 that:0.1572 and:0.1680 I:0.1680)
		( and:0.0486'm:0.0649 not:0.0859 great:0.0236 that:0.1162 that:0.2080 and:0.1855 I:0.2520)
		( and:0.0608'm:0.0796 not:0.1162 great:0.0232 that:0.1357 that:0.2295 and:0.1943 I:0.3047)
		( and:0.0796 am:0.0864 not:0.1426 great:0.0215 that:0.1357 that:0.2188 and:0.2021 I:0.3438)
		( and:0.1025 am:0.0859 not:0.1582 student:0.0233 that:0.1270 that:0.2031 and:0.1924 I:0.3418)
		( and:0.1113 am:0.0845 not:0.1611 student:0.0267 that:0.1147,:0.1875 and:0.1709 I:0.3301)
 I'm a language model.
I'm a language
5600: sample 0: Hello, I'm a language model,
------
		(.:0.9766!!":0.0103 DJs:0.0037's:0.3301 of:0.5078 language:0.9023 Ivanka:0.0029,:1.0000)
		(,:0.1875
:0.0029cing:0.0100mon:0.0036ces:0.0061 so:0.0074 of:0.0164
:0.0275)
		( and:0.1553 and:0.0056 in:0.0045 and:0.0033 second:0.0070,:0.0116 of:0.0164 and:0.0283)
		( and:0.1123 and:0.0104 and:0.0069,:0.0036 second:0.0106,:0.0183 of:0.0258 and:0.0449)
		( and:0.0830 and:0.0131 and:0.0106 the:0.0101 second:0.0147,:0.0312 of:0.0405 and:0.0554)
		(,:0.0591 and:0.0131cing:0.0124 not:0.0216 second:0.0137,:0.0415,:0.0620 and:0.0635)
		(,:0.0464 however:0.0156 am:0.0177 not:0.0386 very:0.0099 that:0.0640 that:0.1133 and:0.0991)
		(,:0.0381 however:0.0233 am:0.0359 not:0.0588 great:0.0160 that:0.1011 that:0.1748 and:0.1484)
		(,:0.0320 but:0.0364 am:0.0596 not:0.0815 great:0.0177 that:0.1465 that:0.2148 and:0.2002)
		(.:0.0352 and:0.0537 am:0.0781 not:0.1055 good:0.0195 that:0.1855 that:0.2129 and:0.2275)
		(.:0.0459 and:0.0752 am:0.0840 not:0.1289 good:0.0204 that:0.1914.:0.2217 and:0.2305)
		(.:0.0659 and:0.1001 am:0.0830 not:0.1406 good:0.0195 that:0.1787.:0.2324 and:0.2070)
		(,:0.0947 and:0.1079 am:0.0757 not:0.1465 bit:0.0208 that:0.1660.:0.2461 and:0.1846)
 and
------
		(!!":0.0103 DJs:0.0037's:0.3301 of:0.5078 language:0.9023 Ivanka:0.0029,:1.0000 and:1.0000)
		(
:0.0029cing:0.0100mon:0.0036ces:0.0061 so:0.0074 of:0.0164
:0.0275 and:0.1030)
		( and:0.0056 in:0.0045 and:0.0033 second:0.0070,:0.0116 of:0.0164 and:0.0283 and:0.0309)
		( and:0.0104 and:0.0069,:0.0036 second:0.0106,:0.0183 of:0.0258 and:0.0449 the:0.0304)
		( and:0.0131 and:0.0106 the:0.0101 second:0.0147,:0.0312 of:0.0405 and:0.0554 other:0.0605)
		( and:0.0131cing:0.0124 not:0.0216 second:0.0137,:0.0415,:0.0620 and:0.0635 other:0.0613)
		( however:0.0156 am:0.0177 not:0.0386 very:0.0099 that:0.0640 that:0.1133 and:0.0991 I:0.0571)
		( however:0.0233 am:0.0359 not:0.0588 great:0.0160 that:0.1011 that:0.1748 and:0.1484 I:0.1206)
		( but:0.0364 am:0.0596 not:0.0815 great:0.0177 that:0.1465 that:0.2148 and:0.2002 I:0.2021)
		( and:0.0537 am:0.0781 not:0.1055 good:0.0195 that:0.1855 that:0.2129 and:0.2275 I:0.2930)
		( and:0.0752 am:0.0840 not:0.1289 good:0.0204 that:0.1914.:0.2217 and:0.2305 I:0.3516)
		( and:0.1001 am:0.0830 not:0.1406 good:0.0195 that:0.1787.:0.2324 and:0.2070 I:0.3672)
		( and:0.1079 am:0.0757 not:0.1465 bit:0.0208 that:0.1660.:0.2461 and:0.1846 I:0.3379)
 I'm a language model.
I'm a language
5750: sample 0: Hello, I'm a language model,
------
		(.:0.9727!!":0.0075riors:0.0029's:0.4062 a:0.4824 language:0.8984 Ivanka:0.0029,:1.0000)
		(,:0.2021
:0.0032cing:0.0204mon:0.0046
:0.0112,:0.0141,:0.0253
:0.0150)
		( and:0.1553
:0.0052 in:0.0050mon:0.0040 second:0.0079,:0.0148 of:0.0266 and:0.0194)
		( and:0.1182 and:0.0121 and:0.0096 designed:0.0056 second:0.0123,:0.0150 of:0.0352 and:0.0359)
		( and:0.0879 and:0.0172 and:0.0127 most:0.0093 second:0.0152,:0.0200 of:0.0505 and:0.0547)
		( and:0.0615 and:0.0177cing:0.0155 not:0.0186 small:0.0123,:0.0286 of:0.0684 and:0.0698)
		(,:0.0420 which:0.0197 am:0.0254 not:0.0378 great:0.0161 that:0.0457 that:0.1089 and:0.1123)
		(,:0.0320 and:0.0256 am:0.0513 not:0.0615 great:0.0251 that:0.0708 that:0.1807 and:0.1777)
		(,:0.0259 and:0.0386 am:0.0791 not:0.0889 great:0.0283 that:0.0957 that:0.2236 and:0.2471)
		(.:0.0251 and:0.0513 am:0.1045 not:0.1177 bit:0.0298 that:0.1138 that:0.2246 and:0.2832)
		(.:0.0298 and:0.0620 am:0.1172 not:0.1475 bit:0.0515 that:0.1162 that:0.1914 and:0.2812)
		(.:0.0391 and:0.0742 am:0.1187 not:0.1650 bit:0.0703 that:0.1147.:0.1816 and:0.2656)
		(.:0.0530 the:0.0811 am:0.1133 not:0.1699 bit:0.0811 that:0.1157.:0.2002 and:0.2393)
 and
------
		(!!":0.0075riors:0.0029's:0.4062 a:0.4824 language:0.8984 Ivanka:0.0029,:1.0000 and:1.0000)
		(
:0.0032cing:0.0204mon:0.0046
:0.0112,:0.0141,:0.0253
:0.0150 and:0.0884)
		(
:0.0052 in:0.0050mon:0.0040 second:0.0079,:0.0148 of:0.0266 and:0.0194 and:0.0231)
		( and:0.0121 and:0.0096 designed:0.0056 second:0.0123,:0.0150 of:0.0352 and:0.0359 other:0.0325)
		( and:0.0172 and:0.0127 most:0.0093 second:0.0152,:0.0200 of:0.0505 and:0.0547 other:0.0732)
		( and:0.0177cing:0.0155 not:0.0186 small:0.0123,:0.0286 of:0.0684 and:0.0698 other:0.0562)
		( which:0.0197 am:0.0254 not:0.0378 great:0.0161 that:0.0457 that:0.1089 and:0.1123 the:0.0469)
		( and:0.0256 am:0.0513 not:0.0615 great:0.0251 that:0.0708 that:0.1807 and:0.1777 I:0.1045)
		( and:0.0386 am:0.0791 not:0.0889 great:0.0283 that:0.0957 that:0.2236 and:0.2471 I:0.1875)
		( and:0.0513 am:0.1045 not:0.1177 bit:0.0298 that:0.1138 that:0.2246 and:0.2832 I:0.2539)
		( and:0.0620 am:0.1172 not:0.1475 bit:0.0515 that:0.1162 that:0.1914 and:0.2812 I:0.2988)
		( and:0.0742 am:0.1187 not:0.1650 bit:0.0703 that:0.1147.:0.1816 and:0.2656 I:0.2969)
		( the:0.0811 am:0.1133 not:0.1699 bit:0.0811 that:0.1157.:0.2002 and:0.2393 I:0.2734)
 I'm a language model.
I am a language
5900: sample 0: Hello, I'm a language model,
------
		(.:0.9688!!":0.0083ublished:0.0037's:0.3750 a:0.5781 language:0.8984 Ivanka:0.0027,:1.0000)
		(,:0.2168
:0.0061cing:0.0188mon:0.0036
:0.0074 so:0.0128 space:0.0153
:0.0286)
		( and:0.1523 in:0.0042 in:0.0049 designed:0.0033 second:0.0095 in:0.0106 and:0.0116 and:0.0170)
		( and:0.1167 and:0.0089 and:0.0095 not:0.0052 second:0.0177,:0.0122 of:0.0199 and:0.0325)
		( and:0.0825 and:0.0132 and:0.0140 not:0.0165 second:0.0162,:0.0189 of:0.0386 and:0.0530)
		( and:0.0557 and:0.0140 was:0.0190 not:0.0298 great:0.0110,:0.0270 of:0.0659 and:0.0859)
		(,:0.0413 which:0.0149 was:0.0291 not:0.0444 great:0.0250 that:0.0398 of:0.0942 and:0.1543)
		(,:0.0327 and:0.0206 was:0.0374 not:0.0605 great:0.0447 that:0.0579 that:0.1167 and:0.2363)
		(,:0.0277 and:0.0349 was:0.0425 not:0.0742 great:0.0527 that:0.0801 that:0.1523 and:0.2930)
		(,:0.0264 and:0.0510�:0.0508 not:0.0869 great:0.0471 that:0.0942 that:0.1631 and:0.3301)
		(,:0.0302 and:0.0732 am:0.0635 not:0.0972 great:0.0378 that:0.0903 that:0.1592 and:0.3262)
		(,:0.0400 and:0.1025 am:0.0723 not:0.0977 great:0.0330 that:0.0796,:0.1611 and:0.2852)
		(,:0.0569 and:0.1191 have:0.0815 not:0.0913 great:0.0292 that:0.0752,:0.1562 and:0.2441)
 and
------
		(!!":0.0083ublished:0.0037's:0.3750 a:0.5781 language:0.8984 Ivanka:0.0027,:1.0000 and:1.0000)
		(
:0.0061cing:0.0188mon:0.0036
:0.0074 so:0.0128 space:0.0153
:0.0286
:0.0557)
		( in:0.0042 in:0.0049 designed:0.0033 second:0.0095 in:0.0106 and:0.0116 and:0.0170 to:0.0175)
		( and:0.0089 and:0.0095 not:0.0052 second:0.0177,:0.0122 of:0.0199 and:0.0325 the:0.0209)
		( and:0.0132 and:0.0140 not:0.0165 second:0.0162,:0.0189 of:0.0386 and:0.0530 other:0.0391)
		( and:0.0140 was:0.0190 not:0.0298 great:0.0110,:0.0270 of:0.0659 and:0.0859 the:0.0469)
		( which:0.0149 was:0.0291 not:0.0444 great:0.0250 that:0.0398 of:0.0942 and:0.1543 I:0.0618)
		( and:0.0206 was:0.0374 not:0.0605 great:0.0447 that:0.0579 that:0.1167 and:0.2363 I:0.1465)
		( and:0.0349 was:0.0425 not:0.0742 great:0.0527 that:0.0801 that:0.1523 and:0.2930 I:0.2734)
		( and:0.0510�:0.0508 not:0.0869 great:0.0471 that:0.0942 that:0.1631 and:0.3301 I:0.4062)
		( and:0.0732 am:0.0635 not:0.0972 great:0.0378 that:0.0903 that:0.1592 and:0.3262 I:0.4590)
		( and:0.1025 am:0.0723 not:0.0977 great:0.0330 that:0.0796,:0.1611 and:0.2852 I:0.4590)
		( and:0.1191 have:0.0815 not:0.0913 great:0.0292 that:0.0752,:0.1562 and:0.2441 I:0.4141)
 I'm a language model. I'm a language model
6050: sample 0: Hello, I'm a language model,
------
		(.:0.9609 sophistic:0.0063ublished:0.0048's:0.3438 a:0.7344 language:0.8516 Ivanka:0.0026,:1.0000)
		(,:0.2461formation:0.0016ces:0.0187mon:0.0034ces:0.0123,:0.0178 space:0.0193
:0.0209)
		( and:0.1396 in:0.0064cing:0.0037 present:0.0045 second:0.0057 in:0.0148 of:0.0110 and:0.0194)
		( and:0.1133 in:0.0113cing:0.0059 most:0.0045 second:0.0103,:0.0153 of:0.0210 and:0.0311)
		( and:0.0830 in:0.0141cing:0.0132 not:0.0139 second:0.0108 arts:0.0231 of:0.0403 and:0.0464)
		(,:0.0630 in:0.0156cing:0.0262 not:0.0273 second:0.0069 arts:0.0391 of:0.0605 and:0.0588)
		(,:0.0493 which:0.0168 am:0.0383 not:0.0481 great:0.0126 arts:0.0430 that:0.1060 and:0.0850)
		(,:0.0405 but:0.0217 am:0.0688 not:0.0742 great:0.0188 that:0.0405 that:0.1641 and:0.1094)
		(,:0.0371 but:0.0298 am:0.1030 not:0.1021 great:0.0214 that:0.0579 that:0.1982 and:0.1299)
		(,:0.0393 the:0.0371 am:0.1260 not:0.1270 great:0.0199 that:0.0684 that:0.2002 and:0.1396)
		(,:0.0518 the:0.0537 am:0.1260 not:0.1455 great:0.0167 that:0.0659 that:0.1768 and:0.1455)
		(,:0.0771 the:0.0767 am:0.1152 not:0.1514 student:0.0209 therapist:0.0579.:0.1729 and:0.1465)
		(,:0.1167 the:0.0986 am:0.0996 not:0.1426 student:0.0229 therapist:0.0532.:0.1807 and:0.1475)
 and
------
		( sophistic:0.0063ublished:0.0048's:0.3438 a:0.7344 language:0.8516 Ivanka:0.0026,:1.0000 and:1.0000)
		(formation:0.0016ces:0.0187mon:0.0034ces:0.0123,:0.0178 space:0.0193
:0.0209 and:0.0718)
		( in:0.0064cing:0.0037 present:0.0045 second:0.0057 in:0.0148 of:0.0110 and:0.0194 and:0.0204)
		( in:0.0113cing:0.0059 most:0.0045 second:0.0103,:0.0153 of:0.0210 and:0.0311 the:0.0315)
		( in:0.0141cing:0.0132 not:0.0139 second:0.0108 arts:0.0231 of:0.0403 and:0.0464 the:0.0503)
		( in:0.0156cing:0.0262 not:0.0273 second:0.0069 arts:0.0391 of:0.0605 and:0.0588 the:0.0422)
		( which:0.0168 am:0.0383 not:0.0481 great:0.0126 arts:0.0430 that:0.1060 and:0.0850 I:0.0811)
		( but:0.0217 am:0.0688 not:0.0742 great:0.0188 that:0.0405 that:0.1641 and:0.1094 I:0.1621)
		( but:0.0298 am:0.1030 not:0.1021 great:0.0214 that:0.0579 that:0.1982 and:0.1299 I:0.2930)
		( the:0.0371 am:0.1260 not:0.1270 great:0.0199 that:0.0684 that:0.2002 and:0.1396 I:0.4043)
		( the:0.0537 am:0.1260 not:0.1455 great:0.0167 that:0.0659 that:0.1768 and:0.1455 I:0.3984)
		( the:0.0767 am:0.1152 not:0.1514 student:0.0209 therapist:0.0579.:0.1729 and:0.1465 I:0.3516)
		( the:0.0986 am:0.0996 not:0.1426 student:0.0229 therapist:0.0532.:0.1807 and:0.1475 I:0.2832)
 I'm a computer model.
I'm a computer
6200: sample 0: Hello, I'm a language model,
------
		(.:0.9375vernment:0.0055ublished:0.0068's:0.2812 a:0.7969 language:0.8906 Ivanka:0.0026,:1.0000)
		(,:0.2129
:0.0022ces:0.0156ms:0.0035ces:0.0093,:0.0214 space:0.0138
:0.0120)
		( and:0.1553 and:0.0038 in:0.0041 present:0.0024 second:0.0054,:0.0145 in:0.0129 and:0.0161)
		( and:0.1201 and:0.0102 and:0.0083 most:0.0042 second:0.0085,:0.0149 of:0.0192 and:0.0253)
		( and:0.0894 and:0.0200 was:0.0162 most:0.0098 second:0.0093,:0.0175 of:0.0327 and:0.0303)
		(,:0.0669 and:0.0223 was:0.0374 most:0.0113 second:0.0071,:0.0210 of:0.0476 and:0.0356)
		(,:0.0500 and:0.0209 was:0.0610 not:0.0197 great:0.0137-:0.0295 that:0.0718 and:0.0583)
		(,:0.0393 and:0.0229 was:0.0815 not:0.0388 great:0.0273-:0.0444 that:0.1299 and:0.0894)
		(,:0.0332 and:0.0347 was:0.0928 not:0.0708 great:0.0352-:0.0593 that:0.1787 and:0.1162)
		(,:0.0322 and:0.0493 was:0.0933 not:0.1104 great:0.0317 that:0.0801 that:0.2002 and:0.1367)
		(.:0.0396 and:0.0645 was:0.0869 not:0.1475 great:0.0253 that:0.0928 that:0.1865 and:0.1504)
		(.:0.0579 and:0.0820 was:0.0801 not:0.1748 great:0.0206 that:0.0933,:0.1855 and:0.1484)
		(.:0.0859 and:0.0879 was:0.0703 not:0.1748 little:0.0205 that:0.0942,:0.1924 and:0.1416)
 and
------
		(vernment:0.0055ublished:0.0068's:0.2812 a:0.7969 language:0.8906 Ivanka:0.0026,:1.0000 and:1.0000)
		(
:0.0022ces:0.0156ms:0.0035ces:0.0093,:0.0214 space:0.0138
:0.0120 and:0.0364)
		( and:0.0038 in:0.0041 present:0.0024 second:0.0054,:0.0145 in:0.0129 and:0.0161 and:0.0146)
		( and:0.0102 and:0.0083 most:0.0042 second:0.0085,:0.0149 of:0.0192 and:0.0253 the:0.0187)
		( and:0.0200 was:0.0162 most:0.0098 second:0.0093,:0.0175 of:0.0327 and:0.0303 the:0.0308)
		( and:0.0223 was:0.0374 most:0.0113 second:0.0071,:0.0210 of:0.0476 and:0.0356 the:0.0361)
		( and:0.0209 was:0.0610 not:0.0197 great:0.0137-:0.0295 that:0.0718 and:0.0583 the:0.0449)
		( and:0.0229 was:0.0815 not:0.0388 great:0.0273-:0.0444 that:0.1299 and:0.0894 I:0.1030)
		( and:0.0347 was:0.0928 not:0.0708 great:0.0352-:0.0593 that:0.1787 and:0.1162 I:0.2471)
		( and:0.0493 was:0.0933 not:0.1104 great:0.0317 that:0.0801 that:0.2002 and:0.1367 I:0.3984)
		( and:0.0645 was:0.0869 not:0.1475 great:0.0253 that:0.0928 that:0.1865 and:0.1504 I:0.4453)
		( and:0.0820 was:0.0801 not:0.1748 great:0.0206 that:0.0933,:0.1855 and:0.1484 I:0.4180)
		( and:0.0879 was:0.0703 not:0.1748 little:0.0205 that:0.0942,:0.1924 and:0.1416 I:0.3477)
 I'm a language model.
I'm a language
6350: sample 0: Hello, I'm a language model,
------
		(.:0.9141 endors:0.0056ublished:0.0070's:0.1777 a:0.8516 language:0.8789 model:0.0034,:1.0000)
		(,:0.2109
:0.0014ces:0.0096ought:0.0067ces:0.0063,:0.0092,:0.0165
:0.0161)
		( and:0.1387 in:0.0028 in:0.0042 present:0.0029 second:0.0038,:0.0095,:0.0123 and:0.0085)
		( and:0.1099 and:0.0067 in:0.0062 present:0.0040 second:0.0071,:0.0123,:0.0181 and:0.0162)
		( and:0.0781 and:0.0107 and:0.0096 not:0.0105 second:0.0092,:0.0173,:0.0325 and:0.0277)
		(,:0.0586 and:0.0118 was:0.0125 not:0.0219 second:0.0079 arts:0.0302,:0.0596 and:0.0444)
		(,:0.0454 which:0.0149 was:0.0234 not:0.0378 great:0.0145 arts:0.0349,:0.0991 and:0.0894)
		(,:0.0369 and:0.0212 am:0.0466 not:0.0635 great:0.0311 that:0.0403,:0.1299 and:0.1455)
		(,:0.0317 and:0.0391 am:0.0752 not:0.0913 great:0.0400 that:0.0564 that:0.1514 and:0.1826)
		(,:0.0311 and:0.0605 am:0.0977 not:0.1216 great:0.0352 that:0.0713,:0.1650 and:0.2080)
		(,:0.0364 and:0.0815 am:0.0981 not:0.1436 great:0.0270 that:0.0728,:0.1826 and:0.2266)
		(,:0.0500 and:0.1040 have:0.0859 not:0.1494 little:0.0293 that:0.0664,:0.1943 and:0.2266)
		(,:0.0752 and:0.1108 have:0.1040 not:0.1445 little:0.0322 that:0.0586,:0.1914 and:0.2129)
 and
------
		( endors:0.0056ublished:0.0070's:0.1777 a:0.8516 language:0.8789 model:0.0034,:1.0000 and:1.0000)
		(
:0.0014ces:0.0096ought:0.0067ces:0.0063,:0.0092,:0.0165
:0.0161 and:0.0322)
		( in:0.0028 in:0.0042 present:0.0029 second:0.0038,:0.0095,:0.0123 and:0.0085 to:0.0128)
		( and:0.0067 in:0.0062 present:0.0040 second:0.0071,:0.0123,:0.0181 and:0.0162 is:0.0219)
		( and:0.0107 and:0.0096 not:0.0105 second:0.0092,:0.0173,:0.0325 and:0.0277 is:0.0339)
		( and:0.0118 was:0.0125 not:0.0219 second:0.0079 arts:0.0302,:0.0596 and:0.0444 the:0.0381)
		( which:0.0149 was:0.0234 not:0.0378 great:0.0145 arts:0.0349,:0.0991 and:0.0894 the:0.0522)
		( and:0.0212 am:0.0466 not:0.0635 great:0.0311 that:0.0403,:0.1299 and:0.1455 I:0.0986)
		( and:0.0391 am:0.0752 not:0.0913 great:0.0400 that:0.0564 that:0.1514 and:0.1826 I:0.3027)
		( and:0.0605 am:0.0977 not:0.1216 great:0.0352 that:0.0713,:0.1650 and:0.2080 I:0.5156)
		( and:0.0815 am:0.0981 not:0.1436 great:0.0270 that:0.0728,:0.1826 and:0.2266 I:0.5430)
		( and:0.1040 have:0.0859 not:0.1494 little:0.0293 that:0.0664,:0.1943 and:0.2266 I:0.4805)
		( and:0.1108 have:0.1040 not:0.1445 little:0.0322 that:0.0586,:0.1914 and:0.2129 I:0.3867)
 I'm a language model.
I'm a language
6500: sample 0: Hello, I'm a language model,
------
		(.:0.8750!!":0.0050ublished:0.0085'm:0.1777 a:0.8945 language:0.8867 model:0.0085,:1.0000)
		(,:0.1904
:0.0015cing:0.0101ms:0.0036 second:0.0040 over:0.0104 space:0.0150
:0.0129)
		( and:0.1533
:0.0023 over:0.0025 say:0.0024 second:0.0062 in:0.0095 space:0.0062 and:0.0104)
		( and:0.1138 and:0.0049 created:0.0045 say:0.0037 second:0.0137 is:0.0140 of:0.0098 and:0.0159)
		( and:0.0835 and:0.0080 created:0.0084 not:0.0113 second:0.0187 is:0.0215 of:0.0228 and:0.0226)
		( and:0.0583 and:0.0103 was:0.0135 not:0.0233 second:0.0165,:0.0227 of:0.0479 and:0.0400)
		(,:0.0452 and:0.0142 was:0.0250 not:0.0422 great:0.0142 that:0.0403 that:0.0850 and:0.1050)
		(,:0.0371 and:0.0247 was:0.0391 not:0.0664 great:0.0289 that:0.0767 that:0.1621 and:0.1885)
		(,:0.0305 and:0.0464 am:0.0598 not:0.0854 great:0.0400 that:0.1147 that:0.2305 and:0.2461)
		(,:0.0288 and:0.0645 am:0.0767 not:0.0991 great:0.0413 that:0.1426 that:0.2617 and:0.2617)
		(.:0.0342 and:0.0874 am:0.0801 not:0.1045 great:0.0381 that:0.1396 that:0.2451 and:0.2412)
		(.:0.0474 and:0.1021 am:0.0728 not:0.1016 great:0.0359 that:0.1235 that:0.2148 and:0.2236)
		(.:0.0723 and:0.1035 have:0.0757 not:0.0923 great:0.0347 nerd:0.1270 that:0.1816 and:0.2100)
 and
------
		(!!":0.0050ublished:0.0085'm:0.1777 a:0.8945 language:0.8867 model:0.0085,:1.0000 and:1.0000)
		(
:0.0015cing:0.0101ms:0.0036 second:0.0040 over:0.0104 space:0.0150
:0.0129 and:0.0342)
		(
:0.0023 over:0.0025 say:0.0024 second:0.0062 in:0.0095 space:0.0062 and:0.0104 and:0.0124)
		( and:0.0049 created:0.0045 say:0.0037 second:0.0137 is:0.0140 of:0.0098 and:0.0159 the:0.0200)
		( and:0.0080 created:0.0084 not:0.0113 second:0.0187 is:0.0215 of:0.0228 and:0.0226 the:0.0344)
		( and:0.0103 was:0.0135 not:0.0233 second:0.0165,:0.0227 of:0.0479 and:0.0400 the:0.0452)
		( and:0.0142 was:0.0250 not:0.0422 great:0.0142 that:0.0403 that:0.0850 and:0.1050 the:0.0583)
		( and:0.0247 was:0.0391 not:0.0664 great:0.0289 that:0.0767 that:0.1621 and:0.1885 I:0.1299)
		( and:0.0464 am:0.0598 not:0.0854 great:0.0400 that:0.1147 that:0.2305 and:0.2461 I:0.4746)
		( and:0.0645 am:0.0767 not:0.0991 great:0.0413 that:0.1426 that:0.2617 and:0.2617 I:0.6602)
		( and:0.0874 am:0.0801 not:0.1045 great:0.0381 that:0.1396 that:0.2451 and:0.2412 I:0.6367)
		( and:0.1021 am:0.0728 not:0.1016 great:0.0359 that:0.1235 that:0.2148 and:0.2236 I:0.5586)
		( and:0.1035 have:0.0757 not:0.0923 great:0.0347 nerd:0.1270 that:0.1816 and:0.2100 I:0.4551)
 I'm a language model. I'm a language model
6650: sample 0: Hello, I'm a language model,
------
		(.:0.7773!!":0.0058ublished:0.0087'm:0.1836 a:0.9102 language:0.9102 model:0.0121,:1.0000)
		(,:0.2451
:0.0066ces:0.0130
:0.0067
:0.0442 over:0.0153 space:0.0142
:0.0391)
		( and:0.1465 and:0.0034 over:0.0052mon:0.0024
:0.0062 in:0.0084 of:0.0115 and:0.0115)
		( and:0.1069 and:0.0077 in:0.0037 and:0.0039
:0.0090 in:0.0085 of:0.0200 and:0.0164)
		( and:0.0742 and:0.0140 and:0.0063 not:0.0104 second:0.0088 language:0.0144 of:0.0405 and:0.0223)
		(,:0.0532 and:0.0167 was:0.0117 not:0.0221 second:0.0075 arts:0.0210 of:0.0767 and:0.0383)
		(,:0.0420 and:0.0193 was:0.0216 not:0.0466 great:0.0115 that:0.0289 of:0.1216 and:0.0786)
		(,:0.0364 and:0.0272 was:0.0330 not:0.0884 great:0.0199 that:0.0500 of:0.1445 and:0.1299)
		(,:0.0356 and:0.0430 was:0.0403 not:0.1299 great:0.0249 that:0.0728 that:0.1758 and:0.1631)
		(,:0.0400 and:0.0586 have:0.0481 not:0.1582 very:0.0253 that:0.0815 that:0.1895 and:0.1875)
		(,:0.0525 and:0.0767 have:0.0654 not:0.1689 very:0.0210 that:0.0684 that:0.1777 and:0.1885)
		(,:0.0762 and:0.0947 have:0.0884 not:0.1650 member:0.0228 teacher:0.0771 that:0.1553 and:0.1816)
		(,:0.1084 and:0.1011 have:0.1152 not:0.1494 student:0.0256 teacher:0.1030,:0.1338 and:0.1611)
 and
------
		(!!":0.0058ublished:0.0087'm:0.1836 a:0.9102 language:0.9102 model:0.0121,:1.0000 and:1.0000)
		(
:0.0066ces:0.0130
:0.0067
:0.0442 over:0.0153 space:0.0142
:0.0391
:0.0444)
		( and:0.0034 over:0.0052mon:0.0024
:0.0062 in:0.0084 of:0.0115 and:0.0115 also:0.0149)
		( and:0.0077 in:0.0037 and:0.0039
:0.0090 in:0.0085 of:0.0200 and:0.0164 the:0.0176)
		( and:0.0140 and:0.0063 not:0.0104 second:0.0088 language:0.0144 of:0.0405 and:0.0223 the:0.0322)
		( and:0.0167 was:0.0117 not:0.0221 second:0.0075 arts:0.0210 of:0.0767 and:0.0383 the:0.0364)
		( and:0.0193 was:0.0216 not:0.0466 great:0.0115 that:0.0289 of:0.1216 and:0.0786 the:0.0415)
		( and:0.0272 was:0.0330 not:0.0884 great:0.0199 that:0.0500 of:0.1445 and:0.1299 it:0.0645)
		( and:0.0430 was:0.0403 not:0.1299 great:0.0249 that:0.0728 that:0.1758 and:0.1631 I:0.1621)
		( and:0.0586 have:0.0481 not:0.1582 very:0.0253 that:0.0815 that:0.1895 and:0.1875 I:0.3066)
		( and:0.0767 have:0.0654 not:0.1689 very:0.0210 that:0.0684 that:0.1777 and:0.1885 I:0.3223)
		( and:0.0947 have:0.0884 not:0.1650 member:0.0228 teacher:0.0771 that:0.1553 and:0.1816 I:0.2715)
		( and:0.1011 have:0.1152 not:0.1494 student:0.0256 teacher:0.1030,:0.1338 and:0.1611 I:0.2051)
 I'm a language model. I'm a language model
6800: sample 0: Hello, I'm a language model,
------
		(.:0.6523ivably:0.0059ublished:0.0104'm:0.2461 a:0.9258 language:0.8984 model:0.0105,:1.0000)
		(,:0.2100 fatty:0.0012cing:0.0073ring:0.0043
:0.0115 over:0.0096 of:0.0064
:0.0177)
		( and:0.1279
:0.0023 over:0.0038mon:0.0020 second:0.0073 in:0.0084 of:0.0119 and:0.0097)
		( and:0.0972 in:0.0050 in:0.0055
:0.0028 second:0.0137 is:0.0092 of:0.0237 and:0.0176)
		( and:0.0693 and:0.0092 and:0.0107 not:0.0105 second:0.0168 is:0.0146 of:0.0503 is:0.0281)
		(,:0.0510 the:0.0117 and:0.0166 not:0.0248 second:0.0129,:0.0211 of:0.0874 and:0.0383)
		(,:0.0393 the:0.0161 was:0.0242 not:0.0527 great:0.0205,:0.0304 of:0.1328 and:0.0747)
		(,:0.0312 which:0.0223 am:0.0442 not:0.1050 great:0.0449 that:0.0513 of:0.1484 and:0.1064)
		(,:0.0273 and:0.0320 am:0.0762 not:0.1562 great:0.0618 that:0.0957 that:0.1836 and:0.1416)
		(,:0.0280 and:0.0425 am:0.1006 not:0.2002 great:0.0569 that:0.1406 that:0.2227 and:0.1650)
		(,:0.0347 and:0.0593 am:0.1030 not:0.2305 great:0.0447 that:0.1602 that:0.2236 and:0.1807)
		(,:0.0515 and:0.0762 have:0.0869 not:0.2412 great:0.0359 that:0.1533 that:0.2031 and:0.1836)
		(,:0.0776 and:0.0835 have:0.1021 not:0.2197 student:0.0334 that:0.1396 that:0.1836 and:0.1729)
 and
------
		(ivably:0.0059ublished:0.0104'm:0.2461 a:0.9258 language:0.8984 model:0.0105,:1.0000 and:1.0000)
		( fatty:0.0012cing:0.0073ring:0.0043
:0.0115 over:0.0096 of:0.0064
:0.0177 and:0.0292)
		(
:0.0023 over:0.0038mon:0.0020 second:0.0073 in:0.0084 of:0.0119 and:0.0097 and:0.0106)
		( in:0.0050 in:0.0055
:0.0028 second:0.0137 is:0.0092 of:0.0237 and:0.0176 is:0.0221)
		( and:0.0092 and:0.0107 not:0.0105 second:0.0168 is:0.0146 of:0.0503 is:0.0281 the:0.0334)
		( the:0.0117 and:0.0166 not:0.0248 second:0.0129,:0.0211 of:0.0874 and:0.0383 the:0.0479)
		( the:0.0161 was:0.0242 not:0.0527 great:0.0205,:0.0304 of:0.1328 and:0.0747 then:0.0571)
		( which:0.0223 am:0.0442 not:0.1050 great:0.0449 that:0.0513 of:0.1484 and:0.1064 then:0.0913)
		( and:0.0320 am:0.0762 not:0.1562 great:0.0618 that:0.0957 that:0.1836 and:0.1416 I:0.2158)
		( and:0.0425 am:0.1006 not:0.2002 great:0.0569 that:0.1406 that:0.2227 and:0.1650 I:0.3574)
		( and:0.0593 am:0.1030 not:0.2305 great:0.0447 that:0.1602 that:0.2236 and:0.1807 I:0.3906)
		( and:0.0762 have:0.0869 not:0.2412 great:0.0359 that:0.1533 that:0.2031 and:0.1836 I:0.3711)
		( and:0.0835 have:0.1021 not:0.2197 student:0.0334 that:0.1396 that:0.1836 and:0.1729 I:0.3086)
 I'm a language model.
I'm a language
6950: sample 0: Hello, I'm a language model,
------
		(.:0.5195ridden:0.0047ublished:0.0106'm:0.2832 a:0.9219 language:0.8867 model:0.0234,:1.0000)
		(,:0.1494ids:0.0016ces:0.0071mon:0.0025
:0.0084 on:0.0056 space:0.0106
:0.0133)
		( and:0.1221
:0.0022 over:0.0018mon:0.0023 second:0.0059 in:0.0057 of:0.0105 and:0.0093)
		( and:0.0942 and:0.0047 and:0.0031 not:0.0074 second:0.0088 as:0.0079 of:0.0244 and:0.0222)
		( and:0.0684 and:0.0086 and:0.0070 not:0.0283 great:0.0095 that:0.0143 of:0.0544 and:0.0347)
		(,:0.0435 when:0.0137�:0.0125 not:0.0579 great:0.0247 that:0.0298 of:0.0981 and:0.0640)
		(,:0.0309 but:0.0221�:0.0281 not:0.0918 great:0.0483 that:0.0491 of:0.1289 and:0.0918)
		( the:0.0221 but:0.0320�:0.0498 not:0.1108 great:0.0732 that:0.0864 that:0.1533 and:0.1089)
		( the:0.0182 but:0.0391�:0.0659 not:0.1245 great:0.0752 that:0.1377 that:0.1973 and:0.1235)
		(.:0.0147 the:0.0398�:0.0723 not:0.1387 great:0.0581 that:0.1699 that:0.2061 and:0.1357)
		(.:0.0173 the:0.0527�:0.0718 not:0.1514 great:0.0388 that:0.1699 that:0.1934 and:0.1455)
		(.:0.0240 the:0.0688 think:0.0767 not:0.1533 great:0.0267 that:0.1396 that:0.1689 and:0.1465)
		(.:0.0354 the:0.0811 think:0.0796 not:0.1475 believer:0.0229 that:0.1050 that:0.1445 a:0.1504)
 a
------
		(ridden:0.0047ublished:0.0106'm:0.2832 a:0.9219 language:0.8867 model:0.0234,:1.0000 a:0.9648)
		(ids:0.0016ces:0.0071mon:0.0025
:0.0084 on:0.0056 space:0.0106
:0.0133
:0.0105)
		(
:0.0022 over:0.0018mon:0.0023 second:0.0059 in:0.0057 of:0.0105 and:0.0093 second:0.0058)
		( and:0.0047 and:0.0031 not:0.0074 second:0.0088 as:0.0079 of:0.0244 and:0.0222
:0.0087)
		( and:0.0086 and:0.0070 not:0.0283 great:0.0095 that:0.0143 of:0.0544 and:0.0347
:0.0070)
		( when:0.0137�:0.0125 not:0.0579 great:0.0247 that:0.0298 of:0.0981 and:0.0640 free:0.0080)
		( but:0.0221�:0.0281 not:0.0918 great:0.0483 that:0.0491 of:0.1289 and:0.0918 language:0.0107)
		( but:0.0320�:0.0498 not:0.1108 great:0.0732 that:0.0864 that:0.1533 and:0.1089 process:0.0140)
		( but:0.0391�:0.0659 not:0.1245 great:0.0752 that:0.1377 that:0.1973 and:0.1235 program:0.0181)
		( the:0.0398�:0.0723 not:0.1387 great:0.0581 that:0.1699 that:0.2061 and:0.1357 program:0.0194)
		( the:0.0527�:0.0718 not:0.1514 great:0.0388 that:0.1699 that:0.1934 and:0.1455 program:0.0212)
		( the:0.0688 think:0.0767 not:0.1533 great:0.0267 that:0.1396 that:0.1689 and:0.1465 program:0.0232)
		( the:0.0811 think:0.0796 not:0.1475 believer:0.0229 that:0.1050 that:0.1445 a:0.1504 model:0.0269)
 model of the human brain.
The human brain is
7100: sample 0: Hello, I'm a language model,
------
		(.:0.3945ridden:0.0039ublished:0.0084'm:0.2256 a:0.9336 language:0.8594 model:0.0221,:1.0000)
		(,:0.1826ids:0.0015ces:0.0059mon:0.0028ces:0.0043 over:0.0067 space:0.0093
:0.0152)
		( and:0.1196
:0.0020 over:0.0028mon:0.0017 second:0.0041 in:0.0054 of:0.0067 and:0.0086)
		( and:0.0889
:0.0031�:0.0026 not:0.0047
:0.0078,:0.0077 of:0.0170 and:0.0129)
		( and:0.0630 and:0.0045�:0.0072 not:0.0159 more:0.0066,:0.0144 of:0.0510 and:0.0197)
		(,:0.0459 and:0.0053�:0.0150 not:0.0359 great:0.0131,:0.0228 of:0.1040 which:0.0359)
		(,:0.0342 which:0.0081�:0.0299 not:0.0747 great:0.0278,:0.0289 of:0.1475 which:0.0703)
		(,:0.0264 which:0.0151�:0.0496 not:0.1230 great:0.0464-:0.0405 of:0.1475 which:0.1147)
		(,:0.0220 and:0.0283�:0.0635 not:0.1611 great:0.0532-:0.0520 that:0.1943 but:0.1406)
		(,:0.0212 and:0.0552�:0.0625 not:0.1895 great:0.0459-:0.0608 that:0.2012 and:0.1553)
		(.:0.0289 and:0.0898 have:0.0674 not:0.2080 great:0.0342-:0.0593 that:0.1797 and:0.1777)
		(,:0.0479 and:0.1367 have:0.0850 not:0.2090 little:0.0359 teacher:0.0500.:0.2109 and:0.1816)
		(,:0.0908 and:0.1582 have:0.1035 not:0.1895 little:0.0398 lear:0.0659.:0.2334 and:0.1777)
 and
------
		(ridden:0.0039ublished:0.0084'm:0.2256 a:0.9336 language:0.8594 model:0.0221,:1.0000 and:1.0000)
		(ids:0.0015ces:0.0059mon:0.0028ces:0.0043 over:0.0067 space:0.0093
:0.0152 and:0.0339)
		(
:0.0020 over:0.0028mon:0.0017 second:0.0041 in:0.0054 of:0.0067 and:0.0086 also:0.0105)
		(
:0.0031�:0.0026 not:0.0047
:0.0078,:0.0077 of:0.0170 and:0.0129 is:0.0160)
		( and:0.0045�:0.0072 not:0.0159 more:0.0066,:0.0144 of:0.0510 and:0.0197 the:0.0228)
		( and:0.0053�:0.0150 not:0.0359 great:0.0131,:0.0228 of:0.1040 which:0.0359 the:0.0258)
		( which:0.0081�:0.0299 not:0.0747 great:0.0278,:0.0289 of:0.1475 which:0.0703 I:0.0476)
		( which:0.0151�:0.0496 not:0.1230 great:0.0464-:0.0405 of:0.1475 which:0.1147 I:0.1445)
		( and:0.0283�:0.0635 not:0.1611 great:0.0532-:0.0520 that:0.1943 but:0.1406 I:0.4043)
		( and:0.0552�:0.0625 not:0.1895 great:0.0459-:0.0608 that:0.2012 and:0.1553 I:0.5430)
		( and:0.0898 have:0.0674 not:0.2080 great:0.0342-:0.0593 that:0.1797 and:0.1777 I:0.5312)
		( and:0.1367 have:0.0850 not:0.2090 little:0.0359 teacher:0.0500.:0.2109 and:0.1816 I:0.4824)
		( and:0.1582 have:0.1035 not:0.1895 little:0.0398 lear:0.0659.:0.2334 and:0.1777 I:0.3945)
 I'm a language model.
I'm not a
7250: sample 0: Hello, I'm a language model,
------
		(.:0.2695ivably:0.0047 I:0.0067'm:0.2354 a:0.9453 language:0.8438 model:0.0250,:1.0000)
		(,:0.1865 paste:0.0012ces:0.0124mon:0.0046ces:0.0048 over:0.0065 space:0.0105
:0.0168)
		( and:0.1260 and:0.0020 over:0.0028mon:0.0031 second:0.0046 in:0.0061 of:0.0065 and:0.0141)
		( and:0.0928 and:0.0056 over:0.0032 the:0.0032 second:0.0091 in:0.0090 of:0.0182 and:0.0236)
		( and:0.0649 and:0.0093 and:0.0085 the:0.0123 second:0.0103,:0.0134 of:0.0457 and:0.0295)
		(,:0.0447 and:0.0107 and:0.0129 not:0.0232 great:0.0090-:0.0190 of:0.0869 and:0.0588)
		(,:0.0312 which:0.0135�:0.0193 not:0.0493 great:0.0238-:0.0364 of:0.1289 and:0.1021)
		(,:0.0231 which:0.0190�:0.0327 not:0.0986 great:0.0513-:0.0552 that:0.1963 and:0.1455)
		(,:0.0186 and:0.0251�:0.0452 not:0.1406 great:0.0640-:0.0732 that:0.2871 and:0.2197)
		(,:0.0175 and:0.0366�:0.0491 not:0.1719 great:0.0562 that:0.0923 that:0.3105 and:0.2734)
		(.:0.0204 and:0.0486 am:0.0598 not:0.1973 great:0.0447-:0.0938 that:0.2715 and:0.2949)
		(.:0.0315 and:0.0630 am:0.0601 not:0.2109 great:0.0364 that:0.0811 that:0.2256 and:0.2871)
		(.:0.0503 and:0.0703.:0.0757 not:0.2002 great:0.0315 lear:0.0718 that:0.1797 and:0.2617)
 and
------
		(ivably:0.0047 I:0.0067'm:0.2354 a:0.9453 language:0.8438 model:0.0250,:1.0000 and:1.0000)
		( paste:0.0012ces:0.0124mon:0.0046ces:0.0048 over:0.0065 space:0.0105
:0.0168 and:0.0510)
		( and:0.0020 over:0.0028mon:0.0031 second:0.0046 in:0.0061 of:0.0065 and:0.0141 and:0.0166)
		( and:0.0056 over:0.0032 the:0.0032 second:0.0091 in:0.0090 of:0.0182 and:0.0236 is:0.0293)
		( and:0.0093 and:0.0085 the:0.0123 second:0.0103,:0.0134 of:0.0457 and:0.0295 the:0.0364)
		( and:0.0107 and:0.0129 not:0.0232 great:0.0090-:0.0190 of:0.0869 and:0.0588 the:0.0378)
		( which:0.0135�:0.0193 not:0.0493 great:0.0238-:0.0364 of:0.1289 and:0.1021 the:0.0325)
		( which:0.0190�:0.0327 not:0.0986 great:0.0513-:0.0552 that:0.1963 and:0.1455 a:0.0708)
		( and:0.0251�:0.0452 not:0.1406 great:0.0640-:0.0732 that:0.2871 and:0.2197 a:0.1055)
		( and:0.0366�:0.0491 not:0.1719 great:0.0562 that:0.0923 that:0.3105 and:0.2734 I:0.1797)
		( and:0.0486 am:0.0598 not:0.1973 great:0.0447-:0.0938 that:0.2715 and:0.2949 I:0.2461)
		( and:0.0630 am:0.0601 not:0.2109 great:0.0364 that:0.0811 that:0.2256 and:0.2871 I:0.2676)
		( and:0.0703.:0.0757 not:0.2002 great:0.0315 lear:0.0718 that:0.1797 and:0.2617 I:0.2559)
 I'm a computer model.
I'm a computer
7400: sample 0: Hello, I'm a language model,
------
		(.:0.1328,:0.0081 I:0.0155'm:0.2217 a:0.9531 language:0.8555 model:0.0405,:1.0000)
		(,:0.1719
:0.0012ces:0.0089mon:0.0033 second:0.0029 between:0.0056 space:0.0108
:0.0160)
		( and:0.1157
:0.0018 in:0.0025mon:0.0021 second:0.0038 in:0.0058 of:0.0067 and:0.0087)
		( and:0.0869 and:0.0035 in:0.0041 also:0.0045 second:0.0072 in:0.0076 of:0.0160 and:0.0127)
		(,:0.0688 and:0.0063 and:0.0073 not:0.0105 second:0.0091,:0.0126 of:0.0427 and:0.0179)
		(,:0.0540 which:0.0121�:0.0108 not:0.0225 second:0.0085 that:0.0214 of:0.0854 which:0.0349)
		(,:0.0420 which:0.0216�:0.0187 not:0.0442 great:0.0120 that:0.0371 of:0.1191 and:0.0664)
		(,:0.0327 which:0.0352�:0.0294 not:0.0806 great:0.0264 that:0.0596 that:0.1426 and:0.1138)
		(,:0.0288 which:0.0488�:0.0405 not:0.1211 great:0.0366 that:0.0869 that:0.1777 and:0.1719)
		(,:0.0294 which:0.0564�:0.0476 not:0.1738 great:0.0369 that:0.1055 that:0.1787 and:0.2197)
		(,:0.0378 which:0.0540�:0.0535 not:0.2305 great:0.0325 that:0.1084 that:0.1514 and:0.2393)
		(,:0.0623 and:0.0618�:0.0635 not:0.2637 great:0.0293 that:0.0947,:0.1426 and:0.2363)
		(,:0.1040 and:0.0718�:0.0776 not:0.2520 great:0.0265 that:0.0815.:0.1592 and:0.2217)
 and
------
		(,:0.0081 I:0.0155'm:0.2217 a:0.9531 language:0.8555 model:0.0405,:1.0000 and:1.0000)
		(
:0.0012ces:0.0089mon:0.0033 second:0.0029 between:0.0056 space:0.0108
:0.0160
:0.0298)
		(
:0.0018 in:0.0025mon:0.0021 second:0.0038 in:0.0058 of:0.0067 and:0.0087 also:0.0110)
		( and:0.0035 in:0.0041 also:0.0045 second:0.0072 in:0.0076 of:0.0160 and:0.0127 is:0.0139)
		( and:0.0063 and:0.0073 not:0.0105 second:0.0091,:0.0126 of:0.0427 and:0.0179 the:0.0214)
		( which:0.0121�:0.0108 not:0.0225 second:0.0085 that:0.0214 of:0.0854 which:0.0349 the:0.0289)
		( which:0.0216�:0.0187 not:0.0442 great:0.0120 that:0.0371 of:0.1191 and:0.0664 I:0.0510)
		( which:0.0352�:0.0294 not:0.0806 great:0.0264 that:0.0596 that:0.1426 and:0.1138 I:0.1318)
		( which:0.0488�:0.0405 not:0.1211 great:0.0366 that:0.0869 that:0.1777 and:0.1719 I:0.2637)
		( which:0.0564�:0.0476 not:0.1738 great:0.0369 that:0.1055 that:0.1787 and:0.2197 I:0.3945)
		( which:0.0540�:0.0535 not:0.2305 great:0.0325 that:0.1084 that:0.1514 and:0.2393 I:0.4512)
		( and:0.0618�:0.0635 not:0.2637 great:0.0293 that:0.0947,:0.1426 and:0.2363 I:0.4766)
		( and:0.0718�:0.0776 not:0.2520 great:0.0265 that:0.0815.:0.1592 and:0.2217 I:0.4277)
 I'm a language model.
I'm a language
7550: sample 0: Hello, I'm a language model,
------
		(.:0.0806,:0.0187 I:0.0162'm:0.3750 a:0.9531 language:0.8125 model:0.0374,:1.0000)
		(,:0.1631 paste:0.0012ces:0.0071mon:0.0033ces:0.0034 over:0.0028 space:0.0120
:0.0104)
		( and:0.1182
:0.0026 over:0.0027 present:0.0028
:0.0038 at:0.0037,:0.0033 and:0.0146)
		(,:0.0854 and:0.0068 and:0.0044 not:0.0043
:0.0076,:0.0067,:0.0107 and:0.0170)
		(,:0.0645 and:0.0117 and:0.0108 not:0.0171 more:0.0087,:0.0147 of:0.0327 and:0.0188)
		(,:0.0459 and:0.0115 and:0.0130 not:0.0374 very:0.0081,:0.0264 of:0.0703 which:0.0398)
		(,:0.0325 which:0.0152�:0.0242 not:0.0703 great:0.0182 that:0.0398 that:0.1074 which:0.0620)
		(,:0.0250 which:0.0236�:0.0525 not:0.1157 great:0.0352 that:0.0674 that:0.1611 and:0.1011)
		(,:0.0210 and:0.0396�:0.0825 not:0.1523 great:0.0459 that:0.1089 that:0.1836 and:0.1611)
		(,:0.0209 and:0.0596�:0.0967 not:0.1943 great:0.0413 that:0.1396 that:0.1719 and:0.2051)
		(.:0.0267 and:0.0713�:0.1011 not:0.2275 great:0.0334 that:0.1436 that:0.1396 and:0.2207)
		(.:0.0430 and:0.0830�:0.1108 not:0.2305 great:0.0266 that:0.1318.:0.1494 and:0.2168)
		(.:0.0659 and:0.0859�:0.1187 not:0.2197 student:0.0304 that:0.1123.:0.1650 and:0.2100)
 and
------
		(,:0.0187 I:0.0162'm:0.3750 a:0.9531 language:0.8125 model:0.0374,:1.0000 and:1.0000)
		( paste:0.0012ces:0.0071mon:0.0033ces:0.0034 over:0.0028 space:0.0120
:0.0104
:0.0288)
		(
:0.0026 over:0.0027 present:0.0028
:0.0038 at:0.0037,:0.0033 and:0.0146 also:0.0156)
		( and:0.0068 and:0.0044 not:0.0043
:0.0076,:0.0067,:0.0107 and:0.0170 is:0.0168)
		( and:0.0117 and:0.0108 not:0.0171 more:0.0087,:0.0147 of:0.0327 and:0.0188 is:0.0233)
		( and:0.0115 and:0.0130 not:0.0374 very:0.0081,:0.0264 of:0.0703 which:0.0398 the:0.0275)
		( which:0.0152�:0.0242 not:0.0703 great:0.0182 that:0.0398 that:0.1074 which:0.0620 so:0.0378)
		( which:0.0236�:0.0525 not:0.1157 great:0.0352 that:0.0674 that:0.1611 and:0.1011 I:0.1455)
		( and:0.0396�:0.0825 not:0.1523 great:0.0459 that:0.1089 that:0.1836 and:0.1611 I:0.3887)
		( and:0.0596�:0.0967 not:0.1943 great:0.0413 that:0.1396 that:0.1719 and:0.2051 I:0.5312)
		( and:0.0713�:0.1011 not:0.2275 great:0.0334 that:0.1436 that:0.1396 and:0.2207 I:0.5273)
		( and:0.0830�:0.1108 not:0.2305 great:0.0266 that:0.1318.:0.1494 and:0.2168 I:0.4766)
		( and:0.0859�:0.1187 not:0.2197 student:0.0304 that:0.1123.:0.1650 and:0.2100 I:0.3691)
 I'm a language model.
I'm a language
7700: sample 0: Hello, I'm a language model,
------
		(.:0.0364,:0.0630 I:0.0280'm:0.3086 a:0.9531 language:0.8164 model:0.0398,:1.0000)
		(,:0.1787ids:0.0023aling:0.0075ought:0.0037ces:0.0031 in:0.0089 space:0.0145
:0.0076)
		(,:0.1196ids:0.0025 in:0.0037mon:0.0024
:0.0025 in:0.0096 of:0.0078 and:0.0110)
		(,:0.1011 and:0.0035 in:0.0052 most:0.0039 is:0.0054,:0.0115 of:0.0157 and:0.0153)
		(,:0.0820 and:0.0065 and:0.0101 not:0.0115 is:0.0081,:0.0167 of:0.0349 and:0.0199)
		(,:0.0620 and:0.0081 was:0.0186 not:0.0211 is:0.0059,:0.0240 of:0.0591 which:0.0420)
		(,:0.0469 which:0.0127 was:0.0275 not:0.0405 very:0.0110,:0.0312 of:0.0815 and:0.0864)
		(,:0.0364 which:0.0216 was:0.0359 not:0.0747 great:0.0232,:0.0396,:0.0894 and:0.1670)
		(,:0.0320 which:0.0325 was:0.0430 not:0.1128 great:0.0381,:0.0552 that:0.1235 and:0.2500)
		(,:0.0344 and:0.0493 was:0.0518 not:0.1484 great:0.0393,:0.0752 that:0.1387 and:0.3027)
		(,:0.0483 and:0.0674 was:0.0581 not:0.1807 bit:0.0415,:0.0859,:0.1484 and:0.3105)
		(,:0.0864 and:0.0835 have:0.0723 not:0.1885 bit:0.0479,:0.0854,:0.1719 and:0.3066)
		(,:0.1445 and:0.0869 have:0.0933 not:0.1787 bit:0.0464,:0.0801,:0.1865 and:0.2734)
 and
------
		(,:0.0630 I:0.0280'm:0.3086 a:0.9531 language:0.8164 model:0.0398,:1.0000 and:1.0000)
		(ids:0.0023aling:0.0075ought:0.0037ces:0.0031 in:0.0089 space:0.0145
:0.0076
:0.0153)
		(ids:0.0025 in:0.0037mon:0.0024
:0.0025 in:0.0096 of:0.0078 and:0.0110 also:0.0136)
		( and:0.0035 in:0.0052 most:0.0039 is:0.0054,:0.0115 of:0.0157 and:0.0153 is:0.0166)
		( and:0.0065 and:0.0101 not:0.0115 is:0.0081,:0.0167 of:0.0349 and:0.0199 other:0.0327)
		( and:0.0081 was:0.0186 not:0.0211 is:0.0059,:0.0240 of:0.0591 which:0.0420 other:0.0337)
		( which:0.0127 was:0.0275 not:0.0405 very:0.0110,:0.0312 of:0.0815 and:0.0864 therefore:0.0383)
		( which:0.0216 was:0.0359 not:0.0747 great:0.0232,:0.0396,:0.0894 and:0.1670 I:0.1787)
		( which:0.0325 was:0.0430 not:0.1128 great:0.0381,:0.0552 that:0.1235 and:0.2500 I:0.4766)
		( and:0.0493 was:0.0518 not:0.1484 great:0.0393,:0.0752 that:0.1387 and:0.3027 I:0.5859)
		( and:0.0674 was:0.0581 not:0.1807 bit:0.0415,:0.0859,:0.1484 and:0.3105 I:0.5742)
		( and:0.0835 have:0.0723 not:0.1885 bit:0.0479,:0.0854,:0.1719 and:0.3066 I:0.5078)
		( and:0.0869 have:0.0933 not:0.1787 bit:0.0464,:0.0801,:0.1865 and:0.2734 I:0.3867)
 I'm a linguist.
I'm a lingu
7850: sample 0: Hello, I'm a language model,
------
		(Hello:0.0208,:0.1523 I:0.0532'm:0.2715 a:0.9570 language:0.8125 model:0.0503,:1.0000)
		(,:0.1572ned:0.0014aling:0.0101ring:0.0023ces:0.0021 prompt:0.0075 space:0.0112
:0.0065)
		( and:0.1084
:0.0015 in:0.0016 conclusion:0.0012 second:0.0037 as:0.0071 of:0.0033 and:0.0069)
		( and:0.0835 and:0.0033 in:0.0036 said:0.0025 second:0.0078 as:0.0151 of:0.0114
:0.0126)
		(,:0.0605 which:0.0073 was:0.0085 most:0.0075 second:0.0088 as:0.0240 of:0.0447 and:0.0188)
		(,:0.0444 which:0.0146 was:0.0145 not:0.0148 better:0.0137 that:0.0265 of:0.1118 which:0.0381)
		(,:0.0322 which:0.0247 was:0.0187 not:0.0325 better:0.0254 that:0.0515 of:0.1660 and:0.0874)
		(,:0.0242 which:0.0332�:0.0342 not:0.0574 great:0.0410 that:0.0781 of:0.1650 and:0.1562)
		(,:0.0204 which:0.0405�:0.0449 not:0.0713 great:0.0513 that:0.1055 that:0.1816 and:0.1865)
		(,:0.0206 and:0.0391'm:0.0486 not:0.0845 great:0.0503 that:0.1177 that:0.1787 and:0.1875)
		(.:0.0289 and:0.0464'm:0.0498 not:0.0977 great:0.0452 that:0.1094 that:0.1533 and:0.1787)
		(.:0.0505 and:0.0537 have:0.0591 not:0.0996 great:0.0403 that:0.0869.:0.1914 and:0.1650)
		(.:0.0825 and:0.0583 have:0.0723 not:0.0972 great:0.0354 that:0.0742.:0.2051 and:0.1514)
 and
------
		(,:0.1523 I:0.0532'm:0.2715 a:0.9570 language:0.8125 model:0.0503,:1.0000 and:1.0000)
		(ned:0.0014aling:0.0101ring:0.0023ces:0.0021 prompt:0.0075 space:0.0112
:0.0065
:0.0272)
		(
:0.0015 in:0.0016 conclusion:0.0012 second:0.0037 as:0.0071 of:0.0033 and:0.0069 and:0.0090)
		( and:0.0033 in:0.0036 said:0.0025 second:0.0078 as:0.0151 of:0.0114
:0.0126 the:0.0194)
		( which:0.0073 was:0.0085 most:0.0075 second:0.0088 as:0.0240 of:0.0447 and:0.0188 the:0.0349)
		( which:0.0146 was:0.0145 not:0.0148 better:0.0137 that:0.0265 of:0.1118 which:0.0381 the:0.0388)
		( which:0.0247 was:0.0187 not:0.0325 better:0.0254 that:0.0515 of:0.1660 and:0.0874 I:0.0635)
		( which:0.0332�:0.0342 not:0.0574 great:0.0410 that:0.0781 of:0.1650 and:0.1562 I:0.2598)
		( which:0.0405�:0.0449 not:0.0713 great:0.0513 that:0.1055 that:0.1816 and:0.1865 I:0.4492)
		( and:0.0391'm:0.0486 not:0.0845 great:0.0503 that:0.1177 that:0.1787 and:0.1875 I:0.4863)
		( and:0.0464'm:0.0498 not:0.0977 great:0.0452 that:0.1094 that:0.1533 and:0.1787 I:0.4551)
		( and:0.0537 have:0.0591 not:0.0996 great:0.0403 that:0.0869.:0.1914 and:0.1650 I:0.3867)
		( and:0.0583 have:0.0723 not:0.0972 great:0.0354 that:0.0742.:0.2051 and:0.1514 I:0.3164)
 I'm a linguist.
I'm a lingu
8000: sample 0: Hello, I'm a language model,
------
		(Hello:0.0168,:0.3555 I:0.0874'm:0.2422 a:0.9531 language:0.8164 model:0.0557,:1.0000)
		(,:0.1787
:0.0012aling:0.0073mon:0.0039ces:0.0023 between:0.0043 space:0.0078
:0.0079)
		(,:0.1035
:0.0022 in:0.0026mon:0.0027 second:0.0033 in:0.0035 of:0.0058 and:0.0064)
		(,:0.0825 when:0.0031 is:0.0058 most:0.0039 is:0.0060 as:0.0075 of:0.0136 and:0.0125)
		(,:0.0630 when:0.0062 was:0.0147 most:0.0112 is:0.0078 language:0.0171 of:0.0413 and:0.0212)
		(,:0.0459 when:0.0077 was:0.0266 not:0.0182 better:0.0098 that:0.0271 of:0.0972 and:0.0374)
		(,:0.0325 which:0.0098�:0.0479 not:0.0332 better:0.0173 that:0.0613 of:0.1553 but:0.0825)
		(,:0.0250 which:0.0166�:0.0801 not:0.0520 great:0.0286 that:0.1040 of:0.1738 but:0.1738)
		(,:0.0210 which:0.0254�:0.0991 not:0.0674 great:0.0334 that:0.1602 that:0.2324 and:0.2480)
		(,:0.0209 and:0.0413�:0.0972 not:0.0903 great:0.0303 that:0.1895 that:0.2637 and:0.2832)
		(.:0.0281 and:0.0574�:0.0850 not:0.1182 good:0.0304 that:0.1689 that:0.2432 and:0.2949)
		(.:0.0520 and:0.0786�:0.0762 not:0.1357 good:0.0283 that:0.1230 that:0.1924 and:0.2773)
		(.:0.0996 and:0.0928�:0.0752 not:0.1387 good:0.0248 that:0.0791 that:0.1436 and:0.2578)
 and
------
		(,:0.3555 I:0.0874'm:0.2422 a:0.9531 language:0.8164 model:0.0557,:1.0000 and:1.0000)
		(
:0.0012aling:0.0073mon:0.0039ces:0.0023 between:0.0043 space:0.0078
:0.0079
:0.0162)
		(
:0.0022 in:0.0026mon:0.0027 second:0.0033 in:0.0035 of:0.0058 and:0.0064 also:0.0070)
		( when:0.0031 is:0.0058 most:0.0039 is:0.0060 as:0.0075 of:0.0136 and:0.0125 is:0.0101)
		( when:0.0062 was:0.0147 most:0.0112 is:0.0078 language:0.0171 of:0.0413 and:0.0212 the:0.0145)
		( when:0.0077 was:0.0266 not:0.0182 better:0.0098 that:0.0271 of:0.0972 and:0.0374 so:0.0220)
		( which:0.0098�:0.0479 not:0.0332 better:0.0173 that:0.0613 of:0.1553 but:0.0825 I:0.0791)
		( which:0.0166�:0.0801 not:0.0520 great:0.0286 that:0.1040 of:0.1738 but:0.1738 I:0.3574)
		( which:0.0254�:0.0991 not:0.0674 great:0.0334 that:0.1602 that:0.2324 and:0.2480 I:0.6328)
		( and:0.0413�:0.0972 not:0.0903 great:0.0303 that:0.1895 that:0.2637 and:0.2832 I:0.6953)
		( and:0.0574�:0.0850 not:0.1182 good:0.0304 that:0.1689 that:0.2432 and:0.2949 I:0.6680)
		( and:0.0786�:0.0762 not:0.1357 good:0.0283 that:0.1230 that:0.1924 and:0.2773 I:0.6367)
		( and:0.0928�:0.0752 not:0.1387 good:0.0248 that:0.0791 that:0.1436 and:0.2578 I:0.5664)
 I'm a linguist. I'm a linguist
8150: sample 0: Hello, I'm a language model,
------
		(Hello:0.0135,:0.5938 I:0.1436'm:0.2891 a:0.9492 language:0.8125 model:0.0605,:1.0000)
		(,:0.1445nea:0.0011aling:0.0063ought:0.0025als:0.0019 over:0.0066 space:0.0099
:0.0086)
		( and:0.0981
:0.0021 shrink:0.0014 conclusion:0.0019 second:0.0021 in:0.0067 of:0.0046 and:0.0073)
		(,:0.0688 when:0.0032 in:0.0022 conclusion:0.0033
:0.0046 as:0.0098 of:0.0145 and:0.0125)
		(,:0.0535 when:0.0061 was:0.0050 not:0.0075 more:0.0055 as:0.0141 of:0.0413 and:0.0203)
		(,:0.0393 when:0.0072 was:0.0091 not:0.0147 better:0.0074 that:0.0157 of:0.0801 and:0.0410)
		(,:0.0298 but:0.0107�:0.0199 not:0.0354 better:0.0137 that:0.0344 that:0.1318 and:0.0913)
		(,:0.0236 but:0.0199�:0.0393 not:0.0786 great:0.0275 that:0.0684 that:0.2490 but:0.1660)
		(,:0.0211 but:0.0334�:0.0576 not:0.1216 great:0.0437 that:0.1123 that:0.3379 and:0.2197)
		(,:0.0215 the:0.0432�:0.0635 not:0.1621 great:0.0479 that:0.1562 that:0.3652 and:0.2354)
		(,:0.0280 and:0.0574�:0.0593 not:0.2031 great:0.0432 that:0.1729 that:0.3301 and:0.2324)
		(,:0.0476 and:0.0786�:0.0591 not:0.2275 great:0.0364 that:0.1592 that:0.2969 and:0.2197)
		(,:0.0815 and:0.0908�:0.0723 not:0.2305 great:0.0322 that:0.1416 that:0.2383 but:0.2100)
 but
------
		(,:0.5938 I:0.1436'm:0.2891 a:0.9492 language:0.8125 model:0.0605,:1.0000 but:0.9375)
		(nea:0.0011aling:0.0063ought:0.0025als:0.0019 over:0.0066 space:0.0099
:0.0086 sometimes:0.0074)
		(
:0.0021 shrink:0.0014 conclusion:0.0019 second:0.0021 in:0.0067 of:0.0046 and:0.0073 also:0.0124)
		( when:0.0032 in:0.0022 conclusion:0.0033
:0.0046 as:0.0098 of:0.0145 and:0.0125 also:0.0190)
		( when:0.0061 was:0.0050 not:0.0075 more:0.0055 as:0.0141 of:0.0413 and:0.0203 also:0.0264)
		( when:0.0072 was:0.0091 not:0.0147 better:0.0074 that:0.0157 of:0.0801 and:0.0410 also:0.0408)
		( but:0.0107�:0.0199 not:0.0354 better:0.0137 that:0.0344 that:0.1318 and:0.0913 also:0.0620)
		( but:0.0199�:0.0393 not:0.0786 great:0.0275 that:0.0684 that:0.2490 but:0.1660 I:0.2266)
		( but:0.0334�:0.0576 not:0.1216 great:0.0437 that:0.1123 that:0.3379 and:0.2197 I:0.4414)
		( the:0.0432�:0.0635 not:0.1621 great:0.0479 that:0.1562 that:0.3652 and:0.2354 I:0.5000)
		( and:0.0574�:0.0593 not:0.2031 great:0.0432 that:0.1729 that:0.3301 and:0.2324 I:0.4473)
		( and:0.0786�:0.0591 not:0.2275 great:0.0364 that:0.1592 that:0.2969 and:0.2197 I:0.3926)
		( and:0.0908�:0.0723 not:0.2305 great:0.0322 that:0.1416 that:0.2383 but:0.2100 I:0.3359)
 I'm not a language model.
- The language
8300: sample 0: Hello, I'm a language model,
------
		(Hello:0.0090,:0.7773 I:0.2637'm:0.2715 a:0.9531 language:0.7656 model:0.0854,:1.0000)
		(,:0.1211nea:0.0012aling:0.0067naire:0.0022ces:0.0025 prompt:0.0069 space:0.0067
:0.0045)
		( and:0.0874 compared:0.0014 compared:0.0016 designed:0.0013 second:0.0032 in:0.0042 of:0.0073 and:0.0044)
		( and:0.0664 when:0.0028 had:0.0022 most:0.0028 second:0.0055 as:0.0079 of:0.0237
:0.0093)
		(,:0.0530 when:0.0065 was:0.0075 most:0.0085 better:0.0079 as:0.0141 of:0.0659 and:0.0173)
		(,:0.0413 when:0.0090 was:0.0146 not:0.0146 better:0.0157,:0.0223 of:0.1030 and:0.0361)
		(,:0.0315 but:0.0139 was:0.0214 not:0.0347 better:0.0283,:0.0280 of:0.1084 but:0.0811)
		(,:0.0249 but:0.0221�:0.0366 not:0.0618 great:0.0352 that:0.0393 that:0.1157 and:0.1416)
		(,:0.0214 but:0.0312�:0.0510 not:0.0801 great:0.0430 that:0.0564 that:0.1436 and:0.2021)
		(,:0.0211 and:0.0437�:0.0557 not:0.0952 great:0.0408 that:0.0674 that:0.1396 and:0.2256)
		(,:0.0253 and:0.0645�:0.0566 not:0.1113 great:0.0344 teacher:0.0669.:0.1445 and:0.2256)
		(.:0.0422 and:0.0830 think:0.0728 not:0.1206 little:0.0352 teacher:0.1147.:0.1846 and:0.2021)
		(.:0.0776 and:0.0928 think:0.0806 not:0.1201 little:0.0364 teacher:0.1494.:0.2041 and:0.1816)
 and
------
		(,:0.7773 I:0.2637'm:0.2715 a:0.9531 language:0.7656 model:0.0854,:1.0000 and:1.0000)
		(nea:0.0012aling:0.0067naire:0.0022ces:0.0025 prompt:0.0069 space:0.0067
:0.0045
:0.0142)
		( compared:0.0014 compared:0.0016 designed:0.0013 second:0.0032 in:0.0042 of:0.0073 and:0.0044 also:0.0087)
		( when:0.0028 had:0.0022 most:0.0028 second:0.0055 as:0.0079 of:0.0237
:0.0093 is:0.0120)
		( when:0.0065 was:0.0075 most:0.0085 better:0.0079 as:0.0141 of:0.0659 and:0.0173 other:0.0254)
		( when:0.0090 was:0.0146 not:0.0146 better:0.0157,:0.0223 of:0.1030 and:0.0361 so:0.0354)
		( but:0.0139 was:0.0214 not:0.0347 better:0.0283,:0.0280 of:0.1084 but:0.0811 I:0.1113)
		( but:0.0221�:0.0366 not:0.0618 great:0.0352 that:0.0393 that:0.1157 and:0.1416 I:0.4180)
		( but:0.0312�:0.0510 not:0.0801 great:0.0430 that:0.0564 that:0.1436 and:0.2021 I:0.5664)
		( and:0.0437�:0.0557 not:0.0952 great:0.0408 that:0.0674 that:0.1396 and:0.2256 I:0.5391)
		( and:0.0645�:0.0566 not:0.1113 great:0.0344 teacher:0.0669.:0.1445 and:0.2256 I:0.4609)
		( and:0.0830 think:0.0728 not:0.1206 little:0.0352 teacher:0.1147.:0.1846 and:0.2021 I:0.3809)
		( and:0.0928 think:0.0806 not:0.1201 little:0.0364 teacher:0.1494.:0.2041 and:0.1816 I:0.3105)
 I'm a language model.
I'm a language
8450: sample 0: Hello, I'm a language model,
------
		(Hello:0.0115,:0.9023 I:0.3184'm:0.2090 a:0.9453 language:0.7266 model:0.0869,:1.0000)
		(,:0.1357umatic:0.0014aling:0.0095ought:0.0019ning:0.0018 prompt:0.0037 space:0.0093
:0.0031)
		( and:0.1035 compared:0.0011 compared:0.0016 designed:0.0021 second:0.0019 in:0.0026 of:0.0036 and:0.0048)
		( and:0.0786 among:0.0019 compared:0.0023 also:0.0033
:0.0040 as:0.0055 of:0.0093 and:0.0095)
		(,:0.0542 the:0.0044 was:0.0062 also:0.0075 is:0.0054 as:0.0096 of:0.0311 and:0.0147)
		(,:0.0410 the:0.0078 was:0.0146 also:0.0127 is:0.0057,:0.0142 of:0.0723 which:0.0369)
		(,:0.0306 which:0.0156�:0.0306 the:0.0190 great:0.0060 that:0.0223 of:0.1289 and:0.0962)
		( in:0.0242 which:0.0258�:0.0605 the:0.0469 perfect:0.0179 that:0.0466 of:0.1494 and:0.1924)
		( in:0.0205 but:0.0361�:0.0840 the:0.0762 great:0.0249 that:0.0835 that:0.1748 and:0.2656)
		(,:0.0189 and:0.0437�:0.0835 not:0.1182 very:0.0233 that:0.1138 that:0.1855 and:0.3086)
		( the:0.0231 and:0.0645�:0.0688 not:0.1768 very:0.0219 that:0.1260 that:0.1777 and:0.3086)
		( the:0.0359 and:0.0869 would:0.0737 not:0.2090 very:0.0206 that:0.1240 that:0.1572 and:0.2949)
		( the:0.0593 and:0.1025 would:0.0820 not:0.2119 very:0.0203 that:0.1187 that:0.1445 and:0.2754)
 and
------
		(,:0.9023 I:0.3184'm:0.2090 a:0.9453 language:0.7266 model:0.0869,:1.0000 and:1.0000)
		(umatic:0.0014aling:0.0095ought:0.0019ning:0.0018 prompt:0.0037 space:0.0093
:0.0031 and:0.0165)
		( compared:0.0011 compared:0.0016 designed:0.0021 second:0.0019 in:0.0026 of:0.0036 and:0.0048 and:0.0085)
		( among:0.0019 compared:0.0023 also:0.0033
:0.0040 as:0.0055 of:0.0093 and:0.0095 the:0.0110)
		( the:0.0044 was:0.0062 also:0.0075 is:0.0054 as:0.0096 of:0.0311 and:0.0147 other:0.0266)
		( the:0.0078 was:0.0146 also:0.0127 is:0.0057,:0.0142 of:0.0723 which:0.0369 other:0.0461)
		( which:0.0156�:0.0306 the:0.0190 great:0.0060 that:0.0223 of:0.1289 and:0.0962 so:0.0452)
		( which:0.0258�:0.0605 the:0.0469 perfect:0.0179 that:0.0466 of:0.1494 and:0.1924 a:0.0718)
		( but:0.0361�:0.0840 the:0.0762 great:0.0249 that:0.0835 that:0.1748 and:0.2656 I:0.1211)
		( and:0.0437�:0.0835 not:0.1182 very:0.0233 that:0.1138 that:0.1855 and:0.3086 I:0.2637)
		( and:0.0645�:0.0688 not:0.1768 very:0.0219 that:0.1260 that:0.1777 and:0.3086 I:0.2812)
		( and:0.0869 would:0.0737 not:0.2090 very:0.0206 that:0.1240 that:0.1572 and:0.2949 I:0.2500)
		( and:0.1025 would:0.0820 not:0.2119 very:0.0203 that:0.1187 that:0.1445 and:0.2754 I:0.2061)
 I'm a language model.
I'm a language
